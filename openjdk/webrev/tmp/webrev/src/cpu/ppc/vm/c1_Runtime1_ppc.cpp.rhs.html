<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta charset="utf-8">
<meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

    <script type="text/javascript" src="../../../../ancnav.js"></script>
    </head>
    <body id="SUNWwebrev" onkeypress="keypress(event);">
    <a name="0"></a>
    <pre>rev <a href="https://bugs.openjdk.java.net/browse/JDK-12270">12270</a> : Reserve R30 to a cleared content register on C1 and C2 code

Several times a 0 is loaded to a register as a temporary value. This can be
improved by caching a 0 into a register.

I didn't notice a performance drop since only applying this patch showed no
drop of performance, hence there are more registers available than normally
needed and this caching technique can be applied.

Despite setting R30_zero as a dedicated register and initialized with 0 for the
C1 and C2 code, new rules for storing 0 related to stb,sth,stw,std were added.</pre><hr></hr>
<pre>
   1 /*
   2  * Copyright (c) 1999, 2015, Oracle and/or its affiliates. All rights reserved.
   3  * Copyright (c) 2012, 2015 SAP SE. All rights reserved.
   4  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   5  *
   6  * This code is free software; you can redistribute it and/or modify it
   7  * under the terms of the GNU General Public License version 2 only, as
   8  * published by the Free Software Foundation.
   9  *
  10  * This code is distributed in the hope that it will be useful, but WITHOUT
  11  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  12  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  13  * version 2 for more details (a copy is included in the LICENSE file that
  14  * accompanied this code).
  15  *
  16  * You should have received a copy of the GNU General Public License version
  17  * 2 along with this work; if not, write to the Free Software Foundation,
  18  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  19  *
  20  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  21  * or visit www.oracle.com if you need additional information or have any
  22  * questions.
  23  *
  24  */
  25 
  26 #include "precompiled.hpp"
  27 #include "c1/c1_Defs.hpp"
  28 #include "c1/c1_MacroAssembler.hpp"
  29 #include "c1/c1_Runtime1.hpp"
  30 #include "interpreter/interpreter.hpp"
  31 #include "nativeInst_ppc.hpp"
  32 #include "oops/compiledICHolder.hpp"
  33 #include "oops/oop.inline.hpp"
  34 #include "prims/jvmtiExport.hpp"
  35 #include "register_ppc.hpp"
  36 #include "runtime/sharedRuntime.hpp"
  37 #include "runtime/signature.hpp"
  38 #include "runtime/vframeArray.hpp"
  39 #include "utilities/macros.hpp"
  40 #include "vmreg_ppc.inline.hpp"
  41 #if INCLUDE_ALL_GCS
  42 #include "gc/g1/g1SATBCardTableModRefBS.hpp"
  43 #endif
  44 
  45 // Implementation of StubAssembler
  46 
  47 int StubAssembler::call_RT(Register oop_result1, Register metadata_result,
  48                            address entry_point, int number_of_arguments) {
  49   set_num_rt_args(0); // Nothing on stack
  50   assert(!(oop_result1-&gt;is_valid() || metadata_result-&gt;is_valid()) ||
  51          oop_result1 != metadata_result, "registers must be different");
  52 
  53   // Currently no stack banging. We assume that there are enough
  54   // StackShadowPages (which have been banged in generate_stack_overflow_check)
  55   // for the stub frame and the runtime frames.
  56 
  57   set_last_Java_frame(R1_SP, noreg);
  58 
  59   // ARG1 must hold thread address.
  60   mr(R3_ARG1, R16_thread);
  61 
  62   address return_pc = call_c_with_frame_resize(entry_point, /*No resize, we have a C compatible frame.*/0);
  63 
  64   reset_last_Java_frame();
  65 
  66   // Check for pending exceptions.
  67   {
  68     ld(R0, in_bytes(Thread::pending_exception_offset()), R16_thread);
  69     cmpdi(CCR0, R0, 0);
  70 
  71     // This used to conditionally jump to forward_exception however it is
  72     // possible if we relocate that the branch will not reach. So we must jump
  73     // around so we can always reach.
  74 
  75     Label ok;
  76     beq(CCR0, ok);
  77 
  78     // Make sure that the vm_results are cleared.
  79     if (oop_result1-&gt;is_valid() || metadata_result-&gt;is_valid()) {
  80       li(R0, 0);
  81       if (oop_result1-&gt;is_valid()) {
  82         std(R0, in_bytes(JavaThread::vm_result_offset()), R16_thread);
  83       }
  84       if (metadata_result-&gt;is_valid()) {
  85         std(R0, in_bytes(JavaThread::vm_result_2_offset()), R16_thread);
  86       }
  87     }
  88 
  89     if (frame_size() == no_frame_size) {
  90       ShouldNotReachHere(); // We always have a frame size.
  91       //pop_frame(); // pop the stub frame
  92       //ld(R0, _abi(lr), R1_SP);
  93       //mtlr(R0);
  94       //load_const_optimized(R0, StubRoutines::forward_exception_entry());
  95       //mtctr(R0);
  96       //bctr();
  97     } else if (_stub_id == Runtime1::forward_exception_id) {
  98       should_not_reach_here();
  99     } else {
 100       // keep stub frame for next call_RT
 101       //load_const_optimized(R0, Runtime1::entry_for(Runtime1::forward_exception_id));
 102       add_const_optimized(R0, R29_TOC, MacroAssembler::offset_to_global_toc(Runtime1::entry_for(Runtime1::forward_exception_id)));
 103       mtctr(R0);
 104       bctr();
 105     }
 106 
 107     bind(ok);
 108   }
 109 
 110   // Get oop results if there are any and reset the values in the thread.
 111   if (oop_result1-&gt;is_valid()) {
 112     get_vm_result(oop_result1);
 113   }
 114   if (metadata_result-&gt;is_valid()) {
 115     get_vm_result_2(metadata_result);
 116   }
 117 
 118   return (int)(return_pc - code_section()-&gt;start());
 119 }
 120 
 121 
 122 int StubAssembler::call_RT(Register oop_result1, Register metadata_result, address entry, Register arg1) {
 123   mr_if_needed(R4_ARG2, arg1);
 124   return call_RT(oop_result1, metadata_result, entry, 1);
 125 }
 126 
 127 
 128 int StubAssembler::call_RT(Register oop_result1, Register metadata_result, address entry, Register arg1, Register arg2) {
 129   mr_if_needed(R4_ARG2, arg1);
 130   mr_if_needed(R5_ARG3, arg2); assert(arg2 != R4_ARG2, "smashed argument");
 131   return call_RT(oop_result1, metadata_result, entry, 2);
 132 }
 133 
 134 
 135 int StubAssembler::call_RT(Register oop_result1, Register metadata_result, address entry, Register arg1, Register arg2, Register arg3) {
 136   mr_if_needed(R4_ARG2, arg1);
 137   mr_if_needed(R5_ARG3, arg2); assert(arg2 != R4_ARG2, "smashed argument");
 138   mr_if_needed(R6_ARG4, arg3); assert(arg3 != R4_ARG2 &amp;&amp; arg3 != R5_ARG3, "smashed argument");
 139   return call_RT(oop_result1, metadata_result, entry, 3);
 140 }
 141 
 142 
 143 // Implementation of Runtime1
 144 
 145 #define __ sasm-&gt;
 146 
 147 static int cpu_reg_save_offsets[FrameMap::nof_cpu_regs];
 148 static int fpu_reg_save_offsets[FrameMap::nof_fpu_regs];
 149 static int frame_size_in_bytes = -1;
 150 
 151 static OopMap* generate_oop_map(StubAssembler* sasm, bool save_fpu_registers) {
 152   assert(frame_size_in_bytes &gt; frame::abi_reg_args_size, "init");
 153   sasm-&gt;set_frame_size(frame_size_in_bytes / BytesPerWord);
 154   int frame_size_in_slots = frame_size_in_bytes / sizeof(jint);
 155   OopMap* oop_map = new OopMap(frame_size_in_slots, 0);
 156 
 157   int i;
 158   for (i = 0; i &lt; FrameMap::nof_cpu_regs; i++) {
 159     Register r = as_Register(i);
 160     if (FrameMap::reg_needs_save(r)) {
 161       int sp_offset = cpu_reg_save_offsets[i];
 162       oop_map-&gt;set_callee_saved(VMRegImpl::stack2reg(sp_offset&gt;&gt;2), r-&gt;as_VMReg());
 163       oop_map-&gt;set_callee_saved(VMRegImpl::stack2reg((sp_offset&gt;&gt;2) + 1), r-&gt;as_VMReg()-&gt;next());
 164     }
 165   }
 166 
 167   if (save_fpu_registers) {
 168     for (i = 0; i &lt; FrameMap::nof_fpu_regs; i++) {
 169       FloatRegister r = as_FloatRegister(i);
 170       int sp_offset = fpu_reg_save_offsets[i];
 171       oop_map-&gt;set_callee_saved(VMRegImpl::stack2reg(sp_offset&gt;&gt;2), r-&gt;as_VMReg());
 172       oop_map-&gt;set_callee_saved(VMRegImpl::stack2reg((sp_offset&gt;&gt;2) + 1), r-&gt;as_VMReg()-&gt;next());
 173     }
 174   }
 175 
 176   return oop_map;
 177 }
 178 
 179 static OopMap* save_live_registers(StubAssembler* sasm, bool save_fpu_registers = true,
 180                                    Register ret_pc = noreg, int stack_preserve = 0) {
 181   if (ret_pc == noreg) {
 182     ret_pc = R0;
 183     __ mflr(ret_pc);
 184   }
 185   __ std(ret_pc, _abi(lr), R1_SP); // C code needs pc in C1 method.
 186   __ push_frame(frame_size_in_bytes + stack_preserve, R0);
 187 
 188   // Record volatile registers as callee-save values in an OopMap so
 189   // their save locations will be propagated to the caller frame's
 190   // RegisterMap during StackFrameStream construction (needed for
 191   // deoptimization; see compiledVFrame::create_stack_value).
 192   // OopMap frame sizes are in c2 stack slot sizes (sizeof(jint)).
 193 
 194   int i;
 195   for (i = 0; i &lt; FrameMap::nof_cpu_regs; i++) {
 196     Register r = as_Register(i);
 197     if (FrameMap::reg_needs_save(r)) {
 198       int sp_offset = cpu_reg_save_offsets[i];
 199       __ std(r, sp_offset + STACK_BIAS, R1_SP);
 200     }
 201   }
 202 
 203   if (save_fpu_registers) {
 204     for (i = 0; i &lt; FrameMap::nof_fpu_regs; i++) {
 205       FloatRegister r = as_FloatRegister(i);
 206       int sp_offset = fpu_reg_save_offsets[i];
 207       __ stfd(r, sp_offset + STACK_BIAS, R1_SP);
 208     }
 209   }
 210 
 211   return generate_oop_map(sasm, save_fpu_registers);
 212 }
 213 
 214 static void restore_live_registers(StubAssembler* sasm, Register result1, Register result2,
 215                                    bool restore_fpu_registers = true) {
 216   for (int i = 0; i &lt; FrameMap::nof_cpu_regs; i++) {
 217     Register r = as_Register(i);
 218     if (FrameMap::reg_needs_save(r) &amp;&amp; r != result1 &amp;&amp; r != result2) {
 219       int sp_offset = cpu_reg_save_offsets[i];
 220       __ ld(r, sp_offset + STACK_BIAS, R1_SP);
 221     }
 222   }
 223 
 224   if (restore_fpu_registers) {
 225     for (int i = 0; i &lt; FrameMap::nof_fpu_regs; i++) {
 226       FloatRegister r = as_FloatRegister(i);
 227       int sp_offset = fpu_reg_save_offsets[i];
 228       __ lfd(r, sp_offset + STACK_BIAS, R1_SP);
 229     }
 230   }
 231 
 232   __ pop_frame();
 233   __ ld(R0, _abi(lr), R1_SP);
 234   __ mtlr(R0);
 235 }
 236 
 237 
 238 void Runtime1::initialize_pd() {
 239   int i;
 240   int sp_offset = frame::abi_reg_args_size;
 241 
 242   for (i = 0; i &lt; FrameMap::nof_cpu_regs; i++) {
 243     Register r = as_Register(i);
 244     if (FrameMap::reg_needs_save(r)) {
 245       cpu_reg_save_offsets[i] = sp_offset;
 246       sp_offset += BytesPerWord;
 247     }
 248   }
 249 
 250   for (i = 0; i &lt; FrameMap::nof_fpu_regs; i++) {
 251     fpu_reg_save_offsets[i] = sp_offset;
 252     sp_offset += BytesPerWord;
 253   }
 254   frame_size_in_bytes = align_size_up(sp_offset, frame::alignment_in_bytes);
 255 }
 256 
 257 
 258 OopMapSet* Runtime1::generate_exception_throw(StubAssembler* sasm, address target, bool has_argument) {
 259   // Make a frame and preserve the caller's caller-save registers.
 260   OopMap* oop_map = save_live_registers(sasm);
 261 
 262   int call_offset;
 263   if (!has_argument) {
 264     call_offset = __ call_RT(noreg, noreg, target);
 265   } else {
 266     call_offset = __ call_RT(noreg, noreg, target, R4_ARG2);
 267   }
 268   OopMapSet* oop_maps = new OopMapSet();
 269   oop_maps-&gt;add_gc_map(call_offset, oop_map);
 270 
 271   __ should_not_reach_here();
 272   return oop_maps;
 273 }
 274 
 275 static OopMapSet* generate_exception_throw_with_stack_parms(StubAssembler* sasm, address target,
 276                                                             int stack_parms) {
 277   // Make a frame and preserve the caller's caller-save registers.
 278   const int parm_size_in_bytes = align_size_up(stack_parms &lt;&lt; LogBytesPerWord, frame::alignment_in_bytes);
 279   const int padding = parm_size_in_bytes - (stack_parms &lt;&lt; LogBytesPerWord);
 280   OopMap* oop_map = save_live_registers(sasm, true, noreg, parm_size_in_bytes);
 281 
 282   int call_offset = 0;
 283   switch (stack_parms) {
 284     case 3:
 285     __ ld(R6_ARG4, frame_size_in_bytes + padding + 16, R1_SP);
 286     case 2:
 287     __ ld(R5_ARG3, frame_size_in_bytes + padding + 8, R1_SP);
 288     case 1:
 289     __ ld(R4_ARG2, frame_size_in_bytes + padding + 0, R1_SP);
 290     call_offset = __ call_RT(noreg, noreg, target);
 291     break;
 292     default: Unimplemented(); break;
 293   }
 294   OopMapSet* oop_maps = new OopMapSet();
 295   oop_maps-&gt;add_gc_map(call_offset, oop_map);
 296 
 297   __ should_not_reach_here();
 298   return oop_maps;
 299 }
 300 
 301 
 302 OopMapSet* Runtime1::generate_stub_call(StubAssembler* sasm, Register result, address target,
 303                                         Register arg1, Register arg2, Register arg3) {
 304   // Make a frame and preserve the caller's caller-save registers.
 305   OopMap* oop_map = save_live_registers(sasm);
 306 
 307   int call_offset;
 308   if (arg1 == noreg) {
 309     call_offset = __ call_RT(result, noreg, target);
 310   } else if (arg2 == noreg) {
 311     call_offset = __ call_RT(result, noreg, target, arg1);
 312   } else if (arg3 == noreg) {
 313     call_offset = __ call_RT(result, noreg, target, arg1, arg2);
 314   } else {
 315     call_offset = __ call_RT(result, noreg, target, arg1, arg2, arg3);
 316   }
 317   OopMapSet* oop_maps = new OopMapSet();
 318   oop_maps-&gt;add_gc_map(call_offset, oop_map);
 319 
 320   restore_live_registers(sasm, result, noreg);
 321   __ blr();
 322   return oop_maps;
 323 }
 324 
 325 static OopMapSet* stub_call_with_stack_parms(StubAssembler* sasm, Register result, address target,
 326                                              int stack_parms, bool do_return = true) {
 327   // Make a frame and preserve the caller's caller-save registers.
 328   const int parm_size_in_bytes = align_size_up(stack_parms &lt;&lt; LogBytesPerWord, frame::alignment_in_bytes);
 329   const int padding = parm_size_in_bytes - (stack_parms &lt;&lt; LogBytesPerWord);
 330   OopMap* oop_map = save_live_registers(sasm, true, noreg, parm_size_in_bytes);
 331 
 332   int call_offset = 0;
 333   switch (stack_parms) {
 334     case 3:
 335     __ ld(R6_ARG4, frame_size_in_bytes + padding + 16, R1_SP);
 336     case 2:
 337     __ ld(R5_ARG3, frame_size_in_bytes + padding + 8, R1_SP);
 338     case 1:
 339     __ ld(R4_ARG2, frame_size_in_bytes + padding + 0, R1_SP);
 340     call_offset = __ call_RT(result, noreg, target);
 341     break;
 342     default: Unimplemented(); break;
 343   }
 344   OopMapSet* oop_maps = new OopMapSet();
 345   oop_maps-&gt;add_gc_map(call_offset, oop_map);
 346 
 347   restore_live_registers(sasm, result, noreg);
 348   if (do_return) __ blr();
 349   return oop_maps;
 350 }
 351 
 352 
 353 OopMapSet* Runtime1::generate_patching(StubAssembler* sasm, address target) {
 354   // Make a frame and preserve the caller's caller-save registers.
 355   OopMap* oop_map = save_live_registers(sasm);
 356 
 357   // Call the runtime patching routine, returns non-zero if nmethod got deopted.
 358   int call_offset = __ call_RT(noreg, noreg, target);
 359   OopMapSet* oop_maps = new OopMapSet();
 360   oop_maps-&gt;add_gc_map(call_offset, oop_map);
 361   __ cmpdi(CCR0, R3_RET, 0);
 362 
 363   // Re-execute the patched instruction or, if the nmethod was deoptmized,
 364   // return to the deoptimization handler entry that will cause re-execution
 365   // of the current bytecode.
 366   DeoptimizationBlob* deopt_blob = SharedRuntime::deopt_blob();
 367   assert(deopt_blob != NULL, "deoptimization blob must have been created");
 368 
 369   // Return to the deoptimization handler entry for unpacking and rexecute.
 370   // If we simply returned the we'd deopt as if any call we patched had just
 371   // returned.
 372 
 373   restore_live_registers(sasm, noreg, noreg);
 374   // Return if patching routine returned 0.
 375   __ bclr(Assembler::bcondCRbiIs1, Assembler::bi0(CCR0, Assembler::equal), Assembler::bhintbhBCLRisReturn);
 376 
 377   address stub = deopt_blob-&gt;unpack_with_reexecution();
 378   //__ load_const_optimized(R0, stub);
 379   __ add_const_optimized(R0, R29_TOC, MacroAssembler::offset_to_global_toc(stub));
 380   __ mtctr(R0);
 381   __ bctr();
 382 
 383   return oop_maps;
 384 }
 385 
 386 OopMapSet* Runtime1::generate_code_for(StubID id, StubAssembler* sasm) {
 387   OopMapSet* oop_maps = NULL;
 388 
 389   // For better readability.
 390   const bool must_gc_arguments = true;
 391   const bool dont_gc_arguments = false;
 392 
 393   // Stub code &amp; info for the different stubs.
 394   switch (id) {
 395     case forward_exception_id:
 396       {
 397         oop_maps = generate_handle_exception(id, sasm);
 398       }
 399       break;
 400 
 401     case new_instance_id:
 402     case fast_new_instance_id:
 403     case fast_new_instance_init_check_id:
 404       {
 405         if (id == new_instance_id) {
 406           __ set_info("new_instance", dont_gc_arguments);
 407         } else if (id == fast_new_instance_id) {
 408           __ set_info("fast new_instance", dont_gc_arguments);
 409         } else {
 410           assert(id == fast_new_instance_init_check_id, "bad StubID");
 411           __ set_info("fast new_instance init check", dont_gc_arguments);
 412         }
 413         // We don't support eden allocation.
 414 //        if ((id == fast_new_instance_id || id == fast_new_instance_init_check_id) &amp;&amp;
 415 //            UseTLAB &amp;&amp; FastTLABRefill) {
 416 //          if (id == fast_new_instance_init_check_id) {
 417 //            // make sure the klass is initialized
 418 //            __ lbz(R0, in_bytes(InstanceKlass::init_state_offset()), R3_ARG1);
 419 //            __ cmpwi(CCR0, R0, InstanceKlass::fully_initialized);
 420 //            __ bne(CCR0, slow_path);
 421 //          }
 422 //#ifdef ASSERT
 423 //          // assert object can be fast path allocated
 424 //          {
 425 //            Label ok, not_ok;
 426 //          __ lwz(R0, in_bytes(Klass::layout_helper_offset()), R3_ARG1);
 427 //          // make sure it's an instance (LH &gt; 0)
 428 //          __ cmpwi(CCR0, R0, 0);
 429 //          __ ble(CCR0, not_ok);
 430 //          __ testbitdi(CCR0, R0, R0, Klass::_lh_instance_slow_path_bit);
 431 //          __ beq(CCR0, ok);
 432 //
 433 //          __ bind(not_ok);
 434 //          __ stop("assert(can be fast path allocated)");
 435 //          __ bind(ok);
 436 //          }
 437 //#endif // ASSERT
 438 //          // We don't support eden allocation.
 439 //          __ bind(slow_path);
 440 //        }
 441         oop_maps = generate_stub_call(sasm, R3_RET, CAST_FROM_FN_PTR(address, new_instance), R4_ARG2);
 442       }
 443       break;
 444 
 445     case counter_overflow_id:
 446         // Bci and method are on stack.
 447         oop_maps = stub_call_with_stack_parms(sasm, noreg, CAST_FROM_FN_PTR(address, counter_overflow), 2);
 448       break;
 449 
 450     case new_type_array_id:
 451     case new_object_array_id:
 452       {
 453         if (id == new_type_array_id) {
 454           __ set_info("new_type_array", dont_gc_arguments);
 455         } else {
 456           __ set_info("new_object_array", dont_gc_arguments);
 457         }
 458 
 459 #ifdef ASSERT
 460         // Assert object type is really an array of the proper kind.
 461         {
 462           int tag = (id == new_type_array_id) ? Klass::_lh_array_tag_type_value : Klass::_lh_array_tag_obj_value;
 463           Label ok;
 464           __ lwz(R0, in_bytes(Klass::layout_helper_offset()), R4_ARG2);
 465           __ srawi(R0, R0, Klass::_lh_array_tag_shift);
 466           __ cmpwi(CCR0, R0, tag);
 467           __ beq(CCR0, ok);
 468           __ stop("assert(is an array klass)");
 469           __ should_not_reach_here();
 470           __ bind(ok);
 471         }
 472 #endif // ASSERT
 473 
 474         // We don't support eden allocation.
 475 
 476         if (id == new_type_array_id) {
 477           oop_maps = generate_stub_call(sasm, R3_RET, CAST_FROM_FN_PTR(address, new_type_array), R4_ARG2, R5_ARG3);
 478         } else {
 479           oop_maps = generate_stub_call(sasm, R3_RET, CAST_FROM_FN_PTR(address, new_object_array), R4_ARG2, R5_ARG3);
 480         }
 481       }
 482       break;
 483 
 484     case new_multi_array_id:
 485       {
 486         // R4: klass
 487         // R5: rank
 488         // R6: address of 1st dimension
 489         __ set_info("new_multi_array", dont_gc_arguments);
 490         oop_maps = generate_stub_call(sasm, R3_RET, CAST_FROM_FN_PTR(address, new_multi_array), R4_ARG2, R5_ARG3, R6_ARG4);
 491       }
 492       break;
 493 
 494     case register_finalizer_id:
 495       {
 496         __ set_info("register_finalizer", dont_gc_arguments);
 497         // This code is called via rt_call. Hence, caller-save registers have been saved.
 498         Register t = R11_scratch1;
 499 
 500         // Load the klass and check the has finalizer flag.
 501         __ load_klass(t, R3_ARG1);
 502         __ lwz(t, in_bytes(Klass::access_flags_offset()), t);
 503         __ testbitdi(CCR0, R0, t, exact_log2(JVM_ACC_HAS_FINALIZER));
 504         // Return if has_finalizer bit == 0 (CR0.eq).
 505         __ bclr(Assembler::bcondCRbiIs1, Assembler::bi0(CCR0, Assembler::equal), Assembler::bhintbhBCLRisReturn);
 506 
 507         __ mflr(R0);
 508         __ std(R0, _abi(lr), R1_SP);
 509         __ push_frame(frame::abi_reg_args_size, R0); // Empty dummy frame (no callee-save regs).
 510         sasm-&gt;set_frame_size(frame::abi_reg_args_size / BytesPerWord);
 511         OopMap* oop_map = new OopMap(frame::abi_reg_args_size / sizeof(jint), 0);
 512         int call_offset = __ call_RT(noreg, noreg,
 513                                      CAST_FROM_FN_PTR(address, SharedRuntime::register_finalizer), R3_ARG1);
 514         oop_maps = new OopMapSet();
 515         oop_maps-&gt;add_gc_map(call_offset, oop_map);
 516 
 517         __ pop_frame();
 518         __ ld(R0, _abi(lr), R1_SP);
 519         __ mtlr(R0);
 520         __ blr();
 521       }
 522       break;
 523 
 524     case throw_range_check_failed_id:
 525       {
 526         __ set_info("range_check_failed", dont_gc_arguments); // Arguments will be discarded.
 527         __ std(R0, -8, R1_SP); // Pass index on stack.
 528         oop_maps = generate_exception_throw_with_stack_parms(sasm, CAST_FROM_FN_PTR(address, throw_range_check_exception), 1);
 529       }
 530       break;
 531 
 532     case throw_index_exception_id:
 533       {
 534         __ set_info("index_range_check_failed", dont_gc_arguments); // Arguments will be discarded.
 535         oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_index_exception), true);
 536       }
 537       break;
 538 
 539     case throw_div0_exception_id:
 540       {
 541         __ set_info("throw_div0_exception", dont_gc_arguments);
 542         oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_div0_exception), false);
 543       }
 544       break;
 545 
 546     case throw_null_pointer_exception_id:
 547       {
 548         __ set_info("throw_null_pointer_exception", dont_gc_arguments);
 549         oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_null_pointer_exception), false);
 550       }
 551       break;
 552 
 553     case handle_exception_nofpu_id:
 554     case handle_exception_id:
 555       {
 556         __ set_info("handle_exception", dont_gc_arguments);
 557         oop_maps = generate_handle_exception(id, sasm);
 558       }
 559       break;
 560 
 561     case handle_exception_from_callee_id:
 562       {
 563         __ set_info("handle_exception_from_callee", dont_gc_arguments);
 564         oop_maps = generate_handle_exception(id, sasm);
 565       }
 566       break;
 567 
 568     case unwind_exception_id:
 569       {
 570         const Register Rexception    = R3 /*LIRGenerator::exceptionOopOpr()*/,
 571                        Rexception_pc = R4 /*LIRGenerator::exceptionPcOpr()*/,
 572                        Rexception_save = R31, Rcaller_sp = R30;
 573         __ set_info("unwind_exception", dont_gc_arguments);
 574 
 575         __ ld(Rcaller_sp, 0, R1_SP);
 576         __ push_frame_reg_args(0, R0); // dummy frame for C call
 577         __ mr(Rexception_save, Rexception); // save over C call
 578         __ ld(Rexception_pc, _abi(lr), Rcaller_sp); // return pc
 579         __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::exception_handler_for_return_address), R16_thread, Rexception_pc);
 580         __ verify_not_null_oop(Rexception_save);
 581         __ mtctr(R3_RET);
 582         __ ld(Rexception_pc, _abi(lr), Rcaller_sp); // return pc
 583         __ mr(R1_SP, Rcaller_sp); // Pop both frames at once.
<a name="1" id="anc1"></a><span class="new"> 584         __ li(R30_zero, 0); // redefine R30 after Rcaller_sp usage</span>
 585         __ mr(Rexception, Rexception_save); // restore
 586         __ mtlr(Rexception_pc);
 587         __ bctr();
 588       }
 589       break;
 590 
 591     case throw_array_store_exception_id:
 592       {
 593         __ set_info("throw_array_store_exception", dont_gc_arguments);
 594         oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_array_store_exception), true);
 595       }
 596       break;
 597 
 598     case throw_class_cast_exception_id:
 599       {
 600         __ set_info("throw_class_cast_exception", dont_gc_arguments);
 601         oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_class_cast_exception), true);
 602       }
 603       break;
 604 
 605     case throw_incompatible_class_change_error_id:
 606       {
 607         __ set_info("throw_incompatible_class_cast_exception", dont_gc_arguments);
 608         oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_incompatible_class_change_error), false);
 609       }
 610       break;
 611 
 612     case slow_subtype_check_id:
 613       { // Support for uint StubRoutine::partial_subtype_check( Klass sub, Klass super );
 614         const Register sub_klass = R5,
 615                        super_klass = R4,
 616                        temp1_reg = R6,
 617                        temp2_reg = R0;
 618         __ check_klass_subtype_slow_path(sub_klass, super_klass, temp1_reg, temp2_reg); // returns with CR0.eq if successful
 619         __ crandc(CCR0, Assembler::equal, CCR0, Assembler::equal); // failed: CR0.ne
 620         __ blr();
 621       }
 622       break;
 623 
 624     case monitorenter_nofpu_id:
 625     case monitorenter_id:
 626       {
 627         __ set_info("monitorenter", dont_gc_arguments);
 628 
 629         int save_fpu_registers = (id == monitorenter_id);
 630         // Make a frame and preserve the caller's caller-save registers.
 631         OopMap* oop_map = save_live_registers(sasm, save_fpu_registers);
 632 
 633         int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, monitorenter), R4_ARG2, R5_ARG3);
 634 
 635         oop_maps = new OopMapSet();
 636         oop_maps-&gt;add_gc_map(call_offset, oop_map);
 637 
 638         restore_live_registers(sasm, noreg, noreg, save_fpu_registers);
 639         __ blr();
 640       }
 641       break;
 642 
 643     case monitorexit_nofpu_id:
 644     case monitorexit_id:
 645       {
 646         // note: Really a leaf routine but must setup last java sp
 647         //       =&gt; use call_RT for now (speed can be improved by
 648         //       doing last java sp setup manually).
 649         __ set_info("monitorexit", dont_gc_arguments);
 650 
 651         int save_fpu_registers = (id == monitorexit_id);
 652         // Make a frame and preserve the caller's caller-save registers.
 653         OopMap* oop_map = save_live_registers(sasm, save_fpu_registers);
 654 
 655         int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, monitorexit), R4_ARG2);
 656 
 657         oop_maps = new OopMapSet();
 658         oop_maps-&gt;add_gc_map(call_offset, oop_map);
 659 
 660         restore_live_registers(sasm, noreg, noreg, save_fpu_registers);
 661         __ blr();
 662       }
 663       break;
 664 
 665     case deoptimize_id:
 666       {
 667         __ set_info("deoptimize", dont_gc_arguments);
 668         __ std(R0, -8, R1_SP); // Pass trap_request on stack.
 669         oop_maps = stub_call_with_stack_parms(sasm, noreg, CAST_FROM_FN_PTR(address, deoptimize), 1, /*do_return*/ false);
 670 
 671         DeoptimizationBlob* deopt_blob = SharedRuntime::deopt_blob();
 672         assert(deopt_blob != NULL, "deoptimization blob must have been created");
 673         address stub = deopt_blob-&gt;unpack_with_reexecution();
 674         //__ load_const_optimized(R0, stub);
 675         __ add_const_optimized(R0, R29_TOC, MacroAssembler::offset_to_global_toc(stub));
 676         __ mtctr(R0);
 677         __ bctr();
 678       }
 679       break;
 680 
 681     case access_field_patching_id:
 682       {
 683         __ set_info("access_field_patching", dont_gc_arguments);
 684         oop_maps = generate_patching(sasm, CAST_FROM_FN_PTR(address, access_field_patching));
 685       }
 686       break;
 687 
 688     case load_klass_patching_id:
 689       {
 690         __ set_info("load_klass_patching", dont_gc_arguments);
 691         oop_maps = generate_patching(sasm, CAST_FROM_FN_PTR(address, move_klass_patching));
 692       }
 693       break;
 694 
 695     case load_mirror_patching_id:
 696       {
 697         __ set_info("load_mirror_patching", dont_gc_arguments);
 698         oop_maps = generate_patching(sasm, CAST_FROM_FN_PTR(address, move_mirror_patching));
 699       }
 700       break;
 701 
 702     case load_appendix_patching_id:
 703       {
 704         __ set_info("load_appendix_patching", dont_gc_arguments);
 705         oop_maps = generate_patching(sasm, CAST_FROM_FN_PTR(address, move_appendix_patching));
 706       }
 707       break;
 708 
 709     case dtrace_object_alloc_id:
 710       { // O0: object
 711         __ unimplemented("stub dtrace_object_alloc_id");
 712         __ set_info("dtrace_object_alloc", dont_gc_arguments);
 713 //        // We can't gc here so skip the oopmap but make sure that all
 714 //        // the live registers get saved.
 715 //        save_live_registers(sasm);
 716 //
 717 //        __ save_thread(L7_thread_cache);
 718 //        __ call(CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_object_alloc),
 719 //                relocInfo::runtime_call_type);
 720 //        __ delayed()-&gt;mov(I0, O0);
 721 //        __ restore_thread(L7_thread_cache);
 722 //
 723 //        restore_live_registers(sasm);
 724 //        __ ret();
 725 //        __ delayed()-&gt;restore();
 726       }
 727       break;
 728 
 729 #if INCLUDE_ALL_GCS
 730     case g1_pre_barrier_slow_id:
 731       {
 732         BarrierSet* bs = Universe::heap()-&gt;barrier_set();
 733         if (bs-&gt;kind() != BarrierSet::G1SATBCTLogging) {
 734           goto unimplemented_entry;
 735         }
 736 
 737         __ set_info("g1_pre_barrier_slow_id", dont_gc_arguments);
 738 
 739         // Using stack slots: pre_val (pre-pushed), spill tmp, spill tmp2.
 740         const int stack_slots = 3;
 741         Register pre_val = R0; // previous value of memory
 742         Register tmp  = R14;
 743         Register tmp2 = R15;
 744 
 745         Label refill, restart;
 746         int satb_q_index_byte_offset =
 747           in_bytes(JavaThread::satb_mark_queue_offset() +
 748                    SATBMarkQueue::byte_offset_of_index());
 749         int satb_q_buf_byte_offset =
 750           in_bytes(JavaThread::satb_mark_queue_offset() +
 751                    SATBMarkQueue::byte_offset_of_buf());
 752 
 753         // Spill
 754         __ std(tmp, -16, R1_SP);
 755         __ std(tmp2, -24, R1_SP);
 756 
 757         __ bind(restart);
 758         // Load the index into the SATB buffer. SATBMarkQueue::_index is a
 759         // size_t so ld_ptr is appropriate.
 760         __ ld(tmp, satb_q_index_byte_offset, R16_thread);
 761 
 762         // index == 0?
 763         __ cmpdi(CCR0, tmp, 0);
 764         __ beq(CCR0, refill);
 765 
 766         __ ld(tmp2, satb_q_buf_byte_offset, R16_thread);
 767         __ ld(pre_val, -8, R1_SP); // Load from stack.
 768         __ addi(tmp, tmp, -oopSize);
 769 
 770         __ std(tmp, satb_q_index_byte_offset, R16_thread);
 771         __ stdx(pre_val, tmp2, tmp); // [_buf + index] := &lt;address_of_card&gt;
 772 
 773         // Restore temp registers and return-from-leaf.
 774         __ ld(tmp2, -24, R1_SP);
 775         __ ld(tmp, -16, R1_SP);
 776         __ blr();
 777 
 778         __ bind(refill);
 779         const int nbytes_save = (MacroAssembler::num_volatile_regs + stack_slots) * BytesPerWord;
 780         __ save_volatile_gprs(R1_SP, -nbytes_save); // except R0
 781         __ mflr(R0);
 782         __ std(R0, _abi(lr), R1_SP);
 783         __ push_frame_reg_args(nbytes_save, R0); // dummy frame for C call
 784         __ call_VM_leaf(CAST_FROM_FN_PTR(address, SATBMarkQueueSet::handle_zero_index_for_thread), R16_thread);
 785         __ pop_frame();
 786         __ ld(R0, _abi(lr), R1_SP);
 787         __ mtlr(R0);
 788         __ restore_volatile_gprs(R1_SP, -nbytes_save); // except R0
 789         __ b(restart);
 790       }
 791       break;
 792 
 793   case g1_post_barrier_slow_id:
 794     {
 795         BarrierSet* bs = Universe::heap()-&gt;barrier_set();
 796         if (bs-&gt;kind() != BarrierSet::G1SATBCTLogging) {
 797           goto unimplemented_entry;
 798         }
 799 
 800         __ set_info("g1_post_barrier_slow_id", dont_gc_arguments);
 801 
 802         // Using stack slots: spill addr, spill tmp2
 803         const int stack_slots = 2;
 804         Register tmp = R0;
 805         Register addr = R14;
 806         Register tmp2 = R15;
 807         jbyte* byte_map_base = ((CardTableModRefBS*)bs)-&gt;byte_map_base;
 808 
 809         Label restart, refill, ret;
 810 
 811         // Spill
 812         __ std(addr, -8, R1_SP);
 813         __ std(tmp2, -16, R1_SP);
 814 
 815         __ srdi(addr, R0, CardTableModRefBS::card_shift); // Addr is passed in R0.
 816         __ load_const_optimized(/*cardtable*/ tmp2, byte_map_base, tmp);
 817         __ add(addr, tmp2, addr);
 818         __ lbz(tmp, 0, addr); // tmp := [addr + cardtable]
 819 
 820         // Return if young card.
 821         __ cmpwi(CCR0, tmp, G1SATBCardTableModRefBS::g1_young_card_val());
 822         __ beq(CCR0, ret);
 823 
 824         // Return if sequential consistent value is already dirty.
 825         __ membar(Assembler::StoreLoad);
 826         __ lbz(tmp, 0, addr); // tmp := [addr + cardtable]
 827 
 828         __ cmpwi(CCR0, tmp, G1SATBCardTableModRefBS::dirty_card_val());
 829         __ beq(CCR0, ret);
 830 
 831         // Not dirty.
 832 
 833         // First, dirty it.
 834         __ li(tmp, G1SATBCardTableModRefBS::dirty_card_val());
 835         __ stb(tmp, 0, addr);
 836 
 837         int dirty_card_q_index_byte_offset =
 838           in_bytes(JavaThread::dirty_card_queue_offset() +
 839                    DirtyCardQueue::byte_offset_of_index());
 840         int dirty_card_q_buf_byte_offset =
 841           in_bytes(JavaThread::dirty_card_queue_offset() +
 842                    DirtyCardQueue::byte_offset_of_buf());
 843 
 844         __ bind(restart);
 845 
 846         // Get the index into the update buffer. DirtyCardQueue::_index is
 847         // a size_t so ld_ptr is appropriate here.
 848         __ ld(tmp2, dirty_card_q_index_byte_offset, R16_thread);
 849 
 850         // index == 0?
 851         __ cmpdi(CCR0, tmp2, 0);
 852         __ beq(CCR0, refill);
 853 
 854         __ ld(tmp, dirty_card_q_buf_byte_offset, R16_thread);
 855         __ addi(tmp2, tmp2, -oopSize);
 856 
 857         __ std(tmp2, dirty_card_q_index_byte_offset, R16_thread);
 858         __ add(tmp2, tmp, tmp2);
 859         __ std(addr, 0, tmp2); // [_buf + index] := &lt;address_of_card&gt;
 860 
 861         // Restore temp registers and return-from-leaf.
 862         __ bind(ret);
 863         __ ld(tmp2, -16, R1_SP);
 864         __ ld(addr, -8, R1_SP);
 865         __ blr();
 866 
 867         __ bind(refill);
 868         const int nbytes_save = (MacroAssembler::num_volatile_regs + stack_slots) * BytesPerWord;
 869         __ save_volatile_gprs(R1_SP, -nbytes_save); // except R0
 870         __ mflr(R0);
 871         __ std(R0, _abi(lr), R1_SP);
 872         __ push_frame_reg_args(nbytes_save, R0); // dummy frame for C call
 873         __ call_VM_leaf(CAST_FROM_FN_PTR(address, DirtyCardQueueSet::handle_zero_index_for_thread), R16_thread);
 874         __ pop_frame();
 875         __ ld(R0, _abi(lr), R1_SP);
 876         __ mtlr(R0);
 877         __ restore_volatile_gprs(R1_SP, -nbytes_save); // except R0
 878         __ b(restart);
 879       }
 880       break;
 881 #endif // INCLUDE_ALL_GCS
 882 
 883     case predicate_failed_trap_id:
 884       {
 885         __ set_info("predicate_failed_trap", dont_gc_arguments);
 886         OopMap* oop_map = save_live_registers(sasm);
 887 
 888         int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, predicate_failed_trap));
 889 
 890         oop_maps = new OopMapSet();
 891         oop_maps-&gt;add_gc_map(call_offset, oop_map);
 892 
 893         DeoptimizationBlob* deopt_blob = SharedRuntime::deopt_blob();
 894         assert(deopt_blob != NULL, "deoptimization blob must have been created");
 895         restore_live_registers(sasm, noreg, noreg);
 896 
 897         address stub = deopt_blob-&gt;unpack_with_reexecution();
 898         //__ load_const_optimized(R0, stub);
 899         __ add_const_optimized(R0, R29_TOC, MacroAssembler::offset_to_global_toc(stub));
 900         __ mtctr(R0);
 901         __ bctr();
 902       }
 903       break;
 904 
 905   default:
 906   unimplemented_entry:
 907       {
 908         __ set_info("unimplemented entry", dont_gc_arguments);
 909         __ mflr(R0);
 910         __ std(R0, _abi(lr), R1_SP);
 911         __ push_frame(frame::abi_reg_args_size, R0); // empty dummy frame
 912         sasm-&gt;set_frame_size(frame::abi_reg_args_size / BytesPerWord);
 913         OopMap* oop_map = new OopMap(frame::abi_reg_args_size / sizeof(jint), 0);
 914 
 915         __ load_const_optimized(R4_ARG2, (int)id);
 916         int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, unimplemented_entry), R4_ARG2);
 917 
 918         oop_maps = new OopMapSet();
 919         oop_maps-&gt;add_gc_map(call_offset, oop_map);
 920         __ should_not_reach_here();
 921       }
 922       break;
 923   }
 924   return oop_maps;
 925 }
 926 
 927 
 928 OopMapSet* Runtime1::generate_handle_exception(StubID id, StubAssembler* sasm) {
 929   __ block_comment("generate_handle_exception");
 930 
 931   // Save registers, if required.
 932   OopMapSet* oop_maps = new OopMapSet();
 933   OopMap* oop_map = NULL;
 934   const Register Rexception    = R3 /*LIRGenerator::exceptionOopOpr()*/,
 935                  Rexception_pc = R4 /*LIRGenerator::exceptionPcOpr()*/;
 936 
 937   switch (id) {
 938   case forward_exception_id:
 939     // We're handling an exception in the context of a compiled frame.
 940     // The registers have been saved in the standard places. Perform
 941     // an exception lookup in the caller and dispatch to the handler
 942     // if found. Otherwise unwind and dispatch to the callers
 943     // exception handler.
 944     oop_map = generate_oop_map(sasm, true);
 945     // Transfer the pending exception to the exception_oop.
 946     // Also load the PC which is typically at SP + frame_size_in_bytes + _abi(lr),
 947     // but we support additional slots in the frame for parameter passing.
 948     __ ld(Rexception_pc, 0, R1_SP);
 949     __ ld(Rexception, in_bytes(JavaThread::pending_exception_offset()), R16_thread);
 950     __ li(R0, 0);
 951     __ ld(Rexception_pc, _abi(lr), Rexception_pc);
 952     __ std(R0, in_bytes(JavaThread::pending_exception_offset()), R16_thread);
 953     break;
 954   case handle_exception_nofpu_id:
 955   case handle_exception_id:
 956     // At this point all registers MAY be live.
 957     oop_map = save_live_registers(sasm, id != handle_exception_nofpu_id, Rexception_pc);
 958     break;
 959   case handle_exception_from_callee_id:
 960     // At this point all registers except exception oop and exception pc are dead.
 961     oop_map = new OopMap(frame_size_in_bytes / sizeof(jint), 0);
 962     sasm-&gt;set_frame_size(frame_size_in_bytes / BytesPerWord);
 963     __ std(Rexception_pc, _abi(lr), R1_SP);
 964     __ push_frame(frame_size_in_bytes, R0);
 965     break;
 966   default:  ShouldNotReachHere();
 967   }
 968 
 969   __ verify_not_null_oop(Rexception);
 970 
 971 #ifdef ASSERT
 972   // Check that fields in JavaThread for exception oop and issuing pc are
 973   // empty before writing to them.
 974   __ ld(R0, in_bytes(JavaThread::exception_oop_offset()), R16_thread);
 975   __ cmpdi(CCR0, R0, 0);
 976   __ asm_assert_eq("exception oop already set", 0x963);
 977   __ ld(R0, in_bytes(JavaThread::exception_pc_offset() ), R16_thread);
 978   __ cmpdi(CCR0, R0, 0);
 979   __ asm_assert_eq("exception pc already set", 0x962);
 980 #endif
 981 
 982   // Save the exception and issuing pc in the thread.
 983   __ std(Rexception,    in_bytes(JavaThread::exception_oop_offset()), R16_thread);
 984   __ std(Rexception_pc, in_bytes(JavaThread::exception_pc_offset() ), R16_thread);
 985 
 986   int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, exception_handler_for_pc));
 987   oop_maps-&gt;add_gc_map(call_offset, oop_map);
 988 
 989   __ mtctr(R3_RET);
 990 
 991   // Note: if nmethod has been deoptimized then regardless of
 992   // whether it had a handler or not we will deoptimize
 993   // by entering the deopt blob with a pending exception.
 994 
 995   // Restore the registers that were saved at the beginning, remove
 996   // the frame and jump to the exception handler.
 997   switch (id) {
 998   case forward_exception_id:
 999   case handle_exception_nofpu_id:
1000   case handle_exception_id:
1001     restore_live_registers(sasm, noreg, noreg, id != handle_exception_nofpu_id);
1002     __ bctr();
1003     break;
1004   case handle_exception_from_callee_id: {
1005     __ pop_frame();
1006     __ ld(Rexception_pc, _abi(lr), R1_SP);
1007     __ mtlr(Rexception_pc);
1008     __ bctr();
1009     break;
1010   }
1011   default:  ShouldNotReachHere();
1012   }
1013 
1014   return oop_maps;
1015 }
1016 
1017 const char *Runtime1::pd_name_for_address(address entry) {
1018   return "&lt;unknown function&gt;";
1019 }
1020 
1021 #undef __
<a name="2" id="anc2"></a><b style="font-size: large; color: red">--- EOF ---</b>















































































</pre><form name="eof"><input name="value" value="2" type="hidden" /></form></body></html>
