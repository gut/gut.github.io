<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta charset="utf-8">
<meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

    <script type="text/javascript" src="../../../../ancnav.js"></script>
    </head>
    <body id="SUNWwebrev" onkeypress="keypress(event);">
    <a name="0"></a>
    <pre>rev <a href="https://bugs.openjdk.java.net/browse/JDK-12270">12270</a> : Reserve R30 to a cleared content register on C1 and C2 code

Several times a 0 is loaded to a register as a temporary value. This can be
improved by caching a 0 into a register.

I didn't notice a performance drop since only applying this patch showed no
drop of performance, hence there are more registers available than normally
needed and this caching technique can be applied.

Despite setting R30_zero as a dedicated register and initialized with 0 for the
C1 and C2 code, new rules for storing 0 related to stb,sth,stw,std were added.</pre><hr></hr>
<pre>
   1 /*
   2  * Copyright (c) 2014, 2016, Oracle and/or its affiliates. All rights reserved.
   3  * Copyright (c) 2015, 2016 SAP SE. All rights reserved.
   4  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   5  *
   6  * This code is free software; you can redistribute it and/or modify it
   7  * under the terms of the GNU General Public License version 2 only, as
   8  * published by the Free Software Foundation.
   9  *
  10  * This code is distributed in the hope that it will be useful, but WITHOUT
  11  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  12  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  13  * version 2 for more details (a copy is included in the LICENSE file that
  14  * accompanied this code).
  15  *
  16  * You should have received a copy of the GNU General Public License version
  17  * 2 along with this work; if not, write to the Free Software Foundation,
  18  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  19  *
  20  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  21  * or visit www.oracle.com if you need additional information or have any
  22  * questions.
  23  *
  24  */
  25 
  26 #include "precompiled.hpp"
  27 #include "asm/macroAssembler.inline.hpp"
  28 #include "interpreter/bytecodeHistogram.hpp"
  29 #include "interpreter/interpreter.hpp"
  30 #include "interpreter/interpreterRuntime.hpp"
  31 #include "interpreter/interp_masm.hpp"
  32 #include "interpreter/templateInterpreterGenerator.hpp"
  33 #include "interpreter/templateTable.hpp"
  34 #include "oops/arrayOop.hpp"
  35 #include "oops/methodData.hpp"
  36 #include "oops/method.hpp"
  37 #include "oops/oop.inline.hpp"
  38 #include "prims/jvmtiExport.hpp"
  39 #include "prims/jvmtiThreadState.hpp"
  40 #include "runtime/arguments.hpp"
  41 #include "runtime/deoptimization.hpp"
  42 #include "runtime/frame.inline.hpp"
  43 #include "runtime/sharedRuntime.hpp"
  44 #include "runtime/stubRoutines.hpp"
  45 #include "runtime/synchronizer.hpp"
  46 #include "runtime/timer.hpp"
  47 #include "runtime/vframeArray.hpp"
  48 #include "utilities/debug.hpp"
  49 #include "utilities/macros.hpp"
  50 
  51 #undef __
  52 #define __ _masm-&gt;
  53 
  54 // Size of interpreter code.  Increase if too small.  Interpreter will
  55 // fail with a guarantee ("not enough space for interpreter generation");
  56 // if too small.
  57 // Run with +PrintInterpreter to get the VM to print out the size.
  58 // Max size with JVMTI
  59 int TemplateInterpreter::InterpreterCodeSize = 230*K;
  60 
  61 #ifdef PRODUCT
  62 #define BLOCK_COMMENT(str) /* nothing */
  63 #else
  64 #define BLOCK_COMMENT(str) __ block_comment(str)
  65 #endif
  66 
  67 #define BIND(label)        __ bind(label); BLOCK_COMMENT(#label ":")
  68 
  69 //-----------------------------------------------------------------------------
  70 
  71 address TemplateInterpreterGenerator::generate_slow_signature_handler() {
  72   // Slow_signature handler that respects the PPC C calling conventions.
  73   //
  74   // We get called by the native entry code with our output register
  75   // area == 8. First we call InterpreterRuntime::get_result_handler
  76   // to copy the pointer to the signature string temporarily to the
  77   // first C-argument and to return the result_handler in
  78   // R3_RET. Since native_entry will copy the jni-pointer to the
  79   // first C-argument slot later on, it is OK to occupy this slot
  80   // temporarilly. Then we copy the argument list on the java
  81   // expression stack into native varargs format on the native stack
  82   // and load arguments into argument registers. Integer arguments in
  83   // the varargs vector will be sign-extended to 8 bytes.
  84   //
  85   // On entry:
  86   //   R3_ARG1        - intptr_t*     Address of java argument list in memory.
  87   //   R15_prev_state - BytecodeInterpreter* Address of interpreter state for
  88   //     this method
  89   //   R19_method
  90   //
  91   // On exit (just before return instruction):
  92   //   R3_RET            - contains the address of the result_handler.
  93   //   R4_ARG2           - is not updated for static methods and contains "this" otherwise.
  94   //   R5_ARG3-R10_ARG8: - When the (i-2)th Java argument is not of type float or double,
  95   //                       ARGi contains this argument. Otherwise, ARGi is not updated.
  96   //   F1_ARG1-F13_ARG13 - contain the first 13 arguments of type float or double.
  97 
  98   const int LogSizeOfTwoInstructions = 3;
  99 
 100   // FIXME: use Argument:: GL: Argument names different numbers!
 101   const int max_fp_register_arguments  = 13;
 102   const int max_int_register_arguments = 6;  // first 2 are reserved
 103 
 104   const Register arg_java       = R21_tmp1;
 105   const Register arg_c          = R22_tmp2;
 106   const Register signature      = R23_tmp3;  // is string
 107   const Register sig_byte       = R24_tmp4;
 108   const Register fpcnt          = R25_tmp5;
 109   const Register argcnt         = R26_tmp6;
 110   const Register intSlot        = R27_tmp7;
 111   const Register target_sp      = R28_tmp8;
 112   const FloatRegister floatSlot = F0;
 113 
 114   address entry = __ function_entry();
 115 
 116   __ save_LR_CR(R0);
 117   __ save_nonvolatile_gprs(R1_SP, _spill_nonvolatiles_neg(r14));
 118   // We use target_sp for storing arguments in the C frame.
 119   __ mr(target_sp, R1_SP);
 120   __ push_frame_reg_args_nonvolatiles(0, R11_scratch1);
 121 
 122   __ mr(arg_java, R3_ARG1);
 123 
 124   __ call_VM_leaf(CAST_FROM_FN_PTR(address, InterpreterRuntime::get_signature), R16_thread, R19_method);
 125 
 126   // Signature is in R3_RET. Signature is callee saved.
 127   __ mr(signature, R3_RET);
 128 
 129   // Get the result handler.
 130   __ call_VM_leaf(CAST_FROM_FN_PTR(address, InterpreterRuntime::get_result_handler), R16_thread, R19_method);
 131 
 132   {
 133     Label L;
 134     // test if static
 135     // _access_flags._flags must be at offset 0.
 136     // TODO PPC port: requires change in shared code.
 137     //assert(in_bytes(AccessFlags::flags_offset()) == 0,
 138     //       "MethodDesc._access_flags == MethodDesc._access_flags._flags");
 139     // _access_flags must be a 32 bit value.
 140     assert(sizeof(AccessFlags) == 4, "wrong size");
 141     __ lwa(R11_scratch1/*access_flags*/, method_(access_flags));
 142     // testbit with condition register.
 143     __ testbitdi(CCR0, R0, R11_scratch1/*access_flags*/, JVM_ACC_STATIC_BIT);
 144     __ btrue(CCR0, L);
 145     // For non-static functions, pass "this" in R4_ARG2 and copy it
 146     // to 2nd C-arg slot.
 147     // We need to box the Java object here, so we use arg_java
 148     // (address of current Java stack slot) as argument and don't
 149     // dereference it as in case of ints, floats, etc.
 150     __ mr(R4_ARG2, arg_java);
 151     __ addi(arg_java, arg_java, -BytesPerWord);
 152     __ std(R4_ARG2, _abi(carg_2), target_sp);
 153     __ bind(L);
 154   }
 155 
 156   // Will be incremented directly after loop_start. argcnt=0
 157   // corresponds to 3rd C argument.
 158   __ li(argcnt, -1);
 159   // arg_c points to 3rd C argument
 160   __ addi(arg_c, target_sp, _abi(carg_3));
 161   // no floating-point args parsed so far
 162   __ li(fpcnt, 0);
 163 
 164   Label move_intSlot_to_ARG, move_floatSlot_to_FARG;
 165   Label loop_start, loop_end;
 166   Label do_int, do_long, do_float, do_double, do_dontreachhere, do_object, do_array, do_boxed;
 167 
 168   // signature points to '(' at entry
 169 #ifdef ASSERT
 170   __ lbz(sig_byte, 0, signature);
 171   __ cmplwi(CCR0, sig_byte, '(');
 172   __ bne(CCR0, do_dontreachhere);
 173 #endif
 174 
 175   __ bind(loop_start);
 176 
 177   __ addi(argcnt, argcnt, 1);
 178   __ lbzu(sig_byte, 1, signature);
 179 
 180   __ cmplwi(CCR0, sig_byte, ')'); // end of signature
 181   __ beq(CCR0, loop_end);
 182 
 183   __ cmplwi(CCR0, sig_byte, 'B'); // byte
 184   __ beq(CCR0, do_int);
 185 
 186   __ cmplwi(CCR0, sig_byte, 'C'); // char
 187   __ beq(CCR0, do_int);
 188 
 189   __ cmplwi(CCR0, sig_byte, 'D'); // double
 190   __ beq(CCR0, do_double);
 191 
 192   __ cmplwi(CCR0, sig_byte, 'F'); // float
 193   __ beq(CCR0, do_float);
 194 
 195   __ cmplwi(CCR0, sig_byte, 'I'); // int
 196   __ beq(CCR0, do_int);
 197 
 198   __ cmplwi(CCR0, sig_byte, 'J'); // long
 199   __ beq(CCR0, do_long);
 200 
 201   __ cmplwi(CCR0, sig_byte, 'S'); // short
 202   __ beq(CCR0, do_int);
 203 
 204   __ cmplwi(CCR0, sig_byte, 'Z'); // boolean
 205   __ beq(CCR0, do_int);
 206 
 207   __ cmplwi(CCR0, sig_byte, 'L'); // object
 208   __ beq(CCR0, do_object);
 209 
 210   __ cmplwi(CCR0, sig_byte, '['); // array
 211   __ beq(CCR0, do_array);
 212 
 213   //  __ cmplwi(CCR0, sig_byte, 'V'); // void cannot appear since we do not parse the return type
 214   //  __ beq(CCR0, do_void);
 215 
 216   __ bind(do_dontreachhere);
 217 
 218   __ unimplemented("ShouldNotReachHere in slow_signature_handler", 120);
 219 
 220   __ bind(do_array);
 221 
 222   {
 223     Label start_skip, end_skip;
 224 
 225     __ bind(start_skip);
 226     __ lbzu(sig_byte, 1, signature);
 227     __ cmplwi(CCR0, sig_byte, '[');
 228     __ beq(CCR0, start_skip); // skip further brackets
 229     __ cmplwi(CCR0, sig_byte, '9');
 230     __ bgt(CCR0, end_skip);   // no optional size
 231     __ cmplwi(CCR0, sig_byte, '0');
 232     __ bge(CCR0, start_skip); // skip optional size
 233     __ bind(end_skip);
 234 
 235     __ cmplwi(CCR0, sig_byte, 'L');
 236     __ beq(CCR0, do_object);  // for arrays of objects, the name of the object must be skipped
 237     __ b(do_boxed);          // otherwise, go directly to do_boxed
 238   }
 239 
 240   __ bind(do_object);
 241   {
 242     Label L;
 243     __ bind(L);
 244     __ lbzu(sig_byte, 1, signature);
 245     __ cmplwi(CCR0, sig_byte, ';');
 246     __ bne(CCR0, L);
 247    }
 248   // Need to box the Java object here, so we use arg_java (address of
 249   // current Java stack slot) as argument and don't dereference it as
 250   // in case of ints, floats, etc.
 251   Label do_null;
 252   __ bind(do_boxed);
 253   __ ld(R0,0, arg_java);
 254   __ cmpdi(CCR0, R0, 0);
 255   __ li(intSlot,0);
 256   __ beq(CCR0, do_null);
 257   __ mr(intSlot, arg_java);
 258   __ bind(do_null);
 259   __ std(intSlot, 0, arg_c);
 260   __ addi(arg_java, arg_java, -BytesPerWord);
 261   __ addi(arg_c, arg_c, BytesPerWord);
 262   __ cmplwi(CCR0, argcnt, max_int_register_arguments);
 263   __ blt(CCR0, move_intSlot_to_ARG);
 264   __ b(loop_start);
 265 
 266   __ bind(do_int);
 267   __ lwa(intSlot, 0, arg_java);
 268   __ std(intSlot, 0, arg_c);
 269   __ addi(arg_java, arg_java, -BytesPerWord);
 270   __ addi(arg_c, arg_c, BytesPerWord);
 271   __ cmplwi(CCR0, argcnt, max_int_register_arguments);
 272   __ blt(CCR0, move_intSlot_to_ARG);
 273   __ b(loop_start);
 274 
 275   __ bind(do_long);
 276   __ ld(intSlot, -BytesPerWord, arg_java);
 277   __ std(intSlot, 0, arg_c);
 278   __ addi(arg_java, arg_java, - 2 * BytesPerWord);
 279   __ addi(arg_c, arg_c, BytesPerWord);
 280   __ cmplwi(CCR0, argcnt, max_int_register_arguments);
 281   __ blt(CCR0, move_intSlot_to_ARG);
 282   __ b(loop_start);
 283 
 284   __ bind(do_float);
 285   __ lfs(floatSlot, 0, arg_java);
 286 #if defined(LINUX)
 287   // Linux uses ELF ABI. Both original ELF and ELFv2 ABIs have float
 288   // in the least significant word of an argument slot.
 289 #if defined(VM_LITTLE_ENDIAN)
 290   __ stfs(floatSlot, 0, arg_c);
 291 #else
 292   __ stfs(floatSlot, 4, arg_c);
 293 #endif
 294 #elif defined(AIX)
 295   // Although AIX runs on big endian CPU, float is in most significant
 296   // word of an argument slot.
 297   __ stfs(floatSlot, 0, arg_c);
 298 #else
 299 #error "unknown OS"
 300 #endif
 301   __ addi(arg_java, arg_java, -BytesPerWord);
 302   __ addi(arg_c, arg_c, BytesPerWord);
 303   __ cmplwi(CCR0, fpcnt, max_fp_register_arguments);
 304   __ blt(CCR0, move_floatSlot_to_FARG);
 305   __ b(loop_start);
 306 
 307   __ bind(do_double);
 308   __ lfd(floatSlot, - BytesPerWord, arg_java);
 309   __ stfd(floatSlot, 0, arg_c);
 310   __ addi(arg_java, arg_java, - 2 * BytesPerWord);
 311   __ addi(arg_c, arg_c, BytesPerWord);
 312   __ cmplwi(CCR0, fpcnt, max_fp_register_arguments);
 313   __ blt(CCR0, move_floatSlot_to_FARG);
 314   __ b(loop_start);
 315 
 316   __ bind(loop_end);
 317 
 318   __ pop_frame();
 319   __ restore_nonvolatile_gprs(R1_SP, _spill_nonvolatiles_neg(r14));
 320   __ restore_LR_CR(R0);
 321 
 322   __ blr();
 323 
 324   Label move_int_arg, move_float_arg;
 325   __ bind(move_int_arg); // each case must consist of 2 instructions (otherwise adapt LogSizeOfTwoInstructions)
 326   __ mr(R5_ARG3, intSlot);  __ b(loop_start);
 327   __ mr(R6_ARG4, intSlot);  __ b(loop_start);
 328   __ mr(R7_ARG5, intSlot);  __ b(loop_start);
 329   __ mr(R8_ARG6, intSlot);  __ b(loop_start);
 330   __ mr(R9_ARG7, intSlot);  __ b(loop_start);
 331   __ mr(R10_ARG8, intSlot); __ b(loop_start);
 332 
 333   __ bind(move_float_arg); // each case must consist of 2 instructions (otherwise adapt LogSizeOfTwoInstructions)
 334   __ fmr(F1_ARG1, floatSlot);   __ b(loop_start);
 335   __ fmr(F2_ARG2, floatSlot);   __ b(loop_start);
 336   __ fmr(F3_ARG3, floatSlot);   __ b(loop_start);
 337   __ fmr(F4_ARG4, floatSlot);   __ b(loop_start);
 338   __ fmr(F5_ARG5, floatSlot);   __ b(loop_start);
 339   __ fmr(F6_ARG6, floatSlot);   __ b(loop_start);
 340   __ fmr(F7_ARG7, floatSlot);   __ b(loop_start);
 341   __ fmr(F8_ARG8, floatSlot);   __ b(loop_start);
 342   __ fmr(F9_ARG9, floatSlot);   __ b(loop_start);
 343   __ fmr(F10_ARG10, floatSlot); __ b(loop_start);
 344   __ fmr(F11_ARG11, floatSlot); __ b(loop_start);
 345   __ fmr(F12_ARG12, floatSlot); __ b(loop_start);
 346   __ fmr(F13_ARG13, floatSlot); __ b(loop_start);
 347 
 348   __ bind(move_intSlot_to_ARG);
 349   __ sldi(R0, argcnt, LogSizeOfTwoInstructions);
 350   __ load_const(R11_scratch1, move_int_arg); // Label must be bound here.
 351   __ add(R11_scratch1, R0, R11_scratch1);
 352   __ mtctr(R11_scratch1/*branch_target*/);
 353   __ bctr();
 354   __ bind(move_floatSlot_to_FARG);
 355   __ sldi(R0, fpcnt, LogSizeOfTwoInstructions);
 356   __ addi(fpcnt, fpcnt, 1);
 357   __ load_const(R11_scratch1, move_float_arg); // Label must be bound here.
 358   __ add(R11_scratch1, R0, R11_scratch1);
 359   __ mtctr(R11_scratch1/*branch_target*/);
 360   __ bctr();
 361 
 362   return entry;
 363 }
 364 
 365 address TemplateInterpreterGenerator::generate_result_handler_for(BasicType type) {
 366   //
 367   // Registers alive
 368   //   R3_RET
 369   //   LR
 370   //
 371   // Registers updated
 372   //   R3_RET
 373   //
 374 
 375   Label done;
 376   address entry = __ pc();
 377 
 378   switch (type) {
 379   case T_BOOLEAN:
 380     // convert !=0 to 1
 381     __ neg(R0, R3_RET);
 382     __ orr(R0, R3_RET, R0);
 383     __ srwi(R3_RET, R0, 31);
 384     break;
 385   case T_BYTE:
 386      // sign extend 8 bits
 387      __ extsb(R3_RET, R3_RET);
 388      break;
 389   case T_CHAR:
 390      // zero extend 16 bits
 391      __ clrldi(R3_RET, R3_RET, 48);
 392      break;
 393   case T_SHORT:
 394      // sign extend 16 bits
 395      __ extsh(R3_RET, R3_RET);
 396      break;
 397   case T_INT:
 398      // sign extend 32 bits
 399      __ extsw(R3_RET, R3_RET);
 400      break;
 401   case T_LONG:
 402      break;
 403   case T_OBJECT:
 404     // unbox result if not null
 405     __ cmpdi(CCR0, R3_RET, 0);
 406     __ beq(CCR0, done);
 407     __ ld(R3_RET, 0, R3_RET);
 408     __ verify_oop(R3_RET);
 409     break;
 410   case T_FLOAT:
 411      break;
 412   case T_DOUBLE:
 413      break;
 414   case T_VOID:
 415      break;
 416   default: ShouldNotReachHere();
 417   }
 418 
 419   BIND(done);
 420   __ blr();
 421 
 422   return entry;
 423 }
 424 
 425 // Abstract method entry.
 426 //
 427 address TemplateInterpreterGenerator::generate_abstract_entry(void) {
 428   address entry = __ pc();
 429 
 430   //
 431   // Registers alive
 432   //   R16_thread     - JavaThread*
 433   //   R19_method     - callee's method (method to be invoked)
 434   //   R1_SP          - SP prepared such that caller's outgoing args are near top
 435   //   LR             - return address to caller
 436   //
 437   // Stack layout at this point:
 438   //
 439   //   0       [TOP_IJAVA_FRAME_ABI]         &lt;-- R1_SP
 440   //           alignment (optional)
 441   //           [outgoing Java arguments]
 442   //           ...
 443   //   PARENT  [PARENT_IJAVA_FRAME_ABI]
 444   //            ...
 445   //
 446 
 447   // Can't use call_VM here because we have not set up a new
 448   // interpreter state. Make the call to the vm and make it look like
 449   // our caller set up the JavaFrameAnchor.
 450   __ set_top_ijava_frame_at_SP_as_last_Java_frame(R1_SP, R12_scratch2/*tmp*/);
 451 
 452   // Push a new C frame and save LR.
 453   __ save_LR_CR(R0);
 454   __ push_frame_reg_args(0, R11_scratch1);
 455 
 456   // This is not a leaf but we have a JavaFrameAnchor now and we will
 457   // check (create) exceptions afterward so this is ok.
 458   __ call_VM_leaf(CAST_FROM_FN_PTR(address, InterpreterRuntime::throw_AbstractMethodError),
 459                   R16_thread);
 460 
 461   // Pop the C frame and restore LR.
 462   __ pop_frame();
 463   __ restore_LR_CR(R0);
 464 
 465   // Reset JavaFrameAnchor from call_VM_leaf above.
 466   __ reset_last_Java_frame();
 467 
 468   // We don't know our caller, so jump to the general forward exception stub,
 469   // which will also pop our full frame off. Satisfy the interface of
 470   // SharedRuntime::generate_forward_exception()
 471   __ load_const_optimized(R11_scratch1, StubRoutines::forward_exception_entry(), R0);
 472   __ mtctr(R11_scratch1);
 473   __ bctr();
 474 
 475   return entry;
 476 }
 477 
 478 // Interpreter intrinsic for WeakReference.get().
 479 // 1. Don't push a full blown frame and go on dispatching, but fetch the value
 480 //    into R8 and return quickly
 481 // 2. If G1 is active we *must* execute this intrinsic for corrrectness:
 482 //    It contains a GC barrier which puts the reference into the satb buffer
 483 //    to indicate that someone holds a strong reference to the object the
 484 //    weak ref points to!
 485 address TemplateInterpreterGenerator::generate_Reference_get_entry(void) {
 486   // Code: _aload_0, _getfield, _areturn
 487   // parameter size = 1
 488   //
 489   // The code that gets generated by this routine is split into 2 parts:
 490   //    1. the "intrinsified" code for G1 (or any SATB based GC),
 491   //    2. the slow path - which is an expansion of the regular method entry.
 492   //
 493   // Notes:
 494   // * In the G1 code we do not check whether we need to block for
 495   //   a safepoint. If G1 is enabled then we must execute the specialized
 496   //   code for Reference.get (except when the Reference object is null)
 497   //   so that we can log the value in the referent field with an SATB
 498   //   update buffer.
 499   //   If the code for the getfield template is modified so that the
 500   //   G1 pre-barrier code is executed when the current method is
 501   //   Reference.get() then going through the normal method entry
 502   //   will be fine.
 503   // * The G1 code can, however, check the receiver object (the instance
 504   //   of java.lang.Reference) and jump to the slow path if null. If the
 505   //   Reference object is null then we obviously cannot fetch the referent
 506   //   and so we don't need to call the G1 pre-barrier. Thus we can use the
 507   //   regular method entry code to generate the NPE.
 508   //
 509 
 510   if (UseG1GC) {
 511     address entry = __ pc();
 512 
 513     const int referent_offset = java_lang_ref_Reference::referent_offset;
 514     guarantee(referent_offset &gt; 0, "referent offset not initialized");
 515 
 516     Label slow_path;
 517 
 518     // Debugging not possible, so can't use __ skip_if_jvmti_mode(slow_path, GR31_SCRATCH);
 519 
 520     // In the G1 code we don't check if we need to reach a safepoint. We
 521     // continue and the thread will safepoint at the next bytecode dispatch.
 522 
 523     // If the receiver is null then it is OK to jump to the slow path.
 524     __ ld(R3_RET, Interpreter::stackElementSize, R15_esp); // get receiver
 525 
 526     // Check if receiver == NULL and go the slow path.
 527     __ cmpdi(CCR0, R3_RET, 0);
 528     __ beq(CCR0, slow_path);
 529 
 530     // Load the value of the referent field.
 531     __ load_heap_oop(R3_RET, referent_offset, R3_RET);
 532 
 533     // Generate the G1 pre-barrier code to log the value of
 534     // the referent field in an SATB buffer. Note with
 535     // these parameters the pre-barrier does not generate
 536     // the load of the previous value.
 537 
 538     // Restore caller sp for c2i case.
 539 #ifdef ASSERT
 540       __ ld(R9_ARG7, 0, R1_SP);
 541       __ ld(R10_ARG8, 0, R21_sender_SP);
 542       __ cmpd(CCR0, R9_ARG7, R10_ARG8);
 543       __ asm_assert_eq("backlink", 0x544);
 544 #endif // ASSERT
 545     __ mr(R1_SP, R21_sender_SP); // Cut the stack back to where the caller started.
 546 
 547     __ g1_write_barrier_pre(noreg,         // obj
 548                             noreg,         // offset
 549                             R3_RET,        // pre_val
 550                             R11_scratch1,  // tmp
 551                             R12_scratch2,  // tmp
 552                             true);         // needs_frame
 553 
 554     __ blr();
 555 
 556     // Generate regular method entry.
 557     __ bind(slow_path);
 558     __ jump_to_entry(Interpreter::entry_for_kind(Interpreter::zerolocals), R11_scratch1);
 559     return entry;
 560   }
 561 
 562   return NULL;
 563 }
 564 
 565 address TemplateInterpreterGenerator::generate_StackOverflowError_handler() {
 566   address entry = __ pc();
 567 
 568   // Expression stack must be empty before entering the VM if an
 569   // exception happened.
 570   __ empty_expression_stack();
 571   // Throw exception.
 572   __ call_VM(noreg,
 573              CAST_FROM_FN_PTR(address,
 574                               InterpreterRuntime::throw_StackOverflowError));
 575   return entry;
 576 }
 577 
 578 address TemplateInterpreterGenerator::generate_ArrayIndexOutOfBounds_handler(const char* name) {
 579   address entry = __ pc();
 580   __ empty_expression_stack();
 581   __ load_const_optimized(R4_ARG2, (address) name);
 582   // Index is in R17_tos.
 583   __ mr(R5_ARG3, R17_tos);
 584   __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::throw_ArrayIndexOutOfBoundsException));
 585   return entry;
 586 }
 587 
 588 #if 0
 589 // Call special ClassCastException constructor taking object to cast
 590 // and target class as arguments.
 591 address TemplateInterpreterGenerator::generate_ClassCastException_verbose_handler() {
 592   address entry = __ pc();
 593 
 594   // Expression stack must be empty before entering the VM if an
 595   // exception happened.
 596   __ empty_expression_stack();
 597 
 598   // Thread will be loaded to R3_ARG1.
 599   // Target class oop is in register R5_ARG3 by convention!
 600   __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::throw_ClassCastException_verbose), R17_tos, R5_ARG3);
 601   // Above call must not return here since exception pending.
 602   DEBUG_ONLY(__ should_not_reach_here();)
 603   return entry;
 604 }
 605 #endif
 606 
 607 address TemplateInterpreterGenerator::generate_ClassCastException_handler() {
 608   address entry = __ pc();
 609   // Expression stack must be empty before entering the VM if an
 610   // exception happened.
 611   __ empty_expression_stack();
 612 
 613   // Load exception object.
 614   // Thread will be loaded to R3_ARG1.
 615   __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::throw_ClassCastException), R17_tos);
 616 #ifdef ASSERT
 617   // Above call must not return here since exception pending.
 618   __ should_not_reach_here();
 619 #endif
 620   return entry;
 621 }
 622 
 623 address TemplateInterpreterGenerator::generate_exception_handler_common(const char* name, const char* message, bool pass_oop) {
 624   address entry = __ pc();
 625   //__ untested("generate_exception_handler_common");
 626   Register Rexception = R17_tos;
 627 
 628   // Expression stack must be empty before entering the VM if an exception happened.
 629   __ empty_expression_stack();
 630 
 631   __ load_const_optimized(R4_ARG2, (address) name, R11_scratch1);
 632   if (pass_oop) {
 633     __ mr(R5_ARG3, Rexception);
 634     __ call_VM(Rexception, CAST_FROM_FN_PTR(address, InterpreterRuntime::create_klass_exception), false);
 635   } else {
 636     __ load_const_optimized(R5_ARG3, (address) message, R11_scratch1);
 637     __ call_VM(Rexception, CAST_FROM_FN_PTR(address, InterpreterRuntime::create_exception), false);
 638   }
 639 
 640   // Throw exception.
 641   __ mr(R3_ARG1, Rexception);
 642   __ load_const_optimized(R11_scratch1, Interpreter::throw_exception_entry(), R12_scratch2);
 643   __ mtctr(R11_scratch1);
 644   __ bctr();
 645 
 646   return entry;
 647 }
 648 
 649 address TemplateInterpreterGenerator::generate_continuation_for(TosState state) {
 650   address entry = __ pc();
 651   __ unimplemented("generate_continuation_for");
 652   return entry;
 653 }
 654 
 655 // This entry is returned to when a call returns to the interpreter.
 656 // When we arrive here, we expect that the callee stack frame is already popped.
 657 address TemplateInterpreterGenerator::generate_return_entry_for(TosState state, int step, size_t index_size) {
 658   address entry = __ pc();
 659 
 660   // Move the value out of the return register back to the TOS cache of current frame.
 661   switch (state) {
 662     case ltos:
 663     case btos:
 664     case ztos:
 665     case ctos:
 666     case stos:
 667     case atos:
 668     case itos: __ mr(R17_tos, R3_RET); break;   // RET -&gt; TOS cache
 669     case ftos:
 670     case dtos: __ fmr(F15_ftos, F1_RET); break; // TOS cache -&gt; GR_FRET
 671     case vtos: break;                           // Nothing to do, this was a void return.
 672     default  : ShouldNotReachHere();
 673   }
 674 
 675   __ restore_interpreter_state(R11_scratch1); // Sets R11_scratch1 = fp.
 676   __ ld(R12_scratch2, _ijava_state_neg(top_frame_sp), R11_scratch1);
 677   __ resize_frame_absolute(R12_scratch2, R11_scratch1, R0);
 678 
 679   // Compiled code destroys templateTableBase, reload.
 680   __ load_const_optimized(R25_templateTableBase, (address)Interpreter::dispatch_table((TosState)0), R12_scratch2);
 681 
 682   if (state == atos) {
 683     __ profile_return_type(R3_RET, R11_scratch1, R12_scratch2);
 684   }
 685 
 686   const Register cache = R11_scratch1;
 687   const Register size  = R12_scratch2;
 688   __ get_cache_and_index_at_bcp(cache, 1, index_size);
 689 
 690   // Get least significant byte of 64 bit value:
 691 #if defined(VM_LITTLE_ENDIAN)
 692   __ lbz(size, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::flags_offset()), cache);
 693 #else
 694   __ lbz(size, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::flags_offset()) + 7, cache);
 695 #endif
 696   __ sldi(size, size, Interpreter::logStackElementSize);
 697   __ add(R15_esp, R15_esp, size);
 698   __ dispatch_next(state, step);
 699   return entry;
 700 }
 701 
 702 address TemplateInterpreterGenerator::generate_deopt_entry_for(TosState state, int step) {
 703   address entry = __ pc();
 704   // If state != vtos, we're returning from a native method, which put it's result
 705   // into the result register. So move the value out of the return register back
 706   // to the TOS cache of current frame.
 707 
 708   switch (state) {
 709     case ltos:
 710     case btos:
 711     case ztos:
 712     case ctos:
 713     case stos:
 714     case atos:
 715     case itos: __ mr(R17_tos, R3_RET); break;   // GR_RET -&gt; TOS cache
 716     case ftos:
 717     case dtos: __ fmr(F15_ftos, F1_RET); break; // TOS cache -&gt; GR_FRET
 718     case vtos: break;                           // Nothing to do, this was a void return.
 719     default  : ShouldNotReachHere();
 720   }
 721 
 722   // Load LcpoolCache @@@ should be already set!
 723   __ get_constant_pool_cache(R27_constPoolCache);
 724 
 725   // Handle a pending exception, fall through if none.
 726   __ check_and_forward_exception(R11_scratch1, R12_scratch2);
 727 
 728   // Start executing bytecodes.
 729   __ dispatch_next(state, step);
 730 
 731   return entry;
 732 }
 733 
 734 address TemplateInterpreterGenerator::generate_safept_entry_for(TosState state, address runtime_entry) {
 735   address entry = __ pc();
 736 
 737   __ push(state);
 738   __ call_VM(noreg, runtime_entry);
 739   __ dispatch_via(vtos, Interpreter::_normal_table.table_for(vtos));
 740 
 741   return entry;
 742 }
 743 
 744 // Helpers for commoning out cases in the various type of method entries.
 745 
 746 // Increment invocation count &amp; check for overflow.
 747 //
 748 // Note: checking for negative value instead of overflow
 749 //       so we have a 'sticky' overflow test.
 750 //
 751 void TemplateInterpreterGenerator::generate_counter_incr(Label* overflow, Label* profile_method, Label* profile_method_continue) {
 752   // Note: In tiered we increment either counters in method or in MDO depending if we're profiling or not.
 753   Register Rscratch1   = R11_scratch1;
 754   Register Rscratch2   = R12_scratch2;
 755   Register R3_counters = R3_ARG1;
 756   Label done;
 757 
 758   if (TieredCompilation) {
 759     const int increment = InvocationCounter::count_increment;
 760     Label no_mdo;
 761     if (ProfileInterpreter) {
 762       const Register Rmdo = R3_counters;
 763       // If no method data exists, go to profile_continue.
 764       __ ld(Rmdo, in_bytes(Method::method_data_offset()), R19_method);
 765       __ cmpdi(CCR0, Rmdo, 0);
 766       __ beq(CCR0, no_mdo);
 767 
 768       // Increment invocation counter in the MDO.
 769       const int mdo_ic_offs = in_bytes(MethodData::invocation_counter_offset()) + in_bytes(InvocationCounter::counter_offset());
 770       __ lwz(Rscratch2, mdo_ic_offs, Rmdo);
 771       __ lwz(Rscratch1, in_bytes(MethodData::invoke_mask_offset()), Rmdo);
 772       __ addi(Rscratch2, Rscratch2, increment);
 773       __ stw(Rscratch2, mdo_ic_offs, Rmdo);
 774       __ and_(Rscratch1, Rscratch2, Rscratch1);
 775       __ bne(CCR0, done);
 776       __ b(*overflow);
 777     }
 778 
 779     // Increment counter in MethodCounters*.
 780     const int mo_ic_offs = in_bytes(MethodCounters::invocation_counter_offset()) + in_bytes(InvocationCounter::counter_offset());
 781     __ bind(no_mdo);
 782     __ get_method_counters(R19_method, R3_counters, done);
 783     __ lwz(Rscratch2, mo_ic_offs, R3_counters);
 784     __ lwz(Rscratch1, in_bytes(MethodCounters::invoke_mask_offset()), R3_counters);
 785     __ addi(Rscratch2, Rscratch2, increment);
 786     __ stw(Rscratch2, mo_ic_offs, R3_counters);
 787     __ and_(Rscratch1, Rscratch2, Rscratch1);
 788     __ beq(CCR0, *overflow);
 789 
 790     __ bind(done);
 791 
 792   } else {
 793 
 794     // Update standard invocation counters.
 795     Register Rsum_ivc_bec = R4_ARG2;
 796     __ get_method_counters(R19_method, R3_counters, done);
 797     __ increment_invocation_counter(R3_counters, Rsum_ivc_bec, R12_scratch2);
 798     // Increment interpreter invocation counter.
 799     if (ProfileInterpreter) {  // %%% Merge this into methodDataOop.
 800       __ lwz(R12_scratch2, in_bytes(MethodCounters::interpreter_invocation_counter_offset()), R3_counters);
 801       __ addi(R12_scratch2, R12_scratch2, 1);
 802       __ stw(R12_scratch2, in_bytes(MethodCounters::interpreter_invocation_counter_offset()), R3_counters);
 803     }
 804     // Check if we must create a method data obj.
 805     if (ProfileInterpreter &amp;&amp; profile_method != NULL) {
 806       const Register profile_limit = Rscratch1;
 807       __ lwz(profile_limit, in_bytes(MethodCounters::interpreter_profile_limit_offset()), R3_counters);
 808       // Test to see if we should create a method data oop.
 809       __ cmpw(CCR0, Rsum_ivc_bec, profile_limit);
 810       __ blt(CCR0, *profile_method_continue);
 811       // If no method data exists, go to profile_method.
 812       __ test_method_data_pointer(*profile_method);
 813     }
 814     // Finally check for counter overflow.
 815     if (overflow) {
 816       const Register invocation_limit = Rscratch1;
 817       __ lwz(invocation_limit, in_bytes(MethodCounters::interpreter_invocation_limit_offset()), R3_counters);
 818       __ cmpw(CCR0, Rsum_ivc_bec, invocation_limit);
 819       __ bge(CCR0, *overflow);
 820     }
 821 
 822     __ bind(done);
 823   }
 824 }
 825 
 826 // Generate code to initiate compilation on invocation counter overflow.
 827 void TemplateInterpreterGenerator::generate_counter_overflow(Label&amp; continue_entry) {
 828   // Generate code to initiate compilation on the counter overflow.
 829 
 830   // InterpreterRuntime::frequency_counter_overflow takes one arguments,
 831   // which indicates if the counter overflow occurs at a backwards branch (NULL bcp)
 832   // We pass zero in.
 833   // The call returns the address of the verified entry point for the method or NULL
 834   // if the compilation did not complete (either went background or bailed out).
 835   //
 836   // Unlike the C++ interpreter above: Check exceptions!
 837   // Assumption: Caller must set the flag "do_not_unlock_if_sychronized" if the monitor of a sync'ed
 838   // method has not yet been created. Thus, no unlocking of a non-existing monitor can occur.
 839 
 840   __ li(R4_ARG2, 0);
 841   __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::frequency_counter_overflow), R4_ARG2, true);
 842 
 843   // Returns verified_entry_point or NULL.
 844   // We ignore it in any case.
 845   __ b(continue_entry);
 846 }
 847 
 848 // See if we've got enough room on the stack for locals plus overhead below
 849 // JavaThread::stack_overflow_limit(). If not, throw a StackOverflowError
 850 // without going through the signal handler, i.e., reserved and yellow zones
 851 // will not be made usable. The shadow zone must suffice to handle the
 852 // overflow.
 853 //
 854 // Kills Rmem_frame_size, Rscratch1.
 855 void TemplateInterpreterGenerator::generate_stack_overflow_check(Register Rmem_frame_size, Register Rscratch1) {
 856   Label done;
 857   assert_different_registers(Rmem_frame_size, Rscratch1);
 858 
 859   BLOCK_COMMENT("stack_overflow_check_with_compare {");
 860   __ sub(Rmem_frame_size, R1_SP, Rmem_frame_size);
 861   __ ld(Rscratch1, thread_(stack_overflow_limit));
 862   __ cmpld(CCR0/*is_stack_overflow*/, Rmem_frame_size, Rscratch1);
 863   __ bgt(CCR0/*is_stack_overflow*/, done);
 864 
 865   // The stack overflows. Load target address of the runtime stub and call it.
 866   assert(StubRoutines::throw_StackOverflowError_entry() != NULL, "generated in wrong order");
 867   __ load_const_optimized(Rscratch1, (StubRoutines::throw_StackOverflowError_entry()), R0);
 868   __ mtctr(Rscratch1);
 869   // Restore caller_sp.
 870 #ifdef ASSERT
 871   __ ld(Rscratch1, 0, R1_SP);
 872   __ ld(R0, 0, R21_sender_SP);
 873   __ cmpd(CCR0, R0, Rscratch1);
 874   __ asm_assert_eq("backlink", 0x547);
 875 #endif // ASSERT
 876   __ mr(R1_SP, R21_sender_SP);
 877   __ bctr();
 878 
 879   __ align(32, 12);
 880   __ bind(done);
 881   BLOCK_COMMENT("} stack_overflow_check_with_compare");
 882 }
 883 
 884 // Lock the current method, interpreter register window must be set up!
 885 void TemplateInterpreterGenerator::lock_method(Register Rflags, Register Rscratch1, Register Rscratch2, bool flags_preloaded) {
 886   const Register Robj_to_lock = Rscratch2;
 887 
 888   {
 889     if (!flags_preloaded) {
 890       __ lwz(Rflags, method_(access_flags));
 891     }
 892 
 893 #ifdef ASSERT
 894     // Check if methods needs synchronization.
 895     {
 896       Label Lok;
 897       __ testbitdi(CCR0, R0, Rflags, JVM_ACC_SYNCHRONIZED_BIT);
 898       __ btrue(CCR0,Lok);
 899       __ stop("method doesn't need synchronization");
 900       __ bind(Lok);
 901     }
 902 #endif // ASSERT
 903   }
 904 
 905   // Get synchronization object to Rscratch2.
 906   {
 907     Label Lstatic;
 908     Label Ldone;
 909 
 910     __ testbitdi(CCR0, R0, Rflags, JVM_ACC_STATIC_BIT);
 911     __ btrue(CCR0, Lstatic);
 912 
 913     // Non-static case: load receiver obj from stack and we're done.
 914     __ ld(Robj_to_lock, R18_locals);
 915     __ b(Ldone);
 916 
 917     __ bind(Lstatic); // Static case: Lock the java mirror
 918     // Load mirror from interpreter frame.
 919     __ ld(Robj_to_lock, _abi(callers_sp), R1_SP);
 920     __ ld(Robj_to_lock, _ijava_state_neg(mirror), Robj_to_lock);
 921 
 922     __ bind(Ldone);
 923     __ verify_oop(Robj_to_lock);
 924   }
 925 
 926   // Got the oop to lock =&gt; execute!
 927   __ add_monitor_to_stack(true, Rscratch1, R0);
 928 
 929   __ std(Robj_to_lock, BasicObjectLock::obj_offset_in_bytes(), R26_monitor);
 930   __ lock_object(R26_monitor, Robj_to_lock);
 931 }
 932 
 933 // Generate a fixed interpreter frame for pure interpreter
 934 // and I2N native transition frames.
 935 //
 936 // Before (stack grows downwards):
 937 //
 938 //         |  ...         |
 939 //         |------------- |
 940 //         |  java arg0   |
 941 //         |  ...         |
 942 //         |  java argn   |
 943 //         |              |   &lt;-   R15_esp
 944 //         |              |
 945 //         |--------------|
 946 //         | abi_112      |
 947 //         |              |   &lt;-   R1_SP
 948 //         |==============|
 949 //
 950 //
 951 // After:
 952 //
 953 //         |  ...         |
 954 //         |  java arg0   |&lt;-   R18_locals
 955 //         |  ...         |
 956 //         |  java argn   |
 957 //         |--------------|
 958 //         |              |
 959 //         |  java locals |
 960 //         |              |
 961 //         |--------------|
 962 //         |  abi_48      |
 963 //         |==============|
 964 //         |              |
 965 //         |   istate     |
 966 //         |              |
 967 //         |--------------|
 968 //         |   monitor    |&lt;-   R26_monitor
 969 //         |--------------|
 970 //         |              |&lt;-   R15_esp
 971 //         | expression   |
 972 //         | stack        |
 973 //         |              |
 974 //         |--------------|
 975 //         |              |
 976 //         | abi_112      |&lt;-   R1_SP
 977 //         |==============|
 978 //
 979 // The top most frame needs an abi space of 112 bytes. This space is needed,
 980 // since we call to c. The c function may spill their arguments to the caller
 981 // frame. When we call to java, we don't need these spill slots. In order to save
 982 // space on the stack, we resize the caller. However, java locals reside in
 983 // the caller frame and the frame has to be increased. The frame_size for the
 984 // current frame was calculated based on max_stack as size for the expression
 985 // stack. At the call, just a part of the expression stack might be used.
 986 // We don't want to waste this space and cut the frame back accordingly.
 987 // The resulting amount for resizing is calculated as follows:
 988 // resize =   (number_of_locals - number_of_arguments) * slot_size
 989 //          + (R1_SP - R15_esp) + 48
 990 //
 991 // The size for the callee frame is calculated:
 992 // framesize = 112 + max_stack + monitor + state_size
 993 //
 994 // maxstack:   Max number of slots on the expression stack, loaded from the method.
 995 // monitor:    We statically reserve room for one monitor object.
 996 // state_size: We save the current state of the interpreter to this area.
 997 //
 998 void TemplateInterpreterGenerator::generate_fixed_frame(bool native_call, Register Rsize_of_parameters, Register Rsize_of_locals) {
 999   Register parent_frame_resize = R6_ARG4, // Frame will grow by this number of bytes.
1000            top_frame_size      = R7_ARG5,
1001            Rconst_method       = R8_ARG6;
1002 
1003   assert_different_registers(Rsize_of_parameters, Rsize_of_locals, parent_frame_resize, top_frame_size);
1004 
1005   __ ld(Rconst_method, method_(const));
1006   __ lhz(Rsize_of_parameters /* number of params */,
1007          in_bytes(ConstMethod::size_of_parameters_offset()), Rconst_method);
1008   if (native_call) {
1009     // If we're calling a native method, we reserve space for the worst-case signature
1010     // handler varargs vector, which is max(Argument::n_register_parameters, parameter_count+2).
1011     // We add two slots to the parameter_count, one for the jni
1012     // environment and one for a possible native mirror.
1013     Label skip_native_calculate_max_stack;
1014     __ addi(top_frame_size, Rsize_of_parameters, 2);
1015     __ cmpwi(CCR0, top_frame_size, Argument::n_register_parameters);
1016     __ bge(CCR0, skip_native_calculate_max_stack);
1017     __ li(top_frame_size, Argument::n_register_parameters);
1018     __ bind(skip_native_calculate_max_stack);
1019     __ sldi(Rsize_of_parameters, Rsize_of_parameters, Interpreter::logStackElementSize);
1020     __ sldi(top_frame_size, top_frame_size, Interpreter::logStackElementSize);
1021     __ sub(parent_frame_resize, R1_SP, R15_esp); // &lt;0, off by Interpreter::stackElementSize!
1022     assert(Rsize_of_locals == noreg, "Rsize_of_locals not initialized"); // Only relevant value is Rsize_of_parameters.
1023   } else {
1024     __ lhz(Rsize_of_locals /* number of params */, in_bytes(ConstMethod::size_of_locals_offset()), Rconst_method);
1025     __ sldi(Rsize_of_parameters, Rsize_of_parameters, Interpreter::logStackElementSize);
1026     __ sldi(Rsize_of_locals, Rsize_of_locals, Interpreter::logStackElementSize);
1027     __ lhz(top_frame_size, in_bytes(ConstMethod::max_stack_offset()), Rconst_method);
1028     __ sub(R11_scratch1, Rsize_of_locals, Rsize_of_parameters); // &gt;=0
1029     __ sub(parent_frame_resize, R1_SP, R15_esp); // &lt;0, off by Interpreter::stackElementSize!
1030     __ sldi(top_frame_size, top_frame_size, Interpreter::logStackElementSize);
1031     __ add(parent_frame_resize, parent_frame_resize, R11_scratch1);
1032   }
1033 
1034   // Compute top frame size.
1035   __ addi(top_frame_size, top_frame_size, frame::abi_reg_args_size + frame::ijava_state_size);
1036 
1037   // Cut back area between esp and max_stack.
1038   __ addi(parent_frame_resize, parent_frame_resize, frame::abi_minframe_size - Interpreter::stackElementSize);
1039 
1040   __ round_to(top_frame_size, frame::alignment_in_bytes);
1041   __ round_to(parent_frame_resize, frame::alignment_in_bytes);
1042   // parent_frame_resize = (locals-parameters) - (ESP-SP-ABI48) Rounded to frame alignment size.
1043   // Enlarge by locals-parameters (not in case of native_call), shrink by ESP-SP-ABI48.
1044 
1045   if (!native_call) {
1046     // Stack overflow check.
1047     // Native calls don't need the stack size check since they have no
1048     // expression stack and the arguments are already on the stack and
1049     // we only add a handful of words to the stack.
1050     __ add(R11_scratch1, parent_frame_resize, top_frame_size);
1051     generate_stack_overflow_check(R11_scratch1, R12_scratch2);
1052   }
1053 
1054   // Set up interpreter state registers.
1055 
<a name="1" id="anc1"></a><span class="new">1056   __ li(R30_zero, 0);</span>
1057   __ add(R18_locals, R15_esp, Rsize_of_parameters);
1058   __ ld(R27_constPoolCache, in_bytes(ConstMethod::constants_offset()), Rconst_method);
1059   __ ld(R27_constPoolCache, ConstantPool::cache_offset_in_bytes(), R27_constPoolCache);
1060 
1061   // Set method data pointer.
1062   if (ProfileInterpreter) {
1063     Label zero_continue;
1064     __ ld(R28_mdx, method_(method_data));
1065     __ cmpdi(CCR0, R28_mdx, 0);
1066     __ beq(CCR0, zero_continue);
1067     __ addi(R28_mdx, R28_mdx, in_bytes(MethodData::data_offset()));
1068     __ bind(zero_continue);
1069   }
1070 
1071   if (native_call) {
1072     __ li(R14_bcp, 0); // Must initialize.
1073   } else {
1074     __ add(R14_bcp, in_bytes(ConstMethod::codes_offset()), Rconst_method);
1075   }
1076 
1077   // Resize parent frame.
1078   __ mflr(R12_scratch2);
1079   __ neg(parent_frame_resize, parent_frame_resize);
1080   __ resize_frame(parent_frame_resize, R11_scratch1);
1081   __ std(R12_scratch2, _abi(lr), R1_SP);
1082 
1083   // Get mirror and store it in the frame as GC root for this Method*.
1084   __ load_mirror_from_const_method(R12_scratch2, Rconst_method);
1085 
1086   __ addi(R26_monitor, R1_SP, - frame::ijava_state_size);
1087   __ addi(R15_esp, R26_monitor, - Interpreter::stackElementSize);
1088 
1089   // Store values.
1090   // R15_esp, R14_bcp, R26_monitor, R28_mdx are saved at java calls
1091   // in InterpreterMacroAssembler::call_from_interpreter.
1092   __ std(R19_method, _ijava_state_neg(method), R1_SP);
1093   __ std(R12_scratch2, _ijava_state_neg(mirror), R1_SP);
1094   __ std(R21_sender_SP, _ijava_state_neg(sender_sp), R1_SP);
1095   __ std(R27_constPoolCache, _ijava_state_neg(cpoolCache), R1_SP);
1096   __ std(R18_locals, _ijava_state_neg(locals), R1_SP);
1097 
1098   // Note: esp, bcp, monitor, mdx live in registers. Hence, the correct version can only
1099   // be found in the frame after save_interpreter_state is done. This is always true
1100   // for non-top frames. But when a signal occurs, dumping the top frame can go wrong,
1101   // because e.g. frame::interpreter_frame_bcp() will not access the correct value
1102   // (Enhanced Stack Trace).
1103   // The signal handler does not save the interpreter state into the frame.
1104   __ li(R0, 0);
1105 #ifdef ASSERT
1106   // Fill remaining slots with constants.
1107   __ load_const_optimized(R11_scratch1, 0x5afe);
1108   __ load_const_optimized(R12_scratch2, 0xdead);
1109 #endif
1110   // We have to initialize some frame slots for native calls (accessed by GC).
1111   if (native_call) {
1112     __ std(R26_monitor, _ijava_state_neg(monitors), R1_SP);
1113     __ std(R14_bcp, _ijava_state_neg(bcp), R1_SP);
1114     if (ProfileInterpreter) { __ std(R28_mdx, _ijava_state_neg(mdx), R1_SP); }
1115   }
1116 #ifdef ASSERT
1117   else {
1118     __ std(R12_scratch2, _ijava_state_neg(monitors), R1_SP);
1119     __ std(R12_scratch2, _ijava_state_neg(bcp), R1_SP);
1120     __ std(R12_scratch2, _ijava_state_neg(mdx), R1_SP);
1121   }
1122   __ std(R11_scratch1, _ijava_state_neg(ijava_reserved), R1_SP);
1123   __ std(R12_scratch2, _ijava_state_neg(esp), R1_SP);
1124   __ std(R12_scratch2, _ijava_state_neg(lresult), R1_SP);
1125   __ std(R12_scratch2, _ijava_state_neg(fresult), R1_SP);
1126 #endif
1127   __ subf(R12_scratch2, top_frame_size, R1_SP);
1128   __ std(R0, _ijava_state_neg(oop_tmp), R1_SP);
1129   __ std(R12_scratch2, _ijava_state_neg(top_frame_sp), R1_SP);
1130 
1131   // Push top frame.
1132   __ push_frame(top_frame_size, R11_scratch1);
1133 }
1134 
1135 // End of helpers
1136 
1137 address TemplateInterpreterGenerator::generate_math_entry(AbstractInterpreter::MethodKind kind) {
1138   if (!Interpreter::math_entry_available(kind)) {
1139     NOT_PRODUCT(__ should_not_reach_here();)
1140     return NULL;
1141   }
1142 
1143   address entry = __ pc();
1144 
1145   __ lfd(F1_RET, Interpreter::stackElementSize, R15_esp);
1146 
1147   // Pop c2i arguments (if any) off when we return.
1148 #ifdef ASSERT
1149   __ ld(R9_ARG7, 0, R1_SP);
1150   __ ld(R10_ARG8, 0, R21_sender_SP);
1151   __ cmpd(CCR0, R9_ARG7, R10_ARG8);
1152   __ asm_assert_eq("backlink", 0x545);
1153 #endif // ASSERT
1154   __ mr(R1_SP, R21_sender_SP); // Cut the stack back to where the caller started.
1155 
1156   if (kind == Interpreter::java_lang_math_sqrt) {
1157     __ fsqrt(F1_RET, F1_RET);
1158   } else if (kind == Interpreter::java_lang_math_abs) {
1159     __ fabs(F1_RET, F1_RET);
1160   } else {
1161     ShouldNotReachHere();
1162   }
1163 
1164   // And we're done.
1165   __ blr();
1166 
1167   __ flush();
1168 
1169   return entry;
1170 }
1171 
1172 void TemplateInterpreterGenerator::bang_stack_shadow_pages(bool native_call) {
1173   // Quick &amp; dirty stack overflow checking: bang the stack &amp; handle trap.
1174   // Note that we do the banging after the frame is setup, since the exception
1175   // handling code expects to find a valid interpreter frame on the stack.
1176   // Doing the banging earlier fails if the caller frame is not an interpreter
1177   // frame.
1178   // (Also, the exception throwing code expects to unlock any synchronized
1179   // method receiever, so do the banging after locking the receiver.)
1180 
1181   // Bang each page in the shadow zone. We can't assume it's been done for
1182   // an interpreter frame with greater than a page of locals, so each page
1183   // needs to be checked.  Only true for non-native.
1184   if (UseStackBanging) {
1185     const int page_size = os::vm_page_size();
1186     const int n_shadow_pages = ((int)JavaThread::stack_shadow_zone_size()) / page_size;
1187     const int start_page = native_call ? n_shadow_pages : 1;
1188     BLOCK_COMMENT("bang_stack_shadow_pages:");
1189     for (int pages = start_page; pages &lt;= n_shadow_pages; pages++) {
1190       __ bang_stack_with_offset(pages*page_size);
1191     }
1192   }
1193 }
1194 
1195 // Interpreter stub for calling a native method. (asm interpreter)
1196 // This sets up a somewhat different looking stack for calling the
1197 // native method than the typical interpreter frame setup.
1198 //
1199 // On entry:
1200 //   R19_method    - method
1201 //   R16_thread    - JavaThread*
1202 //   R15_esp       - intptr_t* sender tos
1203 //
1204 //   abstract stack (grows up)
1205 //     [  IJava (caller of JNI callee)  ]  &lt;-- ASP
1206 //        ...
1207 address TemplateInterpreterGenerator::generate_native_entry(bool synchronized) {
1208 
1209   address entry = __ pc();
1210 
1211   const bool inc_counter = UseCompiler || CountCompiledCalls || LogTouchedMethods;
1212 
1213   // -----------------------------------------------------------------------------
1214   // Allocate a new frame that represents the native callee (i2n frame).
1215   // This is not a full-blown interpreter frame, but in particular, the
1216   // following registers are valid after this:
1217   // - R19_method
1218   // - R18_local (points to start of arguments to native function)
1219   //
1220   //   abstract stack (grows up)
1221   //     [  IJava (caller of JNI callee)  ]  &lt;-- ASP
1222   //        ...
1223 
1224   const Register signature_handler_fd = R11_scratch1;
1225   const Register pending_exception    = R0;
1226   const Register result_handler_addr  = R31;
1227   const Register native_method_fd     = R11_scratch1;
1228   const Register access_flags         = R22_tmp2;
1229   const Register active_handles       = R11_scratch1; // R26_monitor saved to state.
1230   const Register sync_state           = R12_scratch2;
1231   const Register sync_state_addr      = sync_state;   // Address is dead after use.
1232   const Register suspend_flags        = R11_scratch1;
1233 
1234   //=============================================================================
1235   // Allocate new frame and initialize interpreter state.
1236 
1237   Label exception_return;
1238   Label exception_return_sync_check;
1239   Label stack_overflow_return;
1240 
1241   // Generate new interpreter state and jump to stack_overflow_return in case of
1242   // a stack overflow.
1243   //generate_compute_interpreter_state(stack_overflow_return);
1244 
1245   Register size_of_parameters = R22_tmp2;
1246 
1247   generate_fixed_frame(true, size_of_parameters, noreg /* unused */);
1248 
1249   //=============================================================================
1250   // Increment invocation counter. On overflow, entry to JNI method
1251   // will be compiled.
1252   Label invocation_counter_overflow, continue_after_compile;
1253   if (inc_counter) {
1254     if (synchronized) {
1255       // Since at this point in the method invocation the exception handler
1256       // would try to exit the monitor of synchronized methods which hasn't
1257       // been entered yet, we set the thread local variable
1258       // _do_not_unlock_if_synchronized to true. If any exception was thrown by
1259       // runtime, exception handling i.e. unlock_if_synchronized_method will
1260       // check this thread local flag.
1261       // This flag has two effects, one is to force an unwind in the topmost
1262       // interpreter frame and not perform an unlock while doing so.
1263       __ li(R0, 1);
1264       __ stb(R0, in_bytes(JavaThread::do_not_unlock_if_synchronized_offset()), R16_thread);
1265     }
1266     generate_counter_incr(&amp;invocation_counter_overflow, NULL, NULL);
1267 
1268     BIND(continue_after_compile);
1269   }
1270 
1271   bang_stack_shadow_pages(true);
1272 
1273   if (inc_counter) {
1274     // Reset the _do_not_unlock_if_synchronized flag.
1275     if (synchronized) {
1276       __ li(R0, 0);
1277       __ stb(R0, in_bytes(JavaThread::do_not_unlock_if_synchronized_offset()), R16_thread);
1278     }
1279   }
1280 
1281   // access_flags = method-&gt;access_flags();
1282   // Load access flags.
1283   assert(access_flags-&gt;is_nonvolatile(),
1284          "access_flags must be in a non-volatile register");
1285   // Type check.
1286   assert(4 == sizeof(AccessFlags), "unexpected field size");
1287   __ lwz(access_flags, method_(access_flags));
1288 
1289   // We don't want to reload R19_method and access_flags after calls
1290   // to some helper functions.
1291   assert(R19_method-&gt;is_nonvolatile(),
1292          "R19_method must be a non-volatile register");
1293 
1294   // Check for synchronized methods. Must happen AFTER invocation counter
1295   // check, so method is not locked if counter overflows.
1296 
1297   if (synchronized) {
1298     lock_method(access_flags, R11_scratch1, R12_scratch2, true);
1299 
1300     // Update monitor in state.
1301     __ ld(R11_scratch1, 0, R1_SP);
1302     __ std(R26_monitor, _ijava_state_neg(monitors), R11_scratch1);
1303   }
1304 
1305   // jvmti/jvmpi support
1306   __ notify_method_entry();
1307 
1308   //=============================================================================
1309   // Get and call the signature handler.
1310 
1311   __ ld(signature_handler_fd, method_(signature_handler));
1312   Label call_signature_handler;
1313 
1314   __ cmpdi(CCR0, signature_handler_fd, 0);
1315   __ bne(CCR0, call_signature_handler);
1316 
1317   // Method has never been called. Either generate a specialized
1318   // handler or point to the slow one.
1319   //
1320   // Pass parameter 'false' to avoid exception check in call_VM.
1321   __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::prepare_native_call), R19_method, false);
1322 
1323   // Check for an exception while looking up the target method. If we
1324   // incurred one, bail.
1325   __ ld(pending_exception, thread_(pending_exception));
1326   __ cmpdi(CCR0, pending_exception, 0);
1327   __ bne(CCR0, exception_return_sync_check); // Has pending exception.
1328 
1329   // Reload signature handler, it may have been created/assigned in the meanwhile.
1330   __ ld(signature_handler_fd, method_(signature_handler));
1331   __ twi_0(signature_handler_fd); // Order wrt. load of klass mirror and entry point (isync is below).
1332 
1333   BIND(call_signature_handler);
1334 
1335   // Before we call the signature handler we push a new frame to
1336   // protect the interpreter frame volatile registers when we return
1337   // from jni but before we can get back to Java.
1338 
1339   // First set the frame anchor while the SP/FP registers are
1340   // convenient and the slow signature handler can use this same frame
1341   // anchor.
1342 
1343   // We have a TOP_IJAVA_FRAME here, which belongs to us.
1344   __ set_top_ijava_frame_at_SP_as_last_Java_frame(R1_SP, R12_scratch2/*tmp*/);
1345 
1346   // Now the interpreter frame (and its call chain) have been
1347   // invalidated and flushed. We are now protected against eager
1348   // being enabled in native code. Even if it goes eager the
1349   // registers will be reloaded as clean and we will invalidate after
1350   // the call so no spurious flush should be possible.
1351 
1352   // Call signature handler and pass locals address.
1353   //
1354   // Our signature handlers copy required arguments to the C stack
1355   // (outgoing C args), R3_ARG1 to R10_ARG8, and FARG1 to FARG13.
1356   __ mr(R3_ARG1, R18_locals);
1357 #if !defined(ABI_ELFv2)
1358   __ ld(signature_handler_fd, 0, signature_handler_fd);
1359 #endif
1360 
1361   __ call_stub(signature_handler_fd);
1362 
1363   // Remove the register parameter varargs slots we allocated in
1364   // compute_interpreter_state. SP+16 ends up pointing to the ABI
1365   // outgoing argument area.
1366   //
1367   // Not needed on PPC64.
1368   //__ add(SP, SP, Argument::n_register_parameters*BytesPerWord);
1369 
1370   assert(result_handler_addr-&gt;is_nonvolatile(), "result_handler_addr must be in a non-volatile register");
1371   // Save across call to native method.
1372   __ mr(result_handler_addr, R3_RET);
1373 
1374   __ isync(); // Acquire signature handler before trying to fetch the native entry point and klass mirror.
1375 
1376   // Set up fixed parameters and call the native method.
1377   // If the method is static, get mirror into R4_ARG2.
1378   {
1379     Label method_is_not_static;
1380     // Access_flags is non-volatile and still, no need to restore it.
1381 
1382     // Restore access flags.
1383     __ testbitdi(CCR0, R0, access_flags, JVM_ACC_STATIC_BIT);
1384     __ bfalse(CCR0, method_is_not_static);
1385 
1386     __ ld(R11_scratch1, _abi(callers_sp), R1_SP);
1387     // Load mirror from interpreter frame.
1388     __ ld(R12_scratch2, _ijava_state_neg(mirror), R11_scratch1);
1389     // R4_ARG2 = &amp;state-&gt;_oop_temp;
1390     __ addi(R4_ARG2, R11_scratch1, _ijava_state_neg(oop_tmp));
1391     __ std(R12_scratch2/*mirror*/, _ijava_state_neg(oop_tmp), R11_scratch1);
1392     BIND(method_is_not_static);
1393   }
1394 
1395   // At this point, arguments have been copied off the stack into
1396   // their JNI positions. Oops are boxed in-place on the stack, with
1397   // handles copied to arguments. The result handler address is in a
1398   // register.
1399 
1400   // Pass JNIEnv address as first parameter.
1401   __ addir(R3_ARG1, thread_(jni_environment));
1402 
1403   // Load the native_method entry before we change the thread state.
1404   __ ld(native_method_fd, method_(native_function));
1405 
1406   //=============================================================================
1407   // Transition from _thread_in_Java to _thread_in_native. As soon as
1408   // we make this change the safepoint code needs to be certain that
1409   // the last Java frame we established is good. The pc in that frame
1410   // just needs to be near here not an actual return address.
1411 
1412   // We use release_store_fence to update values like the thread state, where
1413   // we don't want the current thread to continue until all our prior memory
1414   // accesses (including the new thread state) are visible to other threads.
1415   __ li(R0, _thread_in_native);
1416   __ release();
1417 
1418   // TODO PPC port assert(4 == JavaThread::sz_thread_state(), "unexpected field size");
1419   __ stw(R0, thread_(thread_state));
1420 
1421   if (UseMembar) {
1422     __ fence();
1423   }
1424 
1425   //=============================================================================
1426   // Call the native method. Argument registers must not have been
1427   // overwritten since "__ call_stub(signature_handler);" (except for
1428   // ARG1 and ARG2 for static methods).
1429   __ call_c(native_method_fd);
1430 
1431   __ li(R0, 0);
1432   __ ld(R11_scratch1, 0, R1_SP);
1433   __ std(R3_RET, _ijava_state_neg(lresult), R11_scratch1);
1434   __ stfd(F1_RET, _ijava_state_neg(fresult), R11_scratch1);
1435   __ std(R0/*mirror*/, _ijava_state_neg(oop_tmp), R11_scratch1); // reset
1436 
1437   // Note: C++ interpreter needs the following here:
1438   // The frame_manager_lr field, which we use for setting the last
1439   // java frame, gets overwritten by the signature handler. Restore
1440   // it now.
1441   //__ get_PC_trash_LR(R11_scratch1);
1442   //__ std(R11_scratch1, _top_ijava_frame_abi(frame_manager_lr), R1_SP);
1443 
1444   // Because of GC R19_method may no longer be valid.
1445 
1446   // Block, if necessary, before resuming in _thread_in_Java state.
1447   // In order for GC to work, don't clear the last_Java_sp until after
1448   // blocking.
1449 
1450   //=============================================================================
1451   // Switch thread to "native transition" state before reading the
1452   // synchronization state. This additional state is necessary
1453   // because reading and testing the synchronization state is not
1454   // atomic w.r.t. GC, as this scenario demonstrates: Java thread A,
1455   // in _thread_in_native state, loads _not_synchronized and is
1456   // preempted. VM thread changes sync state to synchronizing and
1457   // suspends threads for GC. Thread A is resumed to finish this
1458   // native method, but doesn't block here since it didn't see any
1459   // synchronization in progress, and escapes.
1460 
1461   // We use release_store_fence to update values like the thread state, where
1462   // we don't want the current thread to continue until all our prior memory
1463   // accesses (including the new thread state) are visible to other threads.
1464   __ li(R0/*thread_state*/, _thread_in_native_trans);
1465   __ release();
1466   __ stw(R0/*thread_state*/, thread_(thread_state));
1467   if (UseMembar) {
1468     __ fence();
1469   }
1470   // Write serialization page so that the VM thread can do a pseudo remote
1471   // membar. We use the current thread pointer to calculate a thread
1472   // specific offset to write to within the page. This minimizes bus
1473   // traffic due to cache line collision.
1474   else {
1475     __ serialize_memory(R16_thread, R11_scratch1, R12_scratch2);
1476   }
1477 
1478   // Now before we return to java we must look for a current safepoint
1479   // (a new safepoint can not start since we entered native_trans).
1480   // We must check here because a current safepoint could be modifying
1481   // the callers registers right this moment.
1482 
1483   // Acquire isn't strictly necessary here because of the fence, but
1484   // sync_state is declared to be volatile, so we do it anyway
1485   // (cmp-br-isync on one path, release (same as acquire on PPC64) on the other path).
1486   int sync_state_offs = __ load_const_optimized(sync_state_addr, SafepointSynchronize::address_of_state(), /*temp*/R0, true);
1487 
1488   // TODO PPC port assert(4 == SafepointSynchronize::sz_state(), "unexpected field size");
1489   __ lwz(sync_state, sync_state_offs, sync_state_addr);
1490 
1491   // TODO PPC port assert(4 == Thread::sz_suspend_flags(), "unexpected field size");
1492   __ lwz(suspend_flags, thread_(suspend_flags));
1493 
1494   Label sync_check_done;
1495   Label do_safepoint;
1496   // No synchronization in progress nor yet synchronized.
1497   __ cmpwi(CCR0, sync_state, SafepointSynchronize::_not_synchronized);
1498   // Not suspended.
1499   __ cmpwi(CCR1, suspend_flags, 0);
1500 
1501   __ bne(CCR0, do_safepoint);
1502   __ beq(CCR1, sync_check_done);
1503   __ bind(do_safepoint);
1504   __ isync();
1505   // Block. We do the call directly and leave the current
1506   // last_Java_frame setup undisturbed. We must save any possible
1507   // native result across the call. No oop is present.
1508 
1509   __ mr(R3_ARG1, R16_thread);
1510 #if defined(ABI_ELFv2)
1511   __ call_c(CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans),
1512             relocInfo::none);
1513 #else
1514   __ call_c(CAST_FROM_FN_PTR(FunctionDescriptor*, JavaThread::check_special_condition_for_native_trans),
1515             relocInfo::none);
1516 #endif
1517 
1518   __ bind(sync_check_done);
1519 
1520   //=============================================================================
1521   // &lt;&lt;&lt;&lt;&lt;&lt; Back in Interpreter Frame &gt;&gt;&gt;&gt;&gt;
1522 
1523   // We are in thread_in_native_trans here and back in the normal
1524   // interpreter frame. We don't have to do anything special about
1525   // safepoints and we can switch to Java mode anytime we are ready.
1526 
1527   // Note: frame::interpreter_frame_result has a dependency on how the
1528   // method result is saved across the call to post_method_exit. For
1529   // native methods it assumes that the non-FPU/non-void result is
1530   // saved in _native_lresult and a FPU result in _native_fresult. If
1531   // this changes then the interpreter_frame_result implementation
1532   // will need to be updated too.
1533 
1534   // On PPC64, we have stored the result directly after the native call.
1535 
1536   //=============================================================================
1537   // Back in Java
1538 
1539   // We use release_store_fence to update values like the thread state, where
1540   // we don't want the current thread to continue until all our prior memory
1541   // accesses (including the new thread state) are visible to other threads.
1542   __ li(R0/*thread_state*/, _thread_in_Java);
1543   __ release();
1544   __ stw(R0/*thread_state*/, thread_(thread_state));
1545   if (UseMembar) {
1546     __ fence();
1547   }
1548 
1549   if (CheckJNICalls) {
1550     // clear_pending_jni_exception_check
1551     __ load_const_optimized(R0, 0L);
1552     __ st_ptr(R0, JavaThread::pending_jni_exception_check_fn_offset(), R16_thread);
1553   }
1554 
1555   __ reset_last_Java_frame();
1556 
1557   // Jvmdi/jvmpi support. Whether we've got an exception pending or
1558   // not, and whether unlocking throws an exception or not, we notify
1559   // on native method exit. If we do have an exception, we'll end up
1560   // in the caller's context to handle it, so if we don't do the
1561   // notify here, we'll drop it on the floor.
1562   __ notify_method_exit(true/*native method*/,
1563                         ilgl /*illegal state (not used for native methods)*/,
1564                         InterpreterMacroAssembler::NotifyJVMTI,
1565                         false /*check_exceptions*/);
1566 
1567   //=============================================================================
1568   // Handle exceptions
1569 
1570   if (synchronized) {
1571     // Don't check for exceptions since we're still in the i2n frame. Do that
1572     // manually afterwards.
1573     __ unlock_object(R26_monitor, false); // Can also unlock methods.
1574   }
1575 
1576   // Reset active handles after returning from native.
1577   // thread-&gt;active_handles()-&gt;clear();
1578   __ ld(active_handles, thread_(active_handles));
1579   // TODO PPC port assert(4 == JNIHandleBlock::top_size_in_bytes(), "unexpected field size");
1580   __ li(R0, 0);
1581   __ stw(R0, JNIHandleBlock::top_offset_in_bytes(), active_handles);
1582 
1583   Label exception_return_sync_check_already_unlocked;
1584   __ ld(R0/*pending_exception*/, thread_(pending_exception));
1585   __ cmpdi(CCR0, R0/*pending_exception*/, 0);
1586   __ bne(CCR0, exception_return_sync_check_already_unlocked);
1587 
1588   //-----------------------------------------------------------------------------
1589   // No exception pending.
1590 
1591   // Move native method result back into proper registers and return.
1592   // Invoke result handler (may unbox/promote).
1593   __ ld(R11_scratch1, 0, R1_SP);
1594   __ ld(R3_RET, _ijava_state_neg(lresult), R11_scratch1);
1595   __ lfd(F1_RET, _ijava_state_neg(fresult), R11_scratch1);
1596   __ call_stub(result_handler_addr);
1597 
1598   __ merge_frames(/*top_frame_sp*/ R21_sender_SP, /*return_pc*/ R0, R11_scratch1, R12_scratch2);
1599 
1600   // Must use the return pc which was loaded from the caller's frame
1601   // as the VM uses return-pc-patching for deoptimization.
1602   __ mtlr(R0);
1603   __ blr();
1604 
1605   //-----------------------------------------------------------------------------
1606   // An exception is pending. We call into the runtime only if the
1607   // caller was not interpreted. If it was interpreted the
1608   // interpreter will do the correct thing. If it isn't interpreted
1609   // (call stub/compiled code) we will change our return and continue.
1610 
1611   BIND(exception_return_sync_check);
1612 
1613   if (synchronized) {
1614     // Don't check for exceptions since we're still in the i2n frame. Do that
1615     // manually afterwards.
1616     __ unlock_object(R26_monitor, false); // Can also unlock methods.
1617   }
1618   BIND(exception_return_sync_check_already_unlocked);
1619 
1620   const Register return_pc = R31;
1621 
1622   __ ld(return_pc, 0, R1_SP);
1623   __ ld(return_pc, _abi(lr), return_pc);
1624 
1625   // Get the address of the exception handler.
1626   __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::exception_handler_for_return_address),
1627                   R16_thread,
1628                   return_pc /* return pc */);
1629   __ merge_frames(/*top_frame_sp*/ R21_sender_SP, noreg, R11_scratch1, R12_scratch2);
1630 
1631   // Load the PC of the the exception handler into LR.
1632   __ mtlr(R3_RET);
1633 
1634   // Load exception into R3_ARG1 and clear pending exception in thread.
1635   __ ld(R3_ARG1/*exception*/, thread_(pending_exception));
1636   __ li(R4_ARG2, 0);
1637   __ std(R4_ARG2, thread_(pending_exception));
1638 
1639   // Load the original return pc into R4_ARG2.
1640   __ mr(R4_ARG2/*issuing_pc*/, return_pc);
1641 
1642   // Return to exception handler.
1643   __ blr();
1644 
1645   //=============================================================================
1646   // Counter overflow.
1647 
1648   if (inc_counter) {
1649     // Handle invocation counter overflow.
1650     __ bind(invocation_counter_overflow);
1651 
1652     generate_counter_overflow(continue_after_compile);
1653   }
1654 
1655   return entry;
1656 }
1657 
1658 // Generic interpreted method entry to (asm) interpreter.
1659 //
1660 address TemplateInterpreterGenerator::generate_normal_entry(bool synchronized) {
1661   bool inc_counter = UseCompiler || CountCompiledCalls || LogTouchedMethods;
1662   address entry = __ pc();
1663   // Generate the code to allocate the interpreter stack frame.
1664   Register Rsize_of_parameters = R4_ARG2, // Written by generate_fixed_frame.
1665            Rsize_of_locals     = R5_ARG3; // Written by generate_fixed_frame.
1666 
1667   // Does also a stack check to assure this frame fits on the stack.
1668   generate_fixed_frame(false, Rsize_of_parameters, Rsize_of_locals);
1669 
1670   // --------------------------------------------------------------------------
1671   // Zero out non-parameter locals.
1672   // Note: *Always* zero out non-parameter locals as Sparc does. It's not
1673   // worth to ask the flag, just do it.
1674   Register Rslot_addr = R6_ARG4,
1675            Rnum       = R7_ARG5;
1676   Label Lno_locals, Lzero_loop;
1677 
1678   // Set up the zeroing loop.
1679   __ subf(Rnum, Rsize_of_parameters, Rsize_of_locals);
1680   __ subf(Rslot_addr, Rsize_of_parameters, R18_locals);
1681   __ srdi_(Rnum, Rnum, Interpreter::logStackElementSize);
1682   __ beq(CCR0, Lno_locals);
1683   __ li(R0, 0);
1684   __ mtctr(Rnum);
1685 
1686   // The zero locals loop.
1687   __ bind(Lzero_loop);
1688   __ std(R0, 0, Rslot_addr);
1689   __ addi(Rslot_addr, Rslot_addr, -Interpreter::stackElementSize);
1690   __ bdnz(Lzero_loop);
1691 
1692   __ bind(Lno_locals);
1693 
1694   // --------------------------------------------------------------------------
1695   // Counter increment and overflow check.
1696   Label invocation_counter_overflow,
1697         profile_method,
1698         profile_method_continue;
1699   if (inc_counter || ProfileInterpreter) {
1700 
1701     Register Rdo_not_unlock_if_synchronized_addr = R11_scratch1;
1702     if (synchronized) {
1703       // Since at this point in the method invocation the exception handler
1704       // would try to exit the monitor of synchronized methods which hasn't
1705       // been entered yet, we set the thread local variable
1706       // _do_not_unlock_if_synchronized to true. If any exception was thrown by
1707       // runtime, exception handling i.e. unlock_if_synchronized_method will
1708       // check this thread local flag.
1709       // This flag has two effects, one is to force an unwind in the topmost
1710       // interpreter frame and not perform an unlock while doing so.
1711       __ li(R0, 1);
1712       __ stb(R0, in_bytes(JavaThread::do_not_unlock_if_synchronized_offset()), R16_thread);
1713     }
1714 
1715     // Argument and return type profiling.
1716     __ profile_parameters_type(R3_ARG1, R4_ARG2, R5_ARG3, R6_ARG4);
1717 
1718     // Increment invocation counter and check for overflow.
1719     if (inc_counter) {
1720       generate_counter_incr(&amp;invocation_counter_overflow, &amp;profile_method, &amp;profile_method_continue);
1721     }
1722 
1723     __ bind(profile_method_continue);
1724   }
1725 
1726   bang_stack_shadow_pages(false);
1727 
1728   if (inc_counter || ProfileInterpreter) {
1729     // Reset the _do_not_unlock_if_synchronized flag.
1730     if (synchronized) {
1731       __ li(R0, 0);
1732       __ stb(R0, in_bytes(JavaThread::do_not_unlock_if_synchronized_offset()), R16_thread);
1733     }
1734   }
1735 
1736   // --------------------------------------------------------------------------
1737   // Locking of synchronized methods. Must happen AFTER invocation_counter
1738   // check and stack overflow check, so method is not locked if overflows.
1739   if (synchronized) {
1740     lock_method(R3_ARG1, R4_ARG2, R5_ARG3);
1741   }
1742 #ifdef ASSERT
1743   else {
1744     Label Lok;
1745     __ lwz(R0, in_bytes(Method::access_flags_offset()), R19_method);
1746     __ andi_(R0, R0, JVM_ACC_SYNCHRONIZED);
1747     __ asm_assert_eq("method needs synchronization", 0x8521);
1748     __ bind(Lok);
1749   }
1750 #endif // ASSERT
1751 
1752   __ verify_thread();
1753 
1754   // --------------------------------------------------------------------------
1755   // JVMTI support
1756   __ notify_method_entry();
1757 
1758   // --------------------------------------------------------------------------
1759   // Start executing instructions.
1760   __ dispatch_next(vtos);
1761 
1762   // --------------------------------------------------------------------------
1763   // Out of line counter overflow and MDO creation code.
1764   if (ProfileInterpreter) {
1765     // We have decided to profile this method in the interpreter.
1766     __ bind(profile_method);
1767     __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::profile_method));
1768     __ set_method_data_pointer_for_bcp();
1769     __ b(profile_method_continue);
1770   }
1771 
1772   if (inc_counter) {
1773     // Handle invocation counter overflow.
1774     __ bind(invocation_counter_overflow);
1775     generate_counter_overflow(profile_method_continue);
1776   }
1777   return entry;
1778 }
1779 
1780 // CRC32 Intrinsics.
1781 //
1782 // Contract on scratch and work registers.
1783 // =======================================
1784 //
1785 // On ppc, the register set {R2..R12} is available in the interpreter as scratch/work registers.
1786 // You should, however, keep in mind that {R3_ARG1..R10_ARG8} is the C-ABI argument register set.
1787 // You can't rely on these registers across calls.
1788 //
1789 // The generators for CRC32_update and for CRC32_updateBytes use the
1790 // scratch/work register set internally, passing the work registers
1791 // as arguments to the MacroAssembler emitters as required.
1792 //
1793 // R3_ARG1..R6_ARG4 are preset to hold the incoming java arguments.
1794 // Their contents is not constant but may change according to the requirements
1795 // of the emitted code.
1796 //
1797 // All other registers from the scratch/work register set are used "internally"
1798 // and contain garbage (i.e. unpredictable values) once blr() is reached.
1799 // Basically, only R3_RET contains a defined value which is the function result.
1800 //
1801 /**
1802  * Method entry for static native methods:
1803  *   int java.util.zip.CRC32.update(int crc, int b)
1804  */
1805 address TemplateInterpreterGenerator::generate_CRC32_update_entry() {
1806   if (UseCRC32Intrinsics) {
1807     address start = __ pc();  // Remember stub start address (is rtn value).
1808     Label slow_path;
1809 
1810     // Safepoint check
1811     const Register sync_state = R11_scratch1;
1812     int sync_state_offs = __ load_const_optimized(sync_state, SafepointSynchronize::address_of_state(), /*temp*/R0, true);
1813     __ lwz(sync_state, sync_state_offs, sync_state);
1814     __ cmpwi(CCR0, sync_state, SafepointSynchronize::_not_synchronized);
1815     __ bne(CCR0, slow_path);
1816 
1817     // We don't generate local frame and don't align stack because
1818     // we not even call stub code (we generate the code inline)
1819     // and there is no safepoint on this path.
1820 
1821     // Load java parameters.
1822     // R15_esp is callers operand stack pointer, i.e. it points to the parameters.
1823     const Register argP    = R15_esp;
1824     const Register crc     = R3_ARG1;  // crc value
1825     const Register data    = R4_ARG2;  // address of java byte value (kernel_crc32 needs address)
1826     const Register dataLen = R5_ARG3;  // source data len (1 byte). Not used because calling the single-byte emitter.
1827     const Register table   = R6_ARG4;  // address of crc32 table
1828     const Register tmp     = dataLen;  // Reuse unused len register to show we don't actually need a separate tmp here.
1829 
1830     BLOCK_COMMENT("CRC32_update {");
1831 
1832     // Arguments are reversed on java expression stack
1833 #ifdef VM_LITTLE_ENDIAN
1834     __ addi(data, argP, 0+1*wordSize); // (stack) address of byte value. Emitter expects address, not value.
1835                                        // Being passed as an int, the single byte is at offset +0.
1836 #else
1837     __ addi(data, argP, 3+1*wordSize); // (stack) address of byte value. Emitter expects address, not value.
1838                                        // Being passed from java as an int, the single byte is at offset +3.
1839 #endif
1840     __ lwz(crc,  2*wordSize, argP);    // Current crc state, zero extend to 64 bit to have a clean register.
1841 
1842     StubRoutines::ppc64::generate_load_crc_table_addr(_masm, table);
1843     __ kernel_crc32_singleByte(crc, data, dataLen, table, tmp);
1844 
1845     // Restore caller sp for c2i case and return.
1846     __ mr(R1_SP, R21_sender_SP); // Cut the stack back to where the caller started.
1847     __ blr();
1848 
1849     // Generate a vanilla native entry as the slow path.
1850     BLOCK_COMMENT("} CRC32_update");
1851     BIND(slow_path);
1852     __ jump_to_entry(Interpreter::entry_for_kind(Interpreter::native), R11_scratch1);
1853     return start;
1854   }
1855 
1856   return NULL;
1857 }
1858 
1859 // CRC32 Intrinsics.
1860 /**
1861  * Method entry for static native methods:
1862  *   int java.util.zip.CRC32.updateBytes(     int crc, byte[] b,  int off, int len)
1863  *   int java.util.zip.CRC32.updateByteBuffer(int crc, long* buf, int off, int len)
1864  */
1865 address TemplateInterpreterGenerator::generate_CRC32_updateBytes_entry(AbstractInterpreter::MethodKind kind) {
1866   if (UseCRC32Intrinsics) {
1867     address start = __ pc();  // Remember stub start address (is rtn value).
1868     Label slow_path;
1869 
1870     // Safepoint check
1871     const Register sync_state = R11_scratch1;
1872     int sync_state_offs = __ load_const_optimized(sync_state, SafepointSynchronize::address_of_state(), /*temp*/R0, true);
1873     __ lwz(sync_state, sync_state_offs, sync_state);
1874     __ cmpwi(CCR0, sync_state, SafepointSynchronize::_not_synchronized);
1875     __ bne(CCR0, slow_path);
1876 
1877     // We don't generate local frame and don't align stack because
1878     // we not even call stub code (we generate the code inline)
1879     // and there is no safepoint on this path.
1880 
1881     // Load parameters.
1882     // Z_esp is callers operand stack pointer, i.e. it points to the parameters.
1883     const Register argP    = R15_esp;
1884     const Register crc     = R3_ARG1;  // crc value
1885     const Register data    = R4_ARG2;  // address of java byte array
1886     const Register dataLen = R5_ARG3;  // source data len
1887     const Register table   = R6_ARG4;  // address of crc32 table
1888 
1889     const Register t0      = R9;       // scratch registers for crc calculation
1890     const Register t1      = R10;
1891     const Register t2      = R11;
1892     const Register t3      = R12;
1893 
1894     const Register tc0     = R2;       // registers to hold pre-calculated column addresses
1895     const Register tc1     = R7;
1896     const Register tc2     = R8;
1897     const Register tc3     = table;    // table address is reconstructed at the end of kernel_crc32_* emitters
1898 
1899     const Register tmp     = t0;       // Only used very locally to calculate byte buffer address.
1900 
1901     // Arguments are reversed on java expression stack.
1902     // Calculate address of start element.
1903     if (kind == Interpreter::java_util_zip_CRC32_updateByteBuffer) { // Used for "updateByteBuffer direct".
1904       BLOCK_COMMENT("CRC32_updateByteBuffer {");
1905       // crc     @ (SP + 5W) (32bit)
1906       // buf     @ (SP + 3W) (64bit ptr to long array)
1907       // off     @ (SP + 2W) (32bit)
1908       // dataLen @ (SP + 1W) (32bit)
1909       // data = buf + off
1910       __ ld(  data,    3*wordSize, argP);  // start of byte buffer
1911       __ lwa( tmp,     2*wordSize, argP);  // byte buffer offset
1912       __ lwa( dataLen, 1*wordSize, argP);  // #bytes to process
1913       __ lwz( crc,     5*wordSize, argP);  // current crc state
1914       __ add( data, data, tmp);            // Add byte buffer offset.
1915     } else {                                                         // Used for "updateBytes update".
1916       BLOCK_COMMENT("CRC32_updateBytes {");
1917       // crc     @ (SP + 4W) (32bit)
1918       // buf     @ (SP + 3W) (64bit ptr to byte array)
1919       // off     @ (SP + 2W) (32bit)
1920       // dataLen @ (SP + 1W) (32bit)
1921       // data = buf + off + base_offset
1922       __ ld(  data,    3*wordSize, argP);  // start of byte buffer
1923       __ lwa( tmp,     2*wordSize, argP);  // byte buffer offset
1924       __ lwa( dataLen, 1*wordSize, argP);  // #bytes to process
1925       __ add( data, data, tmp);            // add byte buffer offset
1926       __ lwz( crc,     4*wordSize, argP);  // current crc state
1927       __ addi(data, data, arrayOopDesc::base_offset_in_bytes(T_BYTE));
1928     }
1929 
1930     StubRoutines::ppc64::generate_load_crc_table_addr(_masm, table);
1931 
1932     // Performance measurements show the 1word and 2word variants to be almost equivalent,
1933     // with very light advantages for the 1word variant. We chose the 1word variant for
1934     // code compactness.
1935     __ kernel_crc32_1word(crc, data, dataLen, table, t0, t1, t2, t3, tc0, tc1, tc2, tc3);
1936 
1937     // Restore caller sp for c2i case and return.
1938     __ mr(R1_SP, R21_sender_SP); // Cut the stack back to where the caller started.
1939     __ blr();
1940 
1941     // Generate a vanilla native entry as the slow path.
1942     BLOCK_COMMENT("} CRC32_updateBytes(Buffer)");
1943     BIND(slow_path);
1944     __ jump_to_entry(Interpreter::entry_for_kind(Interpreter::native), R11_scratch1);
1945     return start;
1946   }
1947 
1948   return NULL;
1949 }
1950 
1951 // Not supported
1952 address TemplateInterpreterGenerator::generate_CRC32C_updateBytes_entry(AbstractInterpreter::MethodKind kind) {
1953   return NULL;
1954 }
1955 
1956 // =============================================================================
1957 // Exceptions
1958 
1959 void TemplateInterpreterGenerator::generate_throw_exception() {
1960   Register Rexception    = R17_tos,
1961            Rcontinuation = R3_RET;
1962 
1963   // --------------------------------------------------------------------------
1964   // Entry point if an method returns with a pending exception (rethrow).
1965   Interpreter::_rethrow_exception_entry = __ pc();
1966   {
1967     __ restore_interpreter_state(R11_scratch1); // Sets R11_scratch1 = fp.
1968     __ ld(R12_scratch2, _ijava_state_neg(top_frame_sp), R11_scratch1);
1969     __ resize_frame_absolute(R12_scratch2, R11_scratch1, R0);
1970 
1971     // Compiled code destroys templateTableBase, reload.
1972     __ load_const_optimized(R25_templateTableBase, (address)Interpreter::dispatch_table((TosState)0), R11_scratch1);
1973   }
1974 
1975   // Entry point if a interpreted method throws an exception (throw).
1976   Interpreter::_throw_exception_entry = __ pc();
1977   {
1978     __ mr(Rexception, R3_RET);
1979 
1980     __ verify_thread();
1981     __ verify_oop(Rexception);
1982 
1983     // Expression stack must be empty before entering the VM in case of an exception.
1984     __ empty_expression_stack();
1985     // Find exception handler address and preserve exception oop.
1986     // Call C routine to find handler and jump to it.
1987     __ call_VM(Rexception, CAST_FROM_FN_PTR(address, InterpreterRuntime::exception_handler_for_exception), Rexception);
1988     __ mtctr(Rcontinuation);
1989     // Push exception for exception handler bytecodes.
1990     __ push_ptr(Rexception);
1991 
1992     // Jump to exception handler (may be remove activation entry!).
1993     __ bctr();
1994   }
1995 
1996   // If the exception is not handled in the current frame the frame is
1997   // removed and the exception is rethrown (i.e. exception
1998   // continuation is _rethrow_exception).
1999   //
2000   // Note: At this point the bci is still the bxi for the instruction
2001   // which caused the exception and the expression stack is
2002   // empty. Thus, for any VM calls at this point, GC will find a legal
2003   // oop map (with empty expression stack).
2004 
2005   // In current activation
2006   // tos: exception
2007   // bcp: exception bcp
2008 
2009   // --------------------------------------------------------------------------
2010   // JVMTI PopFrame support
2011 
2012   Interpreter::_remove_activation_preserving_args_entry = __ pc();
2013   {
2014     // Set the popframe_processing bit in popframe_condition indicating that we are
2015     // currently handling popframe, so that call_VMs that may happen later do not
2016     // trigger new popframe handling cycles.
2017     __ lwz(R11_scratch1, in_bytes(JavaThread::popframe_condition_offset()), R16_thread);
2018     __ ori(R11_scratch1, R11_scratch1, JavaThread::popframe_processing_bit);
2019     __ stw(R11_scratch1, in_bytes(JavaThread::popframe_condition_offset()), R16_thread);
2020 
2021     // Empty the expression stack, as in normal exception handling.
2022     __ empty_expression_stack();
2023     __ unlock_if_synchronized_method(vtos, /* throw_monitor_exception */ false, /* install_monitor_exception */ false);
2024 
2025     // Check to see whether we are returning to a deoptimized frame.
2026     // (The PopFrame call ensures that the caller of the popped frame is
2027     // either interpreted or compiled and deoptimizes it if compiled.)
2028     // Note that we don't compare the return PC against the
2029     // deoptimization blob's unpack entry because of the presence of
2030     // adapter frames in C2.
2031     Label Lcaller_not_deoptimized;
2032     Register return_pc = R3_ARG1;
2033     __ ld(return_pc, 0, R1_SP);
2034     __ ld(return_pc, _abi(lr), return_pc);
2035     __ call_VM_leaf(CAST_FROM_FN_PTR(address, InterpreterRuntime::interpreter_contains), return_pc);
2036     __ cmpdi(CCR0, R3_RET, 0);
2037     __ bne(CCR0, Lcaller_not_deoptimized);
2038 
2039     // The deoptimized case.
2040     // In this case, we can't call dispatch_next() after the frame is
2041     // popped, but instead must save the incoming arguments and restore
2042     // them after deoptimization has occurred.
2043     __ ld(R4_ARG2, in_bytes(Method::const_offset()), R19_method);
2044     __ lhz(R4_ARG2 /* number of params */, in_bytes(ConstMethod::size_of_parameters_offset()), R4_ARG2);
2045     __ slwi(R4_ARG2, R4_ARG2, Interpreter::logStackElementSize);
2046     __ addi(R5_ARG3, R18_locals, Interpreter::stackElementSize);
2047     __ subf(R5_ARG3, R4_ARG2, R5_ARG3);
2048     // Save these arguments.
2049     __ call_VM_leaf(CAST_FROM_FN_PTR(address, Deoptimization::popframe_preserve_args), R16_thread, R4_ARG2, R5_ARG3);
2050 
2051     // Inform deoptimization that it is responsible for restoring these arguments.
2052     __ load_const_optimized(R11_scratch1, JavaThread::popframe_force_deopt_reexecution_bit);
2053     __ stw(R11_scratch1, in_bytes(JavaThread::popframe_condition_offset()), R16_thread);
2054 
2055     // Return from the current method into the deoptimization blob. Will eventually
2056     // end up in the deopt interpeter entry, deoptimization prepared everything that
2057     // we will reexecute the call that called us.
2058     __ merge_frames(/*top_frame_sp*/ R21_sender_SP, /*reload return_pc*/ return_pc, R11_scratch1, R12_scratch2);
2059     __ mtlr(return_pc);
2060     __ blr();
2061 
2062     // The non-deoptimized case.
2063     __ bind(Lcaller_not_deoptimized);
2064 
2065     // Clear the popframe condition flag.
2066     __ li(R0, 0);
2067     __ stw(R0, in_bytes(JavaThread::popframe_condition_offset()), R16_thread);
2068 
2069     // Get out of the current method and re-execute the call that called us.
2070     __ merge_frames(/*top_frame_sp*/ R21_sender_SP, /*return_pc*/ noreg, R11_scratch1, R12_scratch2);
2071     __ restore_interpreter_state(R11_scratch1);
2072     __ ld(R12_scratch2, _ijava_state_neg(top_frame_sp), R11_scratch1);
2073     __ resize_frame_absolute(R12_scratch2, R11_scratch1, R0);
2074     if (ProfileInterpreter) {
2075       __ set_method_data_pointer_for_bcp();
2076       __ ld(R11_scratch1, 0, R1_SP);
2077       __ std(R28_mdx, _ijava_state_neg(mdx), R11_scratch1);
2078     }
2079 #if INCLUDE_JVMTI
2080     Label L_done;
2081 
2082     __ lbz(R11_scratch1, 0, R14_bcp);
2083     __ cmpwi(CCR0, R11_scratch1, Bytecodes::_invokestatic);
2084     __ bne(CCR0, L_done);
2085 
2086     // The member name argument must be restored if _invokestatic is re-executed after a PopFrame call.
2087     // Detect such a case in the InterpreterRuntime function and return the member name argument, or NULL.
2088     __ ld(R4_ARG2, 0, R18_locals);
2089     __ MacroAssembler::call_VM(R4_ARG2, CAST_FROM_FN_PTR(address, InterpreterRuntime::member_name_arg_or_null), R4_ARG2, R19_method, R14_bcp, false);
2090     __ restore_interpreter_state(R11_scratch1, /*bcp_and_mdx_only*/ true);
2091     __ cmpdi(CCR0, R4_ARG2, 0);
2092     __ beq(CCR0, L_done);
2093     __ std(R4_ARG2, wordSize, R15_esp);
2094     __ bind(L_done);
2095 #endif // INCLUDE_JVMTI
2096     __ dispatch_next(vtos);
2097   }
2098   // end of JVMTI PopFrame support
2099 
2100   // --------------------------------------------------------------------------
2101   // Remove activation exception entry.
2102   // This is jumped to if an interpreted method can't handle an exception itself
2103   // (we come from the throw/rethrow exception entry above). We're going to call
2104   // into the VM to find the exception handler in the caller, pop the current
2105   // frame and return the handler we calculated.
2106   Interpreter::_remove_activation_entry = __ pc();
2107   {
2108     __ pop_ptr(Rexception);
2109     __ verify_thread();
2110     __ verify_oop(Rexception);
2111     __ std(Rexception, in_bytes(JavaThread::vm_result_offset()), R16_thread);
2112 
2113     __ unlock_if_synchronized_method(vtos, /* throw_monitor_exception */ false, true);
2114     __ notify_method_exit(false, vtos, InterpreterMacroAssembler::SkipNotifyJVMTI, false);
2115 
2116     __ get_vm_result(Rexception);
2117 
2118     // We are done with this activation frame; find out where to go next.
2119     // The continuation point will be an exception handler, which expects
2120     // the following registers set up:
2121     //
2122     // RET:  exception oop
2123     // ARG2: Issuing PC (see generate_exception_blob()), only used if the caller is compiled.
2124 
2125     Register return_pc = R31; // Needs to survive the runtime call.
2126     __ ld(return_pc, 0, R1_SP);
2127     __ ld(return_pc, _abi(lr), return_pc);
2128     __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::exception_handler_for_return_address), R16_thread, return_pc);
2129 
2130     // Remove the current activation.
2131     __ merge_frames(/*top_frame_sp*/ R21_sender_SP, /*return_pc*/ noreg, R11_scratch1, R12_scratch2);
2132 
2133     __ mr(R4_ARG2, return_pc);
2134     __ mtlr(R3_RET);
2135     __ mr(R3_RET, Rexception);
2136     __ blr();
2137   }
2138 }
2139 
2140 // JVMTI ForceEarlyReturn support.
2141 // Returns "in the middle" of a method with a "fake" return value.
2142 address TemplateInterpreterGenerator::generate_earlyret_entry_for(TosState state) {
2143 
2144   Register Rscratch1 = R11_scratch1,
2145            Rscratch2 = R12_scratch2;
2146 
2147   address entry = __ pc();
2148   __ empty_expression_stack();
2149 
2150   __ load_earlyret_value(state, Rscratch1);
2151 
2152   __ ld(Rscratch1, in_bytes(JavaThread::jvmti_thread_state_offset()), R16_thread);
2153   // Clear the earlyret state.
2154   __ li(R0, 0);
2155   __ stw(R0, in_bytes(JvmtiThreadState::earlyret_state_offset()), Rscratch1);
2156 
2157   __ remove_activation(state, false, false);
2158   // Copied from TemplateTable::_return.
2159   // Restoration of lr done by remove_activation.
2160   switch (state) {
2161     // Narrow result if state is itos but result type is smaller.
2162     case btos:
2163     case ztos:
2164     case ctos:
2165     case stos:
2166     case itos: __ narrow(R17_tos); /* fall through */
2167     case ltos:
2168     case atos: __ mr(R3_RET, R17_tos); break;
2169     case ftos:
2170     case dtos: __ fmr(F1_RET, F15_ftos); break;
2171     case vtos: // This might be a constructor. Final fields (and volatile fields on PPC64) need
2172                // to get visible before the reference to the object gets stored anywhere.
2173                __ membar(Assembler::StoreStore); break;
2174     default  : ShouldNotReachHere();
2175   }
2176   __ blr();
2177 
2178   return entry;
2179 } // end of ForceEarlyReturn support
2180 
2181 //-----------------------------------------------------------------------------
2182 // Helper for vtos entry point generation
2183 
2184 void TemplateInterpreterGenerator::set_vtos_entry_points(Template* t,
2185                                                          address&amp; bep,
2186                                                          address&amp; cep,
2187                                                          address&amp; sep,
2188                                                          address&amp; aep,
2189                                                          address&amp; iep,
2190                                                          address&amp; lep,
2191                                                          address&amp; fep,
2192                                                          address&amp; dep,
2193                                                          address&amp; vep) {
2194   assert(t-&gt;is_valid() &amp;&amp; t-&gt;tos_in() == vtos, "illegal template");
2195   Label L;
2196 
2197   aep = __ pc();  __ push_ptr();  __ b(L);
2198   fep = __ pc();  __ push_f();    __ b(L);
2199   dep = __ pc();  __ push_d();    __ b(L);
2200   lep = __ pc();  __ push_l();    __ b(L);
2201   __ align(32, 12, 24); // align L
2202   bep = cep = sep =
2203   iep = __ pc();  __ push_i();
2204   vep = __ pc();
2205   __ bind(L);
2206   generate_and_dispatch(t);
2207 }
2208 
2209 //-----------------------------------------------------------------------------
2210 
2211 // Non-product code
2212 #ifndef PRODUCT
2213 address TemplateInterpreterGenerator::generate_trace_code(TosState state) {
2214   //__ flush_bundle();
2215   address entry = __ pc();
2216 
2217   const char *bname = NULL;
2218   uint tsize = 0;
2219   switch(state) {
2220   case ftos:
2221     bname = "trace_code_ftos {";
2222     tsize = 2;
2223     break;
2224   case btos:
2225     bname = "trace_code_btos {";
2226     tsize = 2;
2227     break;
2228   case ztos:
2229     bname = "trace_code_ztos {";
2230     tsize = 2;
2231     break;
2232   case ctos:
2233     bname = "trace_code_ctos {";
2234     tsize = 2;
2235     break;
2236   case stos:
2237     bname = "trace_code_stos {";
2238     tsize = 2;
2239     break;
2240   case itos:
2241     bname = "trace_code_itos {";
2242     tsize = 2;
2243     break;
2244   case ltos:
2245     bname = "trace_code_ltos {";
2246     tsize = 3;
2247     break;
2248   case atos:
2249     bname = "trace_code_atos {";
2250     tsize = 2;
2251     break;
2252   case vtos:
2253     // Note: In case of vtos, the topmost of stack value could be a int or doubl
2254     // In case of a double (2 slots) we won't see the 2nd stack value.
2255     // Maybe we simply should print the topmost 3 stack slots to cope with the problem.
2256     bname = "trace_code_vtos {";
2257     tsize = 2;
2258 
2259     break;
2260   case dtos:
2261     bname = "trace_code_dtos {";
2262     tsize = 3;
2263     break;
2264   default:
2265     ShouldNotReachHere();
2266   }
2267   BLOCK_COMMENT(bname);
2268 
2269   // Support short-cut for TraceBytecodesAt.
2270   // Don't call into the VM if we don't want to trace to speed up things.
2271   Label Lskip_vm_call;
2272   if (TraceBytecodesAt &gt; 0 &amp;&amp; TraceBytecodesAt &lt; max_intx) {
2273     int offs1 = __ load_const_optimized(R11_scratch1, (address) &amp;TraceBytecodesAt, R0, true);
2274     int offs2 = __ load_const_optimized(R12_scratch2, (address) &amp;BytecodeCounter::_counter_value, R0, true);
2275     __ ld(R11_scratch1, offs1, R11_scratch1);
2276     __ lwa(R12_scratch2, offs2, R12_scratch2);
2277     __ cmpd(CCR0, R12_scratch2, R11_scratch1);
2278     __ blt(CCR0, Lskip_vm_call);
2279   }
2280 
2281   __ push(state);
2282   // Load 2 topmost expression stack values.
2283   __ ld(R6_ARG4, tsize*Interpreter::stackElementSize, R15_esp);
2284   __ ld(R5_ARG3, Interpreter::stackElementSize, R15_esp);
2285   __ mflr(R31);
2286   __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::trace_bytecode), /* unused */ R4_ARG2, R5_ARG3, R6_ARG4, false);
2287   __ mtlr(R31);
2288   __ pop(state);
2289 
2290   if (TraceBytecodesAt &gt; 0 &amp;&amp; TraceBytecodesAt &lt; max_intx) {
2291     __ bind(Lskip_vm_call);
2292   }
2293   __ blr();
2294   BLOCK_COMMENT("} trace_code");
2295   return entry;
2296 }
2297 
2298 void TemplateInterpreterGenerator::count_bytecode() {
2299   int offs = __ load_const_optimized(R11_scratch1, (address) &amp;BytecodeCounter::_counter_value, R12_scratch2, true);
2300   __ lwz(R12_scratch2, offs, R11_scratch1);
2301   __ addi(R12_scratch2, R12_scratch2, 1);
2302   __ stw(R12_scratch2, offs, R11_scratch1);
2303 }
2304 
2305 void TemplateInterpreterGenerator::histogram_bytecode(Template* t) {
2306   int offs = __ load_const_optimized(R11_scratch1, (address) &amp;BytecodeHistogram::_counters[t-&gt;bytecode()], R12_scratch2, true);
2307   __ lwz(R12_scratch2, offs, R11_scratch1);
2308   __ addi(R12_scratch2, R12_scratch2, 1);
2309   __ stw(R12_scratch2, offs, R11_scratch1);
2310 }
2311 
2312 void TemplateInterpreterGenerator::histogram_bytecode_pair(Template* t) {
2313   const Register addr = R11_scratch1,
2314                  tmp  = R12_scratch2;
2315   // Get index, shift out old bytecode, bring in new bytecode, and store it.
2316   // _index = (_index &gt;&gt; log2_number_of_codes) |
2317   //          (bytecode &lt;&lt; log2_number_of_codes);
2318   int offs1 = __ load_const_optimized(addr, (address)&amp;BytecodePairHistogram::_index, tmp, true);
2319   __ lwz(tmp, offs1, addr);
2320   __ srwi(tmp, tmp, BytecodePairHistogram::log2_number_of_codes);
2321   __ ori(tmp, tmp, ((int) t-&gt;bytecode()) &lt;&lt; BytecodePairHistogram::log2_number_of_codes);
2322   __ stw(tmp, offs1, addr);
2323 
2324   // Bump bucket contents.
2325   // _counters[_index] ++;
2326   int offs2 = __ load_const_optimized(addr, (address)&amp;BytecodePairHistogram::_counters, R0, true);
2327   __ sldi(tmp, tmp, LogBytesPerInt);
2328   __ add(addr, tmp, addr);
2329   __ lwz(tmp, offs2, addr);
2330   __ addi(tmp, tmp, 1);
2331   __ stw(tmp, offs2, addr);
2332 }
2333 
2334 void TemplateInterpreterGenerator::trace_bytecode(Template* t) {
2335   // Call a little run-time stub to avoid blow-up for each bytecode.
2336   // The run-time runtime saves the right registers, depending on
2337   // the tosca in-state for the given template.
2338 
2339   assert(Interpreter::trace_code(t-&gt;tos_in()) != NULL,
2340          "entry must have been generated");
2341 
2342   // Note: we destroy LR here.
2343   __ bl(Interpreter::trace_code(t-&gt;tos_in()));
2344 }
2345 
2346 void TemplateInterpreterGenerator::stop_interpreter_at() {
2347   Label L;
2348   int offs1 = __ load_const_optimized(R11_scratch1, (address) &amp;StopInterpreterAt, R0, true);
2349   int offs2 = __ load_const_optimized(R12_scratch2, (address) &amp;BytecodeCounter::_counter_value, R0, true);
2350   __ ld(R11_scratch1, offs1, R11_scratch1);
2351   __ lwa(R12_scratch2, offs2, R12_scratch2);
2352   __ cmpd(CCR0, R12_scratch2, R11_scratch1);
2353   __ bne(CCR0, L);
2354   __ illtrap();
2355   __ bind(L);
2356 }
2357 
2358 #endif // !PRODUCT
<a name="2" id="anc2"></a><b style="font-size: large; color: red">--- EOF ---</b>















































































</pre><form name="eof"><input name="value" value="2" type="hidden" /></form></body></html>
