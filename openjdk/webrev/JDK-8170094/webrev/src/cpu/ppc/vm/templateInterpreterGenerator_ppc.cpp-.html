<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta charset="utf-8">
<meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

<title>Old src/cpu/ppc/vm/templateInterpreterGenerator_ppc.cpp</title>
<body id="SUNWwebrev">
<pre>
   1 /*
   2  * Copyright (c) 2014, 2016, Oracle and/or its affiliates. All rights reserved.
   3  * Copyright (c) 2015, 2016 SAP SE. All rights reserved.
   4  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   5  *
   6  * This code is free software; you can redistribute it and/or modify it
   7  * under the terms of the GNU General Public License version 2 only, as
   8  * published by the Free Software Foundation.
   9  *
  10  * This code is distributed in the hope that it will be useful, but WITHOUT
  11  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  12  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  13  * version 2 for more details (a copy is included in the LICENSE file that
  14  * accompanied this code).
  15  *
  16  * You should have received a copy of the GNU General Public License version
  17  * 2 along with this work; if not, write to the Free Software Foundation,
  18  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  19  *
  20  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  21  * or visit www.oracle.com if you need additional information or have any
  22  * questions.
  23  *
  24  */
  25 
  26 #include "precompiled.hpp"
  27 #include "asm/macroAssembler.inline.hpp"
  28 #include "interpreter/bytecodeHistogram.hpp"
  29 #include "interpreter/interpreter.hpp"
  30 #include "interpreter/interpreterRuntime.hpp"
  31 #include "interpreter/interp_masm.hpp"
  32 #include "interpreter/templateInterpreterGenerator.hpp"
  33 #include "interpreter/templateTable.hpp"
  34 #include "oops/arrayOop.hpp"
  35 #include "oops/methodData.hpp"
  36 #include "oops/method.hpp"
  37 #include "oops/oop.inline.hpp"
  38 #include "prims/jvmtiExport.hpp"
  39 #include "prims/jvmtiThreadState.hpp"
  40 #include "runtime/arguments.hpp"
  41 #include "runtime/deoptimization.hpp"
  42 #include "runtime/frame.inline.hpp"
  43 #include "runtime/sharedRuntime.hpp"
  44 #include "runtime/stubRoutines.hpp"
  45 #include "runtime/synchronizer.hpp"
  46 #include "runtime/timer.hpp"
  47 #include "runtime/vframeArray.hpp"
  48 #include "utilities/debug.hpp"
  49 #include "utilities/macros.hpp"
  50 
  51 #undef __
  52 #define __ _masm-&gt;
  53 
  54 // Size of interpreter code.  Increase if too small.  Interpreter will
  55 // fail with a guarantee ("not enough space for interpreter generation");
  56 // if too small.
  57 // Run with +PrintInterpreter to get the VM to print out the size.
  58 // Max size with JVMTI
  59 int TemplateInterpreter::InterpreterCodeSize = 230*K;
  60 
  61 #ifdef PRODUCT
  62 #define BLOCK_COMMENT(str) /* nothing */
  63 #else
  64 #define BLOCK_COMMENT(str) __ block_comment(str)
  65 #endif
  66 
  67 #define BIND(label)        __ bind(label); BLOCK_COMMENT(#label ":")
  68 
  69 //-----------------------------------------------------------------------------
  70 
  71 address TemplateInterpreterGenerator::generate_slow_signature_handler() {
  72   // Slow_signature handler that respects the PPC C calling conventions.
  73   //
  74   // We get called by the native entry code with our output register
  75   // area == 8. First we call InterpreterRuntime::get_result_handler
  76   // to copy the pointer to the signature string temporarily to the
  77   // first C-argument and to return the result_handler in
  78   // R3_RET. Since native_entry will copy the jni-pointer to the
  79   // first C-argument slot later on, it is OK to occupy this slot
  80   // temporarilly. Then we copy the argument list on the java
  81   // expression stack into native varargs format on the native stack
  82   // and load arguments into argument registers. Integer arguments in
  83   // the varargs vector will be sign-extended to 8 bytes.
  84   //
  85   // On entry:
  86   //   R3_ARG1        - intptr_t*     Address of java argument list in memory.
  87   //   R15_prev_state - BytecodeInterpreter* Address of interpreter state for
  88   //     this method
  89   //   R19_method
  90   //
  91   // On exit (just before return instruction):
  92   //   R3_RET            - contains the address of the result_handler.
  93   //   R4_ARG2           - is not updated for static methods and contains "this" otherwise.
  94   //   R5_ARG3-R10_ARG8: - When the (i-2)th Java argument is not of type float or double,
  95   //                       ARGi contains this argument. Otherwise, ARGi is not updated.
  96   //   F1_ARG1-F13_ARG13 - contain the first 13 arguments of type float or double.
  97 
  98   const int LogSizeOfTwoInstructions = 3;
  99 
 100   // FIXME: use Argument:: GL: Argument names different numbers!
 101   const int max_fp_register_arguments  = 13;
 102   const int max_int_register_arguments = 6;  // first 2 are reserved
 103 
 104   const Register arg_java       = R21_tmp1;
 105   const Register arg_c          = R22_tmp2;
 106   const Register signature      = R23_tmp3;  // is string
 107   const Register sig_byte       = R24_tmp4;
 108   const Register fpcnt          = R25_tmp5;
 109   const Register argcnt         = R26_tmp6;
 110   const Register intSlot        = R27_tmp7;
 111   const Register target_sp      = R28_tmp8;
 112   const FloatRegister floatSlot = F0;
 113 
 114   address entry = __ function_entry();
 115 
 116   __ save_LR_CR(R0);
 117   __ save_nonvolatile_gprs(R1_SP, _spill_nonvolatiles_neg(r14));
 118   // We use target_sp for storing arguments in the C frame.
 119   __ mr(target_sp, R1_SP);
 120   __ push_frame_reg_args_nonvolatiles(0, R11_scratch1);
 121 
 122   __ mr(arg_java, R3_ARG1);
 123 
 124   __ call_VM_leaf(CAST_FROM_FN_PTR(address, InterpreterRuntime::get_signature), R16_thread, R19_method);
 125 
 126   // Signature is in R3_RET. Signature is callee saved.
 127   __ mr(signature, R3_RET);
 128 
 129   // Get the result handler.
 130   __ call_VM_leaf(CAST_FROM_FN_PTR(address, InterpreterRuntime::get_result_handler), R16_thread, R19_method);
 131 
 132   {
 133     Label L;
 134     // test if static
 135     // _access_flags._flags must be at offset 0.
 136     // TODO PPC port: requires change in shared code.
 137     //assert(in_bytes(AccessFlags::flags_offset()) == 0,
 138     //       "MethodDesc._access_flags == MethodDesc._access_flags._flags");
 139     // _access_flags must be a 32 bit value.
 140     assert(sizeof(AccessFlags) == 4, "wrong size");
 141     __ lwa(R11_scratch1/*access_flags*/, method_(access_flags));
 142     // testbit with condition register.
 143     __ testbitdi(CCR0, R0, R11_scratch1/*access_flags*/, JVM_ACC_STATIC_BIT);
 144     __ btrue(CCR0, L);
 145     // For non-static functions, pass "this" in R4_ARG2 and copy it
 146     // to 2nd C-arg slot.
 147     // We need to box the Java object here, so we use arg_java
 148     // (address of current Java stack slot) as argument and don't
 149     // dereference it as in case of ints, floats, etc.
 150     __ mr(R4_ARG2, arg_java);
 151     __ addi(arg_java, arg_java, -BytesPerWord);
 152     __ std(R4_ARG2, _abi(carg_2), target_sp);
 153     __ bind(L);
 154   }
 155 
 156   // Will be incremented directly after loop_start. argcnt=0
 157   // corresponds to 3rd C argument.
 158   __ li(argcnt, -1);
 159   // arg_c points to 3rd C argument
 160   __ addi(arg_c, target_sp, _abi(carg_3));
 161   // no floating-point args parsed so far
 162   __ li(fpcnt, 0);
 163 
 164   Label move_intSlot_to_ARG, move_floatSlot_to_FARG;
 165   Label loop_start, loop_end;
 166   Label do_int, do_long, do_float, do_double, do_dontreachhere, do_object, do_array, do_boxed;
 167 
 168   // signature points to '(' at entry
 169 #ifdef ASSERT
 170   __ lbz(sig_byte, 0, signature);
 171   __ cmplwi(CCR0, sig_byte, '(');
 172   __ bne(CCR0, do_dontreachhere);
 173 #endif
 174 
 175   __ bind(loop_start);
 176 
 177   __ addi(argcnt, argcnt, 1);
 178   __ lbzu(sig_byte, 1, signature);
 179 
 180   __ cmplwi(CCR0, sig_byte, ')'); // end of signature
 181   __ beq(CCR0, loop_end);
 182 
 183   __ cmplwi(CCR0, sig_byte, 'B'); // byte
 184   __ beq(CCR0, do_int);
 185 
 186   __ cmplwi(CCR0, sig_byte, 'C'); // char
 187   __ beq(CCR0, do_int);
 188 
 189   __ cmplwi(CCR0, sig_byte, 'D'); // double
 190   __ beq(CCR0, do_double);
 191 
 192   __ cmplwi(CCR0, sig_byte, 'F'); // float
 193   __ beq(CCR0, do_float);
 194 
 195   __ cmplwi(CCR0, sig_byte, 'I'); // int
 196   __ beq(CCR0, do_int);
 197 
 198   __ cmplwi(CCR0, sig_byte, 'J'); // long
 199   __ beq(CCR0, do_long);
 200 
 201   __ cmplwi(CCR0, sig_byte, 'S'); // short
 202   __ beq(CCR0, do_int);
 203 
 204   __ cmplwi(CCR0, sig_byte, 'Z'); // boolean
 205   __ beq(CCR0, do_int);
 206 
 207   __ cmplwi(CCR0, sig_byte, 'L'); // object
 208   __ beq(CCR0, do_object);
 209 
 210   __ cmplwi(CCR0, sig_byte, '['); // array
 211   __ beq(CCR0, do_array);
 212 
 213   //  __ cmplwi(CCR0, sig_byte, 'V'); // void cannot appear since we do not parse the return type
 214   //  __ beq(CCR0, do_void);
 215 
 216   __ bind(do_dontreachhere);
 217 
 218   __ unimplemented("ShouldNotReachHere in slow_signature_handler", 120);
 219 
 220   __ bind(do_array);
 221 
 222   {
 223     Label start_skip, end_skip;
 224 
 225     __ bind(start_skip);
 226     __ lbzu(sig_byte, 1, signature);
 227     __ cmplwi(CCR0, sig_byte, '[');
 228     __ beq(CCR0, start_skip); // skip further brackets
 229     __ cmplwi(CCR0, sig_byte, '9');
 230     __ bgt(CCR0, end_skip);   // no optional size
 231     __ cmplwi(CCR0, sig_byte, '0');
 232     __ bge(CCR0, start_skip); // skip optional size
 233     __ bind(end_skip);
 234 
 235     __ cmplwi(CCR0, sig_byte, 'L');
 236     __ beq(CCR0, do_object);  // for arrays of objects, the name of the object must be skipped
 237     __ b(do_boxed);          // otherwise, go directly to do_boxed
 238   }
 239 
 240   __ bind(do_object);
 241   {
 242     Label L;
 243     __ bind(L);
 244     __ lbzu(sig_byte, 1, signature);
 245     __ cmplwi(CCR0, sig_byte, ';');
 246     __ bne(CCR0, L);
 247    }
 248   // Need to box the Java object here, so we use arg_java (address of
 249   // current Java stack slot) as argument and don't dereference it as
 250   // in case of ints, floats, etc.
 251   Label do_null;
 252   __ bind(do_boxed);
 253   __ ld(R0,0, arg_java);
 254   __ cmpdi(CCR0, R0, 0);
 255   __ li(intSlot,0);
 256   __ beq(CCR0, do_null);
 257   __ mr(intSlot, arg_java);
 258   __ bind(do_null);
 259   __ std(intSlot, 0, arg_c);
 260   __ addi(arg_java, arg_java, -BytesPerWord);
 261   __ addi(arg_c, arg_c, BytesPerWord);
 262   __ cmplwi(CCR0, argcnt, max_int_register_arguments);
 263   __ blt(CCR0, move_intSlot_to_ARG);
 264   __ b(loop_start);
 265 
 266   __ bind(do_int);
 267   __ lwa(intSlot, 0, arg_java);
 268   __ std(intSlot, 0, arg_c);
 269   __ addi(arg_java, arg_java, -BytesPerWord);
 270   __ addi(arg_c, arg_c, BytesPerWord);
 271   __ cmplwi(CCR0, argcnt, max_int_register_arguments);
 272   __ blt(CCR0, move_intSlot_to_ARG);
 273   __ b(loop_start);
 274 
 275   __ bind(do_long);
 276   __ ld(intSlot, -BytesPerWord, arg_java);
 277   __ std(intSlot, 0, arg_c);
 278   __ addi(arg_java, arg_java, - 2 * BytesPerWord);
 279   __ addi(arg_c, arg_c, BytesPerWord);
 280   __ cmplwi(CCR0, argcnt, max_int_register_arguments);
 281   __ blt(CCR0, move_intSlot_to_ARG);
 282   __ b(loop_start);
 283 
 284   __ bind(do_float);
 285   __ lfs(floatSlot, 0, arg_java);
 286 #if defined(LINUX)
 287   // Linux uses ELF ABI. Both original ELF and ELFv2 ABIs have float
 288   // in the least significant word of an argument slot.
 289 #if defined(VM_LITTLE_ENDIAN)
 290   __ stfs(floatSlot, 0, arg_c);
 291 #else
 292   __ stfs(floatSlot, 4, arg_c);
 293 #endif
 294 #elif defined(AIX)
 295   // Although AIX runs on big endian CPU, float is in most significant
 296   // word of an argument slot.
 297   __ stfs(floatSlot, 0, arg_c);
 298 #else
 299 #error "unknown OS"
 300 #endif
 301   __ addi(arg_java, arg_java, -BytesPerWord);
 302   __ addi(arg_c, arg_c, BytesPerWord);
 303   __ cmplwi(CCR0, fpcnt, max_fp_register_arguments);
 304   __ blt(CCR0, move_floatSlot_to_FARG);
 305   __ b(loop_start);
 306 
 307   __ bind(do_double);
 308   __ lfd(floatSlot, - BytesPerWord, arg_java);
 309   __ stfd(floatSlot, 0, arg_c);
 310   __ addi(arg_java, arg_java, - 2 * BytesPerWord);
 311   __ addi(arg_c, arg_c, BytesPerWord);
 312   __ cmplwi(CCR0, fpcnt, max_fp_register_arguments);
 313   __ blt(CCR0, move_floatSlot_to_FARG);
 314   __ b(loop_start);
 315 
 316   __ bind(loop_end);
 317 
 318   __ pop_frame();
 319   __ restore_nonvolatile_gprs(R1_SP, _spill_nonvolatiles_neg(r14));
 320   __ restore_LR_CR(R0);
 321 
 322   __ blr();
 323 
 324   Label move_int_arg, move_float_arg;
 325   __ bind(move_int_arg); // each case must consist of 2 instructions (otherwise adapt LogSizeOfTwoInstructions)
 326   __ mr(R5_ARG3, intSlot);  __ b(loop_start);
 327   __ mr(R6_ARG4, intSlot);  __ b(loop_start);
 328   __ mr(R7_ARG5, intSlot);  __ b(loop_start);
 329   __ mr(R8_ARG6, intSlot);  __ b(loop_start);
 330   __ mr(R9_ARG7, intSlot);  __ b(loop_start);
 331   __ mr(R10_ARG8, intSlot); __ b(loop_start);
 332 
 333   __ bind(move_float_arg); // each case must consist of 2 instructions (otherwise adapt LogSizeOfTwoInstructions)
 334   __ fmr(F1_ARG1, floatSlot);   __ b(loop_start);
 335   __ fmr(F2_ARG2, floatSlot);   __ b(loop_start);
 336   __ fmr(F3_ARG3, floatSlot);   __ b(loop_start);
 337   __ fmr(F4_ARG4, floatSlot);   __ b(loop_start);
 338   __ fmr(F5_ARG5, floatSlot);   __ b(loop_start);
 339   __ fmr(F6_ARG6, floatSlot);   __ b(loop_start);
 340   __ fmr(F7_ARG7, floatSlot);   __ b(loop_start);
 341   __ fmr(F8_ARG8, floatSlot);   __ b(loop_start);
 342   __ fmr(F9_ARG9, floatSlot);   __ b(loop_start);
 343   __ fmr(F10_ARG10, floatSlot); __ b(loop_start);
 344   __ fmr(F11_ARG11, floatSlot); __ b(loop_start);
 345   __ fmr(F12_ARG12, floatSlot); __ b(loop_start);
 346   __ fmr(F13_ARG13, floatSlot); __ b(loop_start);
 347 
 348   __ bind(move_intSlot_to_ARG);
 349   __ sldi(R0, argcnt, LogSizeOfTwoInstructions);
 350   __ load_const(R11_scratch1, move_int_arg); // Label must be bound here.
 351   __ add(R11_scratch1, R0, R11_scratch1);
 352   __ mtctr(R11_scratch1/*branch_target*/);
 353   __ bctr();
 354   __ bind(move_floatSlot_to_FARG);
 355   __ sldi(R0, fpcnt, LogSizeOfTwoInstructions);
 356   __ addi(fpcnt, fpcnt, 1);
 357   __ load_const(R11_scratch1, move_float_arg); // Label must be bound here.
 358   __ add(R11_scratch1, R0, R11_scratch1);
 359   __ mtctr(R11_scratch1/*branch_target*/);
 360   __ bctr();
 361 
 362   return entry;
 363 }
 364 
 365 address TemplateInterpreterGenerator::generate_result_handler_for(BasicType type) {
 366   //
 367   // Registers alive
 368   //   R3_RET
 369   //   LR
 370   //
 371   // Registers updated
 372   //   R3_RET
 373   //
 374 
 375   Label done;
 376   address entry = __ pc();
 377 
 378   switch (type) {
 379   case T_BOOLEAN:
 380     // convert !=0 to 1
 381     __ neg(R0, R3_RET);
 382     __ orr(R0, R3_RET, R0);
 383     __ srwi(R3_RET, R0, 31);
 384     break;
 385   case T_BYTE:
 386      // sign extend 8 bits
 387      __ extsb(R3_RET, R3_RET);
 388      break;
 389   case T_CHAR:
 390      // zero extend 16 bits
 391      __ clrldi(R3_RET, R3_RET, 48);
 392      break;
 393   case T_SHORT:
 394      // sign extend 16 bits
 395      __ extsh(R3_RET, R3_RET);
 396      break;
 397   case T_INT:
 398      // sign extend 32 bits
 399      __ extsw(R3_RET, R3_RET);
 400      break;
 401   case T_LONG:
 402      break;
 403   case T_OBJECT:
 404     // unbox result if not null
 405     __ cmpdi(CCR0, R3_RET, 0);
 406     __ beq(CCR0, done);
 407     __ ld(R3_RET, 0, R3_RET);
 408     __ verify_oop(R3_RET);
 409     break;
 410   case T_FLOAT:
 411      break;
 412   case T_DOUBLE:
 413      break;
 414   case T_VOID:
 415      break;
 416   default: ShouldNotReachHere();
 417   }
 418 
 419   BIND(done);
 420   __ blr();
 421 
 422   return entry;
 423 }
 424 
 425 // Abstract method entry.
 426 //
 427 address TemplateInterpreterGenerator::generate_abstract_entry(void) {
 428   address entry = __ pc();
 429 
 430   //
 431   // Registers alive
 432   //   R16_thread     - JavaThread*
 433   //   R19_method     - callee's method (method to be invoked)
 434   //   R1_SP          - SP prepared such that caller's outgoing args are near top
 435   //   LR             - return address to caller
 436   //
 437   // Stack layout at this point:
 438   //
 439   //   0       [TOP_IJAVA_FRAME_ABI]         &lt;-- R1_SP
 440   //           alignment (optional)
 441   //           [outgoing Java arguments]
 442   //           ...
 443   //   PARENT  [PARENT_IJAVA_FRAME_ABI]
 444   //            ...
 445   //
 446 
 447   // Can't use call_VM here because we have not set up a new
 448   // interpreter state. Make the call to the vm and make it look like
 449   // our caller set up the JavaFrameAnchor.
 450   __ set_top_ijava_frame_at_SP_as_last_Java_frame(R1_SP, R12_scratch2/*tmp*/);
 451 
 452   // Push a new C frame and save LR.
 453   __ save_LR_CR(R0);
 454   __ push_frame_reg_args(0, R11_scratch1);
 455 
 456   // This is not a leaf but we have a JavaFrameAnchor now and we will
 457   // check (create) exceptions afterward so this is ok.
 458   __ call_VM_leaf(CAST_FROM_FN_PTR(address, InterpreterRuntime::throw_AbstractMethodError),
 459                   R16_thread);
 460 
 461   // Pop the C frame and restore LR.
 462   __ pop_frame();
 463   __ restore_LR_CR(R0);
 464 
 465   // Reset JavaFrameAnchor from call_VM_leaf above.
 466   __ reset_last_Java_frame();
 467 
 468   // We don't know our caller, so jump to the general forward exception stub,
 469   // which will also pop our full frame off. Satisfy the interface of
 470   // SharedRuntime::generate_forward_exception()
 471   __ load_const_optimized(R11_scratch1, StubRoutines::forward_exception_entry(), R0);
 472   __ mtctr(R11_scratch1);
 473   __ bctr();
 474 
 475   return entry;
 476 }
 477 
 478 // Interpreter intrinsic for WeakReference.get().
 479 // 1. Don't push a full blown frame and go on dispatching, but fetch the value
 480 //    into R8 and return quickly
 481 // 2. If G1 is active we *must* execute this intrinsic for corrrectness:
 482 //    It contains a GC barrier which puts the reference into the satb buffer
 483 //    to indicate that someone holds a strong reference to the object the
 484 //    weak ref points to!
 485 address TemplateInterpreterGenerator::generate_Reference_get_entry(void) {
 486   // Code: _aload_0, _getfield, _areturn
 487   // parameter size = 1
 488   //
 489   // The code that gets generated by this routine is split into 2 parts:
 490   //    1. the "intrinsified" code for G1 (or any SATB based GC),
 491   //    2. the slow path - which is an expansion of the regular method entry.
 492   //
 493   // Notes:
 494   // * In the G1 code we do not check whether we need to block for
 495   //   a safepoint. If G1 is enabled then we must execute the specialized
 496   //   code for Reference.get (except when the Reference object is null)
 497   //   so that we can log the value in the referent field with an SATB
 498   //   update buffer.
 499   //   If the code for the getfield template is modified so that the
 500   //   G1 pre-barrier code is executed when the current method is
 501   //   Reference.get() then going through the normal method entry
 502   //   will be fine.
 503   // * The G1 code can, however, check the receiver object (the instance
 504   //   of java.lang.Reference) and jump to the slow path if null. If the
 505   //   Reference object is null then we obviously cannot fetch the referent
 506   //   and so we don't need to call the G1 pre-barrier. Thus we can use the
 507   //   regular method entry code to generate the NPE.
 508   //
 509 
 510   if (UseG1GC) {
 511     address entry = __ pc();
 512 
 513     const int referent_offset = java_lang_ref_Reference::referent_offset;
 514     guarantee(referent_offset &gt; 0, "referent offset not initialized");
 515 
 516     Label slow_path;
 517 
 518     // Debugging not possible, so can't use __ skip_if_jvmti_mode(slow_path, GR31_SCRATCH);
 519 
 520     // In the G1 code we don't check if we need to reach a safepoint. We
 521     // continue and the thread will safepoint at the next bytecode dispatch.
 522 
 523     // If the receiver is null then it is OK to jump to the slow path.
 524     __ ld(R3_RET, Interpreter::stackElementSize, R15_esp); // get receiver
 525 
 526     // Check if receiver == NULL and go the slow path.
 527     __ cmpdi(CCR0, R3_RET, 0);
 528     __ beq(CCR0, slow_path);
 529 
 530     // Load the value of the referent field.
 531     __ load_heap_oop(R3_RET, referent_offset, R3_RET);
 532 
 533     // Generate the G1 pre-barrier code to log the value of
 534     // the referent field in an SATB buffer. Note with
 535     // these parameters the pre-barrier does not generate
 536     // the load of the previous value.
 537 
 538     // Restore caller sp for c2i case.
 539 #ifdef ASSERT
 540       __ ld(R9_ARG7, 0, R1_SP);
 541       __ ld(R10_ARG8, 0, R21_sender_SP);
 542       __ cmpd(CCR0, R9_ARG7, R10_ARG8);
 543       __ asm_assert_eq("backlink", 0x544);
 544 #endif // ASSERT
 545     __ mr(R1_SP, R21_sender_SP); // Cut the stack back to where the caller started.
 546 
 547     __ g1_write_barrier_pre(noreg,         // obj
 548                             noreg,         // offset
 549                             R3_RET,        // pre_val
 550                             R11_scratch1,  // tmp
 551                             R12_scratch2,  // tmp
 552                             true);         // needs_frame
 553 
 554     __ blr();
 555 
 556     // Generate regular method entry.
 557     __ bind(slow_path);
 558     __ jump_to_entry(Interpreter::entry_for_kind(Interpreter::zerolocals), R11_scratch1);
 559     return entry;
 560   }
 561 
 562   return NULL;
 563 }
 564 
 565 address TemplateInterpreterGenerator::generate_StackOverflowError_handler() {
 566   address entry = __ pc();
 567 
 568   // Expression stack must be empty before entering the VM if an
 569   // exception happened.
 570   __ empty_expression_stack();
 571   // Throw exception.
 572   __ call_VM(noreg,
 573              CAST_FROM_FN_PTR(address,
 574                               InterpreterRuntime::throw_StackOverflowError));
 575   return entry;
 576 }
 577 
 578 address TemplateInterpreterGenerator::generate_ArrayIndexOutOfBounds_handler(const char* name) {
 579   address entry = __ pc();
 580   __ empty_expression_stack();
 581   __ load_const_optimized(R4_ARG2, (address) name);
 582   // Index is in R17_tos.
 583   __ mr(R5_ARG3, R17_tos);
 584   __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::throw_ArrayIndexOutOfBoundsException));
 585   return entry;
 586 }
 587 
 588 #if 0
 589 // Call special ClassCastException constructor taking object to cast
 590 // and target class as arguments.
 591 address TemplateInterpreterGenerator::generate_ClassCastException_verbose_handler() {
 592   address entry = __ pc();
 593 
 594   // Expression stack must be empty before entering the VM if an
 595   // exception happened.
 596   __ empty_expression_stack();
 597 
 598   // Thread will be loaded to R3_ARG1.
 599   // Target class oop is in register R5_ARG3 by convention!
 600   __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::throw_ClassCastException_verbose), R17_tos, R5_ARG3);
 601   // Above call must not return here since exception pending.
 602   DEBUG_ONLY(__ should_not_reach_here();)
 603   return entry;
 604 }
 605 #endif
 606 
 607 address TemplateInterpreterGenerator::generate_ClassCastException_handler() {
 608   address entry = __ pc();
 609   // Expression stack must be empty before entering the VM if an
 610   // exception happened.
 611   __ empty_expression_stack();
 612 
 613   // Load exception object.
 614   // Thread will be loaded to R3_ARG1.
 615   __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::throw_ClassCastException), R17_tos);
 616 #ifdef ASSERT
 617   // Above call must not return here since exception pending.
 618   __ should_not_reach_here();
 619 #endif
 620   return entry;
 621 }
 622 
 623 address TemplateInterpreterGenerator::generate_exception_handler_common(const char* name, const char* message, bool pass_oop) {
 624   address entry = __ pc();
 625   //__ untested("generate_exception_handler_common");
 626   Register Rexception = R17_tos;
 627 
 628   // Expression stack must be empty before entering the VM if an exception happened.
 629   __ empty_expression_stack();
 630 
 631   __ load_const_optimized(R4_ARG2, (address) name, R11_scratch1);
 632   if (pass_oop) {
 633     __ mr(R5_ARG3, Rexception);
 634     __ call_VM(Rexception, CAST_FROM_FN_PTR(address, InterpreterRuntime::create_klass_exception), false);
 635   } else {
 636     __ load_const_optimized(R5_ARG3, (address) message, R11_scratch1);
 637     __ call_VM(Rexception, CAST_FROM_FN_PTR(address, InterpreterRuntime::create_exception), false);
 638   }
 639 
 640   // Throw exception.
 641   __ mr(R3_ARG1, Rexception);
 642   __ load_const_optimized(R11_scratch1, Interpreter::throw_exception_entry(), R12_scratch2);
 643   __ mtctr(R11_scratch1);
 644   __ bctr();
 645 
 646   return entry;
 647 }
 648 
 649 address TemplateInterpreterGenerator::generate_continuation_for(TosState state) {
 650   address entry = __ pc();
 651   __ unimplemented("generate_continuation_for");
 652   return entry;
 653 }
 654 
 655 // This entry is returned to when a call returns to the interpreter.
 656 // When we arrive here, we expect that the callee stack frame is already popped.
 657 address TemplateInterpreterGenerator::generate_return_entry_for(TosState state, int step, size_t index_size) {
 658   address entry = __ pc();
 659 
 660   // Move the value out of the return register back to the TOS cache of current frame.
 661   switch (state) {
 662     case ltos:
 663     case btos:
 664     case ztos:
 665     case ctos:
 666     case stos:
 667     case atos:
 668     case itos: __ mr(R17_tos, R3_RET); break;   // RET -&gt; TOS cache
 669     case ftos:
 670     case dtos: __ fmr(F15_ftos, F1_RET); break; // TOS cache -&gt; GR_FRET
 671     case vtos: break;                           // Nothing to do, this was a void return.
 672     default  : ShouldNotReachHere();
 673   }
 674 
 675   __ restore_interpreter_state(R11_scratch1); // Sets R11_scratch1 = fp.
 676   __ ld(R12_scratch2, _ijava_state_neg(top_frame_sp), R11_scratch1);
 677   __ resize_frame_absolute(R12_scratch2, R11_scratch1, R0);
 678 
 679   // Compiled code destroys templateTableBase, reload.
 680   __ load_const_optimized(R25_templateTableBase, (address)Interpreter::dispatch_table((TosState)0), R12_scratch2);
 681 
 682   if (state == atos) {
 683     __ profile_return_type(R3_RET, R11_scratch1, R12_scratch2);
 684   }
 685 
 686   const Register cache = R11_scratch1;
 687   const Register size  = R12_scratch2;
 688   __ get_cache_and_index_at_bcp(cache, 1, index_size);
 689 
 690   // Get least significant byte of 64 bit value:
 691 #if defined(VM_LITTLE_ENDIAN)
 692   __ lbz(size, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::flags_offset()), cache);
 693 #else
 694   __ lbz(size, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::flags_offset()) + 7, cache);
 695 #endif
 696   __ sldi(size, size, Interpreter::logStackElementSize);
 697   __ add(R15_esp, R15_esp, size);
 698   __ dispatch_next(state, step);
 699   return entry;
 700 }
 701 
 702 address TemplateInterpreterGenerator::generate_deopt_entry_for(TosState state, int step) {
 703   address entry = __ pc();
 704   // If state != vtos, we're returning from a native method, which put it's result
 705   // into the result register. So move the value out of the return register back
 706   // to the TOS cache of current frame.
 707 
 708   switch (state) {
 709     case ltos:
 710     case btos:
 711     case ztos:
 712     case ctos:
 713     case stos:
 714     case atos:
 715     case itos: __ mr(R17_tos, R3_RET); break;   // GR_RET -&gt; TOS cache
 716     case ftos:
 717     case dtos: __ fmr(F15_ftos, F1_RET); break; // TOS cache -&gt; GR_FRET
 718     case vtos: break;                           // Nothing to do, this was a void return.
 719     default  : ShouldNotReachHere();
 720   }
 721 
 722   // Load LcpoolCache @@@ should be already set!
 723   __ get_constant_pool_cache(R27_constPoolCache);
 724 
 725   // Handle a pending exception, fall through if none.
 726   __ check_and_forward_exception(R11_scratch1, R12_scratch2);
 727 
 728   // Start executing bytecodes.
 729   __ dispatch_next(state, step);
 730 
 731   return entry;
 732 }
 733 
 734 address TemplateInterpreterGenerator::generate_safept_entry_for(TosState state, address runtime_entry) {
 735   address entry = __ pc();
 736 
 737   __ push(state);
 738   __ call_VM(noreg, runtime_entry);
 739   __ dispatch_via(vtos, Interpreter::_normal_table.table_for(vtos));
 740 
 741   return entry;
 742 }
 743 
 744 // Helpers for commoning out cases in the various type of method entries.
 745 
 746 // Increment invocation count &amp; check for overflow.
 747 //
 748 // Note: checking for negative value instead of overflow
 749 //       so we have a 'sticky' overflow test.
 750 //
 751 void TemplateInterpreterGenerator::generate_counter_incr(Label* overflow, Label* profile_method, Label* profile_method_continue) {
 752   // Note: In tiered we increment either counters in method or in MDO depending if we're profiling or not.
 753   Register Rscratch1   = R11_scratch1;
 754   Register Rscratch2   = R12_scratch2;
 755   Register R3_counters = R3_ARG1;
 756   Label done;
 757 
 758   if (TieredCompilation) {
 759     const int increment = InvocationCounter::count_increment;
 760     Label no_mdo;
 761     if (ProfileInterpreter) {
 762       const Register Rmdo = R3_counters;
 763       // If no method data exists, go to profile_continue.
 764       __ ld(Rmdo, in_bytes(Method::method_data_offset()), R19_method);
 765       __ cmpdi(CCR0, Rmdo, 0);
 766       __ beq(CCR0, no_mdo);
 767 
 768       // Increment invocation counter in the MDO.
 769       const int mdo_ic_offs = in_bytes(MethodData::invocation_counter_offset()) + in_bytes(InvocationCounter::counter_offset());
 770       __ lwz(Rscratch2, mdo_ic_offs, Rmdo);
 771       __ lwz(Rscratch1, in_bytes(MethodData::invoke_mask_offset()), Rmdo);
 772       __ addi(Rscratch2, Rscratch2, increment);
 773       __ stw(Rscratch2, mdo_ic_offs, Rmdo);
 774       __ and_(Rscratch1, Rscratch2, Rscratch1);
 775       __ bne(CCR0, done);
 776       __ b(*overflow);
 777     }
 778 
 779     // Increment counter in MethodCounters*.
 780     const int mo_ic_offs = in_bytes(MethodCounters::invocation_counter_offset()) + in_bytes(InvocationCounter::counter_offset());
 781     __ bind(no_mdo);
 782     __ get_method_counters(R19_method, R3_counters, done);
 783     __ lwz(Rscratch2, mo_ic_offs, R3_counters);
 784     __ lwz(Rscratch1, in_bytes(MethodCounters::invoke_mask_offset()), R3_counters);
 785     __ addi(Rscratch2, Rscratch2, increment);
 786     __ stw(Rscratch2, mo_ic_offs, R3_counters);
 787     __ and_(Rscratch1, Rscratch2, Rscratch1);
 788     __ beq(CCR0, *overflow);
 789 
 790     __ bind(done);
 791 
 792   } else {
 793 
 794     // Update standard invocation counters.
 795     Register Rsum_ivc_bec = R4_ARG2;
 796     __ get_method_counters(R19_method, R3_counters, done);
 797     __ increment_invocation_counter(R3_counters, Rsum_ivc_bec, R12_scratch2);
 798     // Increment interpreter invocation counter.
 799     if (ProfileInterpreter) {  // %%% Merge this into methodDataOop.
 800       __ lwz(R12_scratch2, in_bytes(MethodCounters::interpreter_invocation_counter_offset()), R3_counters);
 801       __ addi(R12_scratch2, R12_scratch2, 1);
 802       __ stw(R12_scratch2, in_bytes(MethodCounters::interpreter_invocation_counter_offset()), R3_counters);
 803     }
 804     // Check if we must create a method data obj.
 805     if (ProfileInterpreter &amp;&amp; profile_method != NULL) {
 806       const Register profile_limit = Rscratch1;
 807       __ lwz(profile_limit, in_bytes(MethodCounters::interpreter_profile_limit_offset()), R3_counters);
 808       // Test to see if we should create a method data oop.
 809       __ cmpw(CCR0, Rsum_ivc_bec, profile_limit);
 810       __ blt(CCR0, *profile_method_continue);
 811       // If no method data exists, go to profile_method.
 812       __ test_method_data_pointer(*profile_method);
 813     }
 814     // Finally check for counter overflow.
 815     if (overflow) {
 816       const Register invocation_limit = Rscratch1;
 817       __ lwz(invocation_limit, in_bytes(MethodCounters::interpreter_invocation_limit_offset()), R3_counters);
 818       __ cmpw(CCR0, Rsum_ivc_bec, invocation_limit);
 819       __ bge(CCR0, *overflow);
 820     }
 821 
 822     __ bind(done);
 823   }
 824 }
 825 
 826 // Generate code to initiate compilation on invocation counter overflow.
 827 void TemplateInterpreterGenerator::generate_counter_overflow(Label&amp; continue_entry) {
 828   // Generate code to initiate compilation on the counter overflow.
 829 
 830   // InterpreterRuntime::frequency_counter_overflow takes one arguments,
 831   // which indicates if the counter overflow occurs at a backwards branch (NULL bcp)
 832   // We pass zero in.
 833   // The call returns the address of the verified entry point for the method or NULL
 834   // if the compilation did not complete (either went background or bailed out).
 835   //
 836   // Unlike the C++ interpreter above: Check exceptions!
 837   // Assumption: Caller must set the flag "do_not_unlock_if_sychronized" if the monitor of a sync'ed
 838   // method has not yet been created. Thus, no unlocking of a non-existing monitor can occur.
 839 
 840   __ li(R4_ARG2, 0);
 841   __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::frequency_counter_overflow), R4_ARG2, true);
 842 
 843   // Returns verified_entry_point or NULL.
 844   // We ignore it in any case.
 845   __ b(continue_entry);
 846 }
 847 
 848 // See if we've got enough room on the stack for locals plus overhead below
 849 // JavaThread::stack_overflow_limit(). If not, throw a StackOverflowError
 850 // without going through the signal handler, i.e., reserved and yellow zones
 851 // will not be made usable. The shadow zone must suffice to handle the
 852 // overflow.
 853 //
 854 // Kills Rmem_frame_size, Rscratch1.
 855 void TemplateInterpreterGenerator::generate_stack_overflow_check(Register Rmem_frame_size, Register Rscratch1) {
 856   Label done;
 857   assert_different_registers(Rmem_frame_size, Rscratch1);
 858 
 859   BLOCK_COMMENT("stack_overflow_check_with_compare {");
 860   __ sub(Rmem_frame_size, R1_SP, Rmem_frame_size);
 861   __ ld(Rscratch1, thread_(stack_overflow_limit));
 862   __ cmpld(CCR0/*is_stack_overflow*/, Rmem_frame_size, Rscratch1);
 863   __ bgt(CCR0/*is_stack_overflow*/, done);
 864 
 865   // The stack overflows. Load target address of the runtime stub and call it.
 866   assert(StubRoutines::throw_StackOverflowError_entry() != NULL, "generated in wrong order");
 867   __ load_const_optimized(Rscratch1, (StubRoutines::throw_StackOverflowError_entry()), R0);
 868   __ mtctr(Rscratch1);
 869   // Restore caller_sp.
 870 #ifdef ASSERT
 871   __ ld(Rscratch1, 0, R1_SP);
 872   __ ld(R0, 0, R21_sender_SP);
 873   __ cmpd(CCR0, R0, Rscratch1);
 874   __ asm_assert_eq("backlink", 0x547);
 875 #endif // ASSERT
 876   __ mr(R1_SP, R21_sender_SP);
 877   __ bctr();
 878 
 879   __ align(32, 12);
 880   __ bind(done);
 881   BLOCK_COMMENT("} stack_overflow_check_with_compare");
 882 }
 883 
 884 // Lock the current method, interpreter register window must be set up!
 885 void TemplateInterpreterGenerator::lock_method(Register Rflags, Register Rscratch1, Register Rscratch2, bool flags_preloaded) {
 886   const Register Robj_to_lock = Rscratch2;
 887 
 888   {
 889     if (!flags_preloaded) {
 890       __ lwz(Rflags, method_(access_flags));
 891     }
 892 
 893 #ifdef ASSERT
 894     // Check if methods needs synchronization.
 895     {
 896       Label Lok;
 897       __ testbitdi(CCR0, R0, Rflags, JVM_ACC_SYNCHRONIZED_BIT);
 898       __ btrue(CCR0,Lok);
 899       __ stop("method doesn't need synchronization");
 900       __ bind(Lok);
 901     }
 902 #endif // ASSERT
 903   }
 904 
 905   // Get synchronization object to Rscratch2.
 906   {
 907     Label Lstatic;
 908     Label Ldone;
 909 
 910     __ testbitdi(CCR0, R0, Rflags, JVM_ACC_STATIC_BIT);
 911     __ btrue(CCR0, Lstatic);
 912 
 913     // Non-static case: load receiver obj from stack and we're done.
 914     __ ld(Robj_to_lock, R18_locals);
 915     __ b(Ldone);
 916 
 917     __ bind(Lstatic); // Static case: Lock the java mirror
 918     // Load mirror from interpreter frame.
 919     __ ld(Robj_to_lock, _abi(callers_sp), R1_SP);
 920     __ ld(Robj_to_lock, _ijava_state_neg(mirror), Robj_to_lock);
 921 
 922     __ bind(Ldone);
 923     __ verify_oop(Robj_to_lock);
 924   }
 925 
 926   // Got the oop to lock =&gt; execute!
 927   __ add_monitor_to_stack(true, Rscratch1, R0);
 928 
 929   __ std(Robj_to_lock, BasicObjectLock::obj_offset_in_bytes(), R26_monitor);
 930   __ lock_object(R26_monitor, Robj_to_lock);
 931 }
 932 
 933 // Generate a fixed interpreter frame for pure interpreter
 934 // and I2N native transition frames.
 935 //
 936 // Before (stack grows downwards):
 937 //
 938 //         |  ...         |
 939 //         |------------- |
 940 //         |  java arg0   |
 941 //         |  ...         |
 942 //         |  java argn   |
 943 //         |              |   &lt;-   R15_esp
 944 //         |              |
 945 //         |--------------|
 946 //         | abi_112      |
 947 //         |              |   &lt;-   R1_SP
 948 //         |==============|
 949 //
 950 //
 951 // After:
 952 //
 953 //         |  ...         |
 954 //         |  java arg0   |&lt;-   R18_locals
 955 //         |  ...         |
 956 //         |  java argn   |
 957 //         |--------------|
 958 //         |              |
 959 //         |  java locals |
 960 //         |              |
 961 //         |--------------|
 962 //         |  abi_48      |
 963 //         |==============|
 964 //         |              |
 965 //         |   istate     |
 966 //         |              |
 967 //         |--------------|
 968 //         |   monitor    |&lt;-   R26_monitor
 969 //         |--------------|
 970 //         |              |&lt;-   R15_esp
 971 //         | expression   |
 972 //         | stack        |
 973 //         |              |
 974 //         |--------------|
 975 //         |              |
 976 //         | abi_112      |&lt;-   R1_SP
 977 //         |==============|
 978 //
 979 // The top most frame needs an abi space of 112 bytes. This space is needed,
 980 // since we call to c. The c function may spill their arguments to the caller
 981 // frame. When we call to java, we don't need these spill slots. In order to save
 982 // space on the stack, we resize the caller. However, java locals reside in
 983 // the caller frame and the frame has to be increased. The frame_size for the
 984 // current frame was calculated based on max_stack as size for the expression
 985 // stack. At the call, just a part of the expression stack might be used.
 986 // We don't want to waste this space and cut the frame back accordingly.
 987 // The resulting amount for resizing is calculated as follows:
 988 // resize =   (number_of_locals - number_of_arguments) * slot_size
 989 //          + (R1_SP - R15_esp) + 48
 990 //
 991 // The size for the callee frame is calculated:
 992 // framesize = 112 + max_stack + monitor + state_size
 993 //
 994 // maxstack:   Max number of slots on the expression stack, loaded from the method.
 995 // monitor:    We statically reserve room for one monitor object.
 996 // state_size: We save the current state of the interpreter to this area.
 997 //
 998 void TemplateInterpreterGenerator::generate_fixed_frame(bool native_call, Register Rsize_of_parameters, Register Rsize_of_locals) {
 999   Register parent_frame_resize = R6_ARG4, // Frame will grow by this number of bytes.
1000            top_frame_size      = R7_ARG5,
1001            Rconst_method       = R8_ARG6;
1002 
1003   assert_different_registers(Rsize_of_parameters, Rsize_of_locals, parent_frame_resize, top_frame_size);
1004 
1005   __ ld(Rconst_method, method_(const));
1006   __ lhz(Rsize_of_parameters /* number of params */,
1007          in_bytes(ConstMethod::size_of_parameters_offset()), Rconst_method);
1008   if (native_call) {
1009     // If we're calling a native method, we reserve space for the worst-case signature
1010     // handler varargs vector, which is max(Argument::n_register_parameters, parameter_count+2).
1011     // We add two slots to the parameter_count, one for the jni
1012     // environment and one for a possible native mirror.
1013     Label skip_native_calculate_max_stack;
1014     __ addi(top_frame_size, Rsize_of_parameters, 2);
1015     __ cmpwi(CCR0, top_frame_size, Argument::n_register_parameters);
1016     __ bge(CCR0, skip_native_calculate_max_stack);
1017     __ li(top_frame_size, Argument::n_register_parameters);
1018     __ bind(skip_native_calculate_max_stack);
1019     __ sldi(Rsize_of_parameters, Rsize_of_parameters, Interpreter::logStackElementSize);
1020     __ sldi(top_frame_size, top_frame_size, Interpreter::logStackElementSize);
1021     __ sub(parent_frame_resize, R1_SP, R15_esp); // &lt;0, off by Interpreter::stackElementSize!
1022     assert(Rsize_of_locals == noreg, "Rsize_of_locals not initialized"); // Only relevant value is Rsize_of_parameters.
1023   } else {
1024     __ lhz(Rsize_of_locals /* number of params */, in_bytes(ConstMethod::size_of_locals_offset()), Rconst_method);
1025     __ sldi(Rsize_of_parameters, Rsize_of_parameters, Interpreter::logStackElementSize);
1026     __ sldi(Rsize_of_locals, Rsize_of_locals, Interpreter::logStackElementSize);
1027     __ lhz(top_frame_size, in_bytes(ConstMethod::max_stack_offset()), Rconst_method);
1028     __ sub(R11_scratch1, Rsize_of_locals, Rsize_of_parameters); // &gt;=0
1029     __ sub(parent_frame_resize, R1_SP, R15_esp); // &lt;0, off by Interpreter::stackElementSize!
1030     __ sldi(top_frame_size, top_frame_size, Interpreter::logStackElementSize);
1031     __ add(parent_frame_resize, parent_frame_resize, R11_scratch1);
1032   }
1033 
1034   // Compute top frame size.
1035   __ addi(top_frame_size, top_frame_size, frame::abi_reg_args_size + frame::ijava_state_size);
1036 
1037   // Cut back area between esp and max_stack.
1038   __ addi(parent_frame_resize, parent_frame_resize, frame::abi_minframe_size - Interpreter::stackElementSize);
1039 
1040   __ round_to(top_frame_size, frame::alignment_in_bytes);
1041   __ round_to(parent_frame_resize, frame::alignment_in_bytes);
1042   // parent_frame_resize = (locals-parameters) - (ESP-SP-ABI48) Rounded to frame alignment size.
1043   // Enlarge by locals-parameters (not in case of native_call), shrink by ESP-SP-ABI48.
1044 
1045   if (!native_call) {
1046     // Stack overflow check.
1047     // Native calls don't need the stack size check since they have no
1048     // expression stack and the arguments are already on the stack and
1049     // we only add a handful of words to the stack.
1050     __ add(R11_scratch1, parent_frame_resize, top_frame_size);
1051     generate_stack_overflow_check(R11_scratch1, R12_scratch2);
1052   }
1053 
1054   // Set up interpreter state registers.
1055 
1056   __ add(R18_locals, R15_esp, Rsize_of_parameters);
1057   __ ld(R27_constPoolCache, in_bytes(ConstMethod::constants_offset()), Rconst_method);
1058   __ ld(R27_constPoolCache, ConstantPool::cache_offset_in_bytes(), R27_constPoolCache);
1059 
1060   // Set method data pointer.
1061   if (ProfileInterpreter) {
1062     Label zero_continue;
1063     __ ld(R28_mdx, method_(method_data));
1064     __ cmpdi(CCR0, R28_mdx, 0);
1065     __ beq(CCR0, zero_continue);
1066     __ addi(R28_mdx, R28_mdx, in_bytes(MethodData::data_offset()));
1067     __ bind(zero_continue);
1068   }
1069 
1070   if (native_call) {
1071     __ li(R14_bcp, 0); // Must initialize.
1072   } else {
1073     __ add(R14_bcp, in_bytes(ConstMethod::codes_offset()), Rconst_method);
1074   }
1075 
1076   // Resize parent frame.
1077   __ mflr(R12_scratch2);
1078   __ neg(parent_frame_resize, parent_frame_resize);
1079   __ resize_frame(parent_frame_resize, R11_scratch1);
1080   __ std(R12_scratch2, _abi(lr), R1_SP);
1081 
1082   // Get mirror and store it in the frame as GC root for this Method*.
1083   __ load_mirror_from_const_method(R12_scratch2, Rconst_method);
1084 
1085   __ addi(R26_monitor, R1_SP, - frame::ijava_state_size);
1086   __ addi(R15_esp, R26_monitor, - Interpreter::stackElementSize);
1087 
1088   // Store values.
1089   // R15_esp, R14_bcp, R26_monitor, R28_mdx are saved at java calls
1090   // in InterpreterMacroAssembler::call_from_interpreter.
1091   __ std(R19_method, _ijava_state_neg(method), R1_SP);
1092   __ std(R12_scratch2, _ijava_state_neg(mirror), R1_SP);
1093   __ std(R21_sender_SP, _ijava_state_neg(sender_sp), R1_SP);
1094   __ std(R27_constPoolCache, _ijava_state_neg(cpoolCache), R1_SP);
1095   __ std(R18_locals, _ijava_state_neg(locals), R1_SP);
1096 
1097   // Note: esp, bcp, monitor, mdx live in registers. Hence, the correct version can only
1098   // be found in the frame after save_interpreter_state is done. This is always true
1099   // for non-top frames. But when a signal occurs, dumping the top frame can go wrong,
1100   // because e.g. frame::interpreter_frame_bcp() will not access the correct value
1101   // (Enhanced Stack Trace).
1102   // The signal handler does not save the interpreter state into the frame.
1103   __ li(R0, 0);
1104 #ifdef ASSERT
1105   // Fill remaining slots with constants.
1106   __ load_const_optimized(R11_scratch1, 0x5afe);
1107   __ load_const_optimized(R12_scratch2, 0xdead);
1108 #endif
1109   // We have to initialize some frame slots for native calls (accessed by GC).
1110   if (native_call) {
1111     __ std(R26_monitor, _ijava_state_neg(monitors), R1_SP);
1112     __ std(R14_bcp, _ijava_state_neg(bcp), R1_SP);
1113     if (ProfileInterpreter) { __ std(R28_mdx, _ijava_state_neg(mdx), R1_SP); }
1114   }
1115 #ifdef ASSERT
1116   else {
1117     __ std(R12_scratch2, _ijava_state_neg(monitors), R1_SP);
1118     __ std(R12_scratch2, _ijava_state_neg(bcp), R1_SP);
1119     __ std(R12_scratch2, _ijava_state_neg(mdx), R1_SP);
1120   }
1121   __ std(R11_scratch1, _ijava_state_neg(ijava_reserved), R1_SP);
1122   __ std(R12_scratch2, _ijava_state_neg(esp), R1_SP);
1123   __ std(R12_scratch2, _ijava_state_neg(lresult), R1_SP);
1124   __ std(R12_scratch2, _ijava_state_neg(fresult), R1_SP);
1125 #endif
1126   __ subf(R12_scratch2, top_frame_size, R1_SP);
1127   __ std(R0, _ijava_state_neg(oop_tmp), R1_SP);
1128   __ std(R12_scratch2, _ijava_state_neg(top_frame_sp), R1_SP);
1129 
1130   // Push top frame.
1131   __ push_frame(top_frame_size, R11_scratch1);
1132 }
1133 
1134 // End of helpers
1135 
1136 address TemplateInterpreterGenerator::generate_math_entry(AbstractInterpreter::MethodKind kind) {
1137   if (!Interpreter::math_entry_available(kind)) {
1138     NOT_PRODUCT(__ should_not_reach_here();)
1139     return NULL;
1140   }
1141 
1142   address entry = __ pc();
1143 
1144   __ lfd(F1_RET, Interpreter::stackElementSize, R15_esp);
1145 
1146   // Pop c2i arguments (if any) off when we return.
1147 #ifdef ASSERT
1148   __ ld(R9_ARG7, 0, R1_SP);
1149   __ ld(R10_ARG8, 0, R21_sender_SP);
1150   __ cmpd(CCR0, R9_ARG7, R10_ARG8);
1151   __ asm_assert_eq("backlink", 0x545);
1152 #endif // ASSERT
1153   __ mr(R1_SP, R21_sender_SP); // Cut the stack back to where the caller started.
1154 
1155   if (kind == Interpreter::java_lang_math_sqrt) {
1156     __ fsqrt(F1_RET, F1_RET);
1157   } else if (kind == Interpreter::java_lang_math_abs) {
1158     __ fabs(F1_RET, F1_RET);
1159   } else {
1160     ShouldNotReachHere();
1161   }
1162 
1163   // And we're done.
1164   __ blr();
1165 
1166   __ flush();
1167 
1168   return entry;
1169 }
1170 
1171 void TemplateInterpreterGenerator::bang_stack_shadow_pages(bool native_call) {
1172   // Quick &amp; dirty stack overflow checking: bang the stack &amp; handle trap.
1173   // Note that we do the banging after the frame is setup, since the exception
1174   // handling code expects to find a valid interpreter frame on the stack.
1175   // Doing the banging earlier fails if the caller frame is not an interpreter
1176   // frame.
1177   // (Also, the exception throwing code expects to unlock any synchronized
1178   // method receiever, so do the banging after locking the receiver.)
1179 
1180   // Bang each page in the shadow zone. We can't assume it's been done for
1181   // an interpreter frame with greater than a page of locals, so each page
1182   // needs to be checked.  Only true for non-native.
1183   if (UseStackBanging) {
1184     const int page_size = os::vm_page_size();
1185     const int n_shadow_pages = ((int)JavaThread::stack_shadow_zone_size()) / page_size;
1186     const int start_page = native_call ? n_shadow_pages : 1;
1187     BLOCK_COMMENT("bang_stack_shadow_pages:");
1188     for (int pages = start_page; pages &lt;= n_shadow_pages; pages++) {
1189       __ bang_stack_with_offset(pages*page_size);
1190     }
1191   }
1192 }
1193 
1194 // Interpreter stub for calling a native method. (asm interpreter)
1195 // This sets up a somewhat different looking stack for calling the
1196 // native method than the typical interpreter frame setup.
1197 //
1198 // On entry:
1199 //   R19_method    - method
1200 //   R16_thread    - JavaThread*
1201 //   R15_esp       - intptr_t* sender tos
1202 //
1203 //   abstract stack (grows up)
1204 //     [  IJava (caller of JNI callee)  ]  &lt;-- ASP
1205 //        ...
1206 address TemplateInterpreterGenerator::generate_native_entry(bool synchronized) {
1207 
1208   address entry = __ pc();
1209 
1210   const bool inc_counter = UseCompiler || CountCompiledCalls || LogTouchedMethods;
1211 
1212   // -----------------------------------------------------------------------------
1213   // Allocate a new frame that represents the native callee (i2n frame).
1214   // This is not a full-blown interpreter frame, but in particular, the
1215   // following registers are valid after this:
1216   // - R19_method
1217   // - R18_local (points to start of arguments to native function)
1218   //
1219   //   abstract stack (grows up)
1220   //     [  IJava (caller of JNI callee)  ]  &lt;-- ASP
1221   //        ...
1222 
1223   const Register signature_handler_fd = R11_scratch1;
1224   const Register pending_exception    = R0;
1225   const Register result_handler_addr  = R31;
1226   const Register native_method_fd     = R11_scratch1;
1227   const Register access_flags         = R22_tmp2;
1228   const Register active_handles       = R11_scratch1; // R26_monitor saved to state.
1229   const Register sync_state           = R12_scratch2;
1230   const Register sync_state_addr      = sync_state;   // Address is dead after use.
1231   const Register suspend_flags        = R11_scratch1;
1232 
1233   //=============================================================================
1234   // Allocate new frame and initialize interpreter state.
1235 
1236   Label exception_return;
1237   Label exception_return_sync_check;
1238   Label stack_overflow_return;
1239 
1240   // Generate new interpreter state and jump to stack_overflow_return in case of
1241   // a stack overflow.
1242   //generate_compute_interpreter_state(stack_overflow_return);
1243 
1244   Register size_of_parameters = R22_tmp2;
1245 
1246   generate_fixed_frame(true, size_of_parameters, noreg /* unused */);
1247 
1248   //=============================================================================
1249   // Increment invocation counter. On overflow, entry to JNI method
1250   // will be compiled.
1251   Label invocation_counter_overflow, continue_after_compile;
1252   if (inc_counter) {
1253     if (synchronized) {
1254       // Since at this point in the method invocation the exception handler
1255       // would try to exit the monitor of synchronized methods which hasn't
1256       // been entered yet, we set the thread local variable
1257       // _do_not_unlock_if_synchronized to true. If any exception was thrown by
1258       // runtime, exception handling i.e. unlock_if_synchronized_method will
1259       // check this thread local flag.
1260       // This flag has two effects, one is to force an unwind in the topmost
1261       // interpreter frame and not perform an unlock while doing so.
1262       __ li(R0, 1);
1263       __ stb(R0, in_bytes(JavaThread::do_not_unlock_if_synchronized_offset()), R16_thread);
1264     }
1265     generate_counter_incr(&amp;invocation_counter_overflow, NULL, NULL);
1266 
1267     BIND(continue_after_compile);
1268   }
1269 
1270   bang_stack_shadow_pages(true);
1271 
1272   if (inc_counter) {
1273     // Reset the _do_not_unlock_if_synchronized flag.
1274     if (synchronized) {
1275       __ li(R0, 0);
1276       __ stb(R0, in_bytes(JavaThread::do_not_unlock_if_synchronized_offset()), R16_thread);
1277     }
1278   }
1279 
1280   // access_flags = method-&gt;access_flags();
1281   // Load access flags.
1282   assert(access_flags-&gt;is_nonvolatile(),
1283          "access_flags must be in a non-volatile register");
1284   // Type check.
1285   assert(4 == sizeof(AccessFlags), "unexpected field size");
1286   __ lwz(access_flags, method_(access_flags));
1287 
1288   // We don't want to reload R19_method and access_flags after calls
1289   // to some helper functions.
1290   assert(R19_method-&gt;is_nonvolatile(),
1291          "R19_method must be a non-volatile register");
1292 
1293   // Check for synchronized methods. Must happen AFTER invocation counter
1294   // check, so method is not locked if counter overflows.
1295 
1296   if (synchronized) {
1297     lock_method(access_flags, R11_scratch1, R12_scratch2, true);
1298 
1299     // Update monitor in state.
1300     __ ld(R11_scratch1, 0, R1_SP);
1301     __ std(R26_monitor, _ijava_state_neg(monitors), R11_scratch1);
1302   }
1303 
1304   // jvmti/jvmpi support
1305   __ notify_method_entry();
1306 
1307   //=============================================================================
1308   // Get and call the signature handler.
1309 
1310   __ ld(signature_handler_fd, method_(signature_handler));
1311   Label call_signature_handler;
1312 
1313   __ cmpdi(CCR0, signature_handler_fd, 0);
1314   __ bne(CCR0, call_signature_handler);
1315 
1316   // Method has never been called. Either generate a specialized
1317   // handler or point to the slow one.
1318   //
1319   // Pass parameter 'false' to avoid exception check in call_VM.
1320   __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::prepare_native_call), R19_method, false);
1321 
1322   // Check for an exception while looking up the target method. If we
1323   // incurred one, bail.
1324   __ ld(pending_exception, thread_(pending_exception));
1325   __ cmpdi(CCR0, pending_exception, 0);
1326   __ bne(CCR0, exception_return_sync_check); // Has pending exception.
1327 
1328   // Reload signature handler, it may have been created/assigned in the meanwhile.
1329   __ ld(signature_handler_fd, method_(signature_handler));
1330   __ twi_0(signature_handler_fd); // Order wrt. load of klass mirror and entry point (isync is below).
1331 
1332   BIND(call_signature_handler);
1333 
1334   // Before we call the signature handler we push a new frame to
1335   // protect the interpreter frame volatile registers when we return
1336   // from jni but before we can get back to Java.
1337 
1338   // First set the frame anchor while the SP/FP registers are
1339   // convenient and the slow signature handler can use this same frame
1340   // anchor.
1341 
1342   // We have a TOP_IJAVA_FRAME here, which belongs to us.
1343   __ set_top_ijava_frame_at_SP_as_last_Java_frame(R1_SP, R12_scratch2/*tmp*/);
1344 
1345   // Now the interpreter frame (and its call chain) have been
1346   // invalidated and flushed. We are now protected against eager
1347   // being enabled in native code. Even if it goes eager the
1348   // registers will be reloaded as clean and we will invalidate after
1349   // the call so no spurious flush should be possible.
1350 
1351   // Call signature handler and pass locals address.
1352   //
1353   // Our signature handlers copy required arguments to the C stack
1354   // (outgoing C args), R3_ARG1 to R10_ARG8, and FARG1 to FARG13.
1355   __ mr(R3_ARG1, R18_locals);
1356 #if !defined(ABI_ELFv2)
1357   __ ld(signature_handler_fd, 0, signature_handler_fd);
1358 #endif
1359 
1360   __ call_stub(signature_handler_fd);
1361 
1362   // Remove the register parameter varargs slots we allocated in
1363   // compute_interpreter_state. SP+16 ends up pointing to the ABI
1364   // outgoing argument area.
1365   //
1366   // Not needed on PPC64.
1367   //__ add(SP, SP, Argument::n_register_parameters*BytesPerWord);
1368 
1369   assert(result_handler_addr-&gt;is_nonvolatile(), "result_handler_addr must be in a non-volatile register");
1370   // Save across call to native method.
1371   __ mr(result_handler_addr, R3_RET);
1372 
1373   __ isync(); // Acquire signature handler before trying to fetch the native entry point and klass mirror.
1374 
1375   // Set up fixed parameters and call the native method.
1376   // If the method is static, get mirror into R4_ARG2.
1377   {
1378     Label method_is_not_static;
1379     // Access_flags is non-volatile and still, no need to restore it.
1380 
1381     // Restore access flags.
1382     __ testbitdi(CCR0, R0, access_flags, JVM_ACC_STATIC_BIT);
1383     __ bfalse(CCR0, method_is_not_static);
1384 
1385     __ ld(R11_scratch1, _abi(callers_sp), R1_SP);
1386     // Load mirror from interpreter frame.
1387     __ ld(R12_scratch2, _ijava_state_neg(mirror), R11_scratch1);
1388     // R4_ARG2 = &amp;state-&gt;_oop_temp;
1389     __ addi(R4_ARG2, R11_scratch1, _ijava_state_neg(oop_tmp));
1390     __ std(R12_scratch2/*mirror*/, _ijava_state_neg(oop_tmp), R11_scratch1);
1391     BIND(method_is_not_static);
1392   }
1393 
1394   // At this point, arguments have been copied off the stack into
1395   // their JNI positions. Oops are boxed in-place on the stack, with
1396   // handles copied to arguments. The result handler address is in a
1397   // register.
1398 
1399   // Pass JNIEnv address as first parameter.
1400   __ addir(R3_ARG1, thread_(jni_environment));
1401 
1402   // Load the native_method entry before we change the thread state.
1403   __ ld(native_method_fd, method_(native_function));
1404 
1405   //=============================================================================
1406   // Transition from _thread_in_Java to _thread_in_native. As soon as
1407   // we make this change the safepoint code needs to be certain that
1408   // the last Java frame we established is good. The pc in that frame
1409   // just needs to be near here not an actual return address.
1410 
1411   // We use release_store_fence to update values like the thread state, where
1412   // we don't want the current thread to continue until all our prior memory
1413   // accesses (including the new thread state) are visible to other threads.
1414   __ li(R0, _thread_in_native);
1415   __ release();
1416 
1417   // TODO PPC port assert(4 == JavaThread::sz_thread_state(), "unexpected field size");
1418   __ stw(R0, thread_(thread_state));
1419 
1420   if (UseMembar) {
1421     __ fence();
1422   }
1423 
1424   //=============================================================================
1425   // Call the native method. Argument registers must not have been
1426   // overwritten since "__ call_stub(signature_handler);" (except for
1427   // ARG1 and ARG2 for static methods).
1428   __ call_c(native_method_fd);
1429 
1430   __ li(R0, 0);
1431   __ ld(R11_scratch1, 0, R1_SP);
1432   __ std(R3_RET, _ijava_state_neg(lresult), R11_scratch1);
1433   __ stfd(F1_RET, _ijava_state_neg(fresult), R11_scratch1);
1434   __ std(R0/*mirror*/, _ijava_state_neg(oop_tmp), R11_scratch1); // reset
1435 
1436   // Note: C++ interpreter needs the following here:
1437   // The frame_manager_lr field, which we use for setting the last
1438   // java frame, gets overwritten by the signature handler. Restore
1439   // it now.
1440   //__ get_PC_trash_LR(R11_scratch1);
1441   //__ std(R11_scratch1, _top_ijava_frame_abi(frame_manager_lr), R1_SP);
1442 
1443   // Because of GC R19_method may no longer be valid.
1444 
1445   // Block, if necessary, before resuming in _thread_in_Java state.
1446   // In order for GC to work, don't clear the last_Java_sp until after
1447   // blocking.
1448 
1449   //=============================================================================
1450   // Switch thread to "native transition" state before reading the
1451   // synchronization state. This additional state is necessary
1452   // because reading and testing the synchronization state is not
1453   // atomic w.r.t. GC, as this scenario demonstrates: Java thread A,
1454   // in _thread_in_native state, loads _not_synchronized and is
1455   // preempted. VM thread changes sync state to synchronizing and
1456   // suspends threads for GC. Thread A is resumed to finish this
1457   // native method, but doesn't block here since it didn't see any
1458   // synchronization in progress, and escapes.
1459 
1460   // We use release_store_fence to update values like the thread state, where
1461   // we don't want the current thread to continue until all our prior memory
1462   // accesses (including the new thread state) are visible to other threads.
1463   __ li(R0/*thread_state*/, _thread_in_native_trans);
1464   __ release();
1465   __ stw(R0/*thread_state*/, thread_(thread_state));
1466   if (UseMembar) {
1467     __ fence();
1468   }
1469   // Write serialization page so that the VM thread can do a pseudo remote
1470   // membar. We use the current thread pointer to calculate a thread
1471   // specific offset to write to within the page. This minimizes bus
1472   // traffic due to cache line collision.
1473   else {
1474     __ serialize_memory(R16_thread, R11_scratch1, R12_scratch2);
1475   }
1476 
1477   // Now before we return to java we must look for a current safepoint
1478   // (a new safepoint can not start since we entered native_trans).
1479   // We must check here because a current safepoint could be modifying
1480   // the callers registers right this moment.
1481 
1482   // Acquire isn't strictly necessary here because of the fence, but
1483   // sync_state is declared to be volatile, so we do it anyway
1484   // (cmp-br-isync on one path, release (same as acquire on PPC64) on the other path).
1485   int sync_state_offs = __ load_const_optimized(sync_state_addr, SafepointSynchronize::address_of_state(), /*temp*/R0, true);
1486 
1487   // TODO PPC port assert(4 == SafepointSynchronize::sz_state(), "unexpected field size");
1488   __ lwz(sync_state, sync_state_offs, sync_state_addr);
1489 
1490   // TODO PPC port assert(4 == Thread::sz_suspend_flags(), "unexpected field size");
1491   __ lwz(suspend_flags, thread_(suspend_flags));
1492 
1493   Label sync_check_done;
1494   Label do_safepoint;
1495   // No synchronization in progress nor yet synchronized.
1496   __ cmpwi(CCR0, sync_state, SafepointSynchronize::_not_synchronized);
1497   // Not suspended.
1498   __ cmpwi(CCR1, suspend_flags, 0);
1499 
1500   __ bne(CCR0, do_safepoint);
1501   __ beq(CCR1, sync_check_done);
1502   __ bind(do_safepoint);
1503   __ isync();
1504   // Block. We do the call directly and leave the current
1505   // last_Java_frame setup undisturbed. We must save any possible
1506   // native result across the call. No oop is present.
1507 
1508   __ mr(R3_ARG1, R16_thread);
1509 #if defined(ABI_ELFv2)
1510   __ call_c(CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans),
1511             relocInfo::none);
1512 #else
1513   __ call_c(CAST_FROM_FN_PTR(FunctionDescriptor*, JavaThread::check_special_condition_for_native_trans),
1514             relocInfo::none);
1515 #endif
1516 
1517   __ bind(sync_check_done);
1518 
1519   //=============================================================================
1520   // &lt;&lt;&lt;&lt;&lt;&lt; Back in Interpreter Frame &gt;&gt;&gt;&gt;&gt;
1521 
1522   // We are in thread_in_native_trans here and back in the normal
1523   // interpreter frame. We don't have to do anything special about
1524   // safepoints and we can switch to Java mode anytime we are ready.
1525 
1526   // Note: frame::interpreter_frame_result has a dependency on how the
1527   // method result is saved across the call to post_method_exit. For
1528   // native methods it assumes that the non-FPU/non-void result is
1529   // saved in _native_lresult and a FPU result in _native_fresult. If
1530   // this changes then the interpreter_frame_result implementation
1531   // will need to be updated too.
1532 
1533   // On PPC64, we have stored the result directly after the native call.
1534 
1535   //=============================================================================
1536   // Back in Java
1537 
1538   // We use release_store_fence to update values like the thread state, where
1539   // we don't want the current thread to continue until all our prior memory
1540   // accesses (including the new thread state) are visible to other threads.
1541   __ li(R0/*thread_state*/, _thread_in_Java);
1542   __ release();
1543   __ stw(R0/*thread_state*/, thread_(thread_state));
1544   if (UseMembar) {
1545     __ fence();
1546   }
1547 
1548   if (CheckJNICalls) {
1549     // clear_pending_jni_exception_check
1550     __ load_const_optimized(R0, 0L);
1551     __ st_ptr(R0, JavaThread::pending_jni_exception_check_fn_offset(), R16_thread);
1552   }
1553 
1554   __ reset_last_Java_frame();
1555 
1556   // Jvmdi/jvmpi support. Whether we've got an exception pending or
1557   // not, and whether unlocking throws an exception or not, we notify
1558   // on native method exit. If we do have an exception, we'll end up
1559   // in the caller's context to handle it, so if we don't do the
1560   // notify here, we'll drop it on the floor.
1561   __ notify_method_exit(true/*native method*/,
1562                         ilgl /*illegal state (not used for native methods)*/,
1563                         InterpreterMacroAssembler::NotifyJVMTI,
1564                         false /*check_exceptions*/);
1565 
1566   //=============================================================================
1567   // Handle exceptions
1568 
1569   if (synchronized) {
1570     // Don't check for exceptions since we're still in the i2n frame. Do that
1571     // manually afterwards.
1572     __ unlock_object(R26_monitor, false); // Can also unlock methods.
1573   }
1574 
1575   // Reset active handles after returning from native.
1576   // thread-&gt;active_handles()-&gt;clear();
1577   __ ld(active_handles, thread_(active_handles));
1578   // TODO PPC port assert(4 == JNIHandleBlock::top_size_in_bytes(), "unexpected field size");
1579   __ li(R0, 0);
1580   __ stw(R0, JNIHandleBlock::top_offset_in_bytes(), active_handles);
1581 
1582   Label exception_return_sync_check_already_unlocked;
1583   __ ld(R0/*pending_exception*/, thread_(pending_exception));
1584   __ cmpdi(CCR0, R0/*pending_exception*/, 0);
1585   __ bne(CCR0, exception_return_sync_check_already_unlocked);
1586 
1587   //-----------------------------------------------------------------------------
1588   // No exception pending.
1589 
1590   // Move native method result back into proper registers and return.
1591   // Invoke result handler (may unbox/promote).
1592   __ ld(R11_scratch1, 0, R1_SP);
1593   __ ld(R3_RET, _ijava_state_neg(lresult), R11_scratch1);
1594   __ lfd(F1_RET, _ijava_state_neg(fresult), R11_scratch1);
1595   __ call_stub(result_handler_addr);
1596 
1597   __ merge_frames(/*top_frame_sp*/ R21_sender_SP, /*return_pc*/ R0, R11_scratch1, R12_scratch2);
1598 
1599   // Must use the return pc which was loaded from the caller's frame
1600   // as the VM uses return-pc-patching for deoptimization.
1601   __ mtlr(R0);
1602   __ blr();
1603 
1604   //-----------------------------------------------------------------------------
1605   // An exception is pending. We call into the runtime only if the
1606   // caller was not interpreted. If it was interpreted the
1607   // interpreter will do the correct thing. If it isn't interpreted
1608   // (call stub/compiled code) we will change our return and continue.
1609 
1610   BIND(exception_return_sync_check);
1611 
1612   if (synchronized) {
1613     // Don't check for exceptions since we're still in the i2n frame. Do that
1614     // manually afterwards.
1615     __ unlock_object(R26_monitor, false); // Can also unlock methods.
1616   }
1617   BIND(exception_return_sync_check_already_unlocked);
1618 
1619   const Register return_pc = R31;
1620 
1621   __ ld(return_pc, 0, R1_SP);
1622   __ ld(return_pc, _abi(lr), return_pc);
1623 
1624   // Get the address of the exception handler.
1625   __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::exception_handler_for_return_address),
1626                   R16_thread,
1627                   return_pc /* return pc */);
1628   __ merge_frames(/*top_frame_sp*/ R21_sender_SP, noreg, R11_scratch1, R12_scratch2);
1629 
1630   // Load the PC of the the exception handler into LR.
1631   __ mtlr(R3_RET);
1632 
1633   // Load exception into R3_ARG1 and clear pending exception in thread.
1634   __ ld(R3_ARG1/*exception*/, thread_(pending_exception));
1635   __ li(R4_ARG2, 0);
1636   __ std(R4_ARG2, thread_(pending_exception));
1637 
1638   // Load the original return pc into R4_ARG2.
1639   __ mr(R4_ARG2/*issuing_pc*/, return_pc);
1640 
1641   // Return to exception handler.
1642   __ blr();
1643 
1644   //=============================================================================
1645   // Counter overflow.
1646 
1647   if (inc_counter) {
1648     // Handle invocation counter overflow.
1649     __ bind(invocation_counter_overflow);
1650 
1651     generate_counter_overflow(continue_after_compile);
1652   }
1653 
1654   return entry;
1655 }
1656 
1657 // Generic interpreted method entry to (asm) interpreter.
1658 //
1659 address TemplateInterpreterGenerator::generate_normal_entry(bool synchronized) {
1660   bool inc_counter = UseCompiler || CountCompiledCalls || LogTouchedMethods;
1661   address entry = __ pc();
1662   // Generate the code to allocate the interpreter stack frame.
1663   Register Rsize_of_parameters = R4_ARG2, // Written by generate_fixed_frame.
1664            Rsize_of_locals     = R5_ARG3; // Written by generate_fixed_frame.
1665 
1666   // Does also a stack check to assure this frame fits on the stack.
1667   generate_fixed_frame(false, Rsize_of_parameters, Rsize_of_locals);
1668 
1669   // --------------------------------------------------------------------------
1670   // Zero out non-parameter locals.
1671   // Note: *Always* zero out non-parameter locals as Sparc does. It's not
1672   // worth to ask the flag, just do it.
1673   Register Rslot_addr = R6_ARG4,
1674            Rnum       = R7_ARG5;
1675   Label Lno_locals, Lzero_loop;
1676 
1677   // Set up the zeroing loop.
1678   __ subf(Rnum, Rsize_of_parameters, Rsize_of_locals);
1679   __ subf(Rslot_addr, Rsize_of_parameters, R18_locals);
1680   __ srdi_(Rnum, Rnum, Interpreter::logStackElementSize);
1681   __ beq(CCR0, Lno_locals);
1682   __ li(R0, 0);
1683   __ mtctr(Rnum);
1684 
1685   // The zero locals loop.
1686   __ bind(Lzero_loop);
1687   __ std(R0, 0, Rslot_addr);
1688   __ addi(Rslot_addr, Rslot_addr, -Interpreter::stackElementSize);
1689   __ bdnz(Lzero_loop);
1690 
1691   __ bind(Lno_locals);
1692 
1693   // --------------------------------------------------------------------------
1694   // Counter increment and overflow check.
1695   Label invocation_counter_overflow,
1696         profile_method,
1697         profile_method_continue;
1698   if (inc_counter || ProfileInterpreter) {
1699 
1700     Register Rdo_not_unlock_if_synchronized_addr = R11_scratch1;
1701     if (synchronized) {
1702       // Since at this point in the method invocation the exception handler
1703       // would try to exit the monitor of synchronized methods which hasn't
1704       // been entered yet, we set the thread local variable
1705       // _do_not_unlock_if_synchronized to true. If any exception was thrown by
1706       // runtime, exception handling i.e. unlock_if_synchronized_method will
1707       // check this thread local flag.
1708       // This flag has two effects, one is to force an unwind in the topmost
1709       // interpreter frame and not perform an unlock while doing so.
1710       __ li(R0, 1);
1711       __ stb(R0, in_bytes(JavaThread::do_not_unlock_if_synchronized_offset()), R16_thread);
1712     }
1713 
1714     // Argument and return type profiling.
1715     __ profile_parameters_type(R3_ARG1, R4_ARG2, R5_ARG3, R6_ARG4);
1716 
1717     // Increment invocation counter and check for overflow.
1718     if (inc_counter) {
1719       generate_counter_incr(&amp;invocation_counter_overflow, &amp;profile_method, &amp;profile_method_continue);
1720     }
1721 
1722     __ bind(profile_method_continue);
1723   }
1724 
1725   bang_stack_shadow_pages(false);
1726 
1727   if (inc_counter || ProfileInterpreter) {
1728     // Reset the _do_not_unlock_if_synchronized flag.
1729     if (synchronized) {
1730       __ li(R0, 0);
1731       __ stb(R0, in_bytes(JavaThread::do_not_unlock_if_synchronized_offset()), R16_thread);
1732     }
1733   }
1734 
1735   // --------------------------------------------------------------------------
1736   // Locking of synchronized methods. Must happen AFTER invocation_counter
1737   // check and stack overflow check, so method is not locked if overflows.
1738   if (synchronized) {
1739     lock_method(R3_ARG1, R4_ARG2, R5_ARG3);
1740   }
1741 #ifdef ASSERT
1742   else {
1743     Label Lok;
1744     __ lwz(R0, in_bytes(Method::access_flags_offset()), R19_method);
1745     __ andi_(R0, R0, JVM_ACC_SYNCHRONIZED);
1746     __ asm_assert_eq("method needs synchronization", 0x8521);
1747     __ bind(Lok);
1748   }
1749 #endif // ASSERT
1750 
1751   __ verify_thread();
1752 
1753   // --------------------------------------------------------------------------
1754   // JVMTI support
1755   __ notify_method_entry();
1756 
1757   // --------------------------------------------------------------------------
1758   // Start executing instructions.
1759   __ dispatch_next(vtos);
1760 
1761   // --------------------------------------------------------------------------
1762   // Out of line counter overflow and MDO creation code.
1763   if (ProfileInterpreter) {
1764     // We have decided to profile this method in the interpreter.
1765     __ bind(profile_method);
1766     __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::profile_method));
1767     __ set_method_data_pointer_for_bcp();
1768     __ b(profile_method_continue);
1769   }
1770 
1771   if (inc_counter) {
1772     // Handle invocation counter overflow.
1773     __ bind(invocation_counter_overflow);
1774     generate_counter_overflow(profile_method_continue);
1775   }
1776   return entry;
1777 }
1778 
1779 // CRC32 Intrinsics.
1780 //
1781 // Contract on scratch and work registers.
1782 // =======================================
1783 //
1784 // On ppc, the register set {R2..R12} is available in the interpreter as scratch/work registers.
1785 // You should, however, keep in mind that {R3_ARG1..R10_ARG8} is the C-ABI argument register set.
1786 // You can't rely on these registers across calls.
1787 //
1788 // The generators for CRC32_update and for CRC32_updateBytes use the
1789 // scratch/work register set internally, passing the work registers
1790 // as arguments to the MacroAssembler emitters as required.
1791 //
1792 // R3_ARG1..R6_ARG4 are preset to hold the incoming java arguments.
1793 // Their contents is not constant but may change according to the requirements
1794 // of the emitted code.
1795 //
1796 // All other registers from the scratch/work register set are used "internally"
1797 // and contain garbage (i.e. unpredictable values) once blr() is reached.
1798 // Basically, only R3_RET contains a defined value which is the function result.
1799 //
1800 /**
1801  * Method entry for static native methods:
1802  *   int java.util.zip.CRC32.update(int crc, int b)
1803  */
1804 address TemplateInterpreterGenerator::generate_CRC32_update_entry() {
1805   if (UseCRC32Intrinsics) {
1806     address start = __ pc();  // Remember stub start address (is rtn value).
1807     Label slow_path;
1808 
1809     // Safepoint check
1810     const Register sync_state = R11_scratch1;
1811     int sync_state_offs = __ load_const_optimized(sync_state, SafepointSynchronize::address_of_state(), /*temp*/R0, true);
1812     __ lwz(sync_state, sync_state_offs, sync_state);
1813     __ cmpwi(CCR0, sync_state, SafepointSynchronize::_not_synchronized);
1814     __ bne(CCR0, slow_path);
1815 
1816     // We don't generate local frame and don't align stack because
1817     // we not even call stub code (we generate the code inline)
1818     // and there is no safepoint on this path.
1819 
1820     // Load java parameters.
1821     // R15_esp is callers operand stack pointer, i.e. it points to the parameters.
1822     const Register argP    = R15_esp;
1823     const Register crc     = R3_ARG1;  // crc value
1824     const Register data    = R4_ARG2;  // address of java byte value (kernel_crc32 needs address)
1825     const Register dataLen = R5_ARG3;  // source data len (1 byte). Not used because calling the single-byte emitter.
1826     const Register table   = R6_ARG4;  // address of crc32 table
1827     const Register tmp     = dataLen;  // Reuse unused len register to show we don't actually need a separate tmp here.
1828 
1829     BLOCK_COMMENT("CRC32_update {");
1830 
1831     // Arguments are reversed on java expression stack
1832 #ifdef VM_LITTLE_ENDIAN
1833     __ addi(data, argP, 0+1*wordSize); // (stack) address of byte value. Emitter expects address, not value.
1834                                        // Being passed as an int, the single byte is at offset +0.
1835 #else
1836     __ addi(data, argP, 3+1*wordSize); // (stack) address of byte value. Emitter expects address, not value.
1837                                        // Being passed from java as an int, the single byte is at offset +3.
1838 #endif
1839     __ lwz(crc,  2*wordSize, argP);    // Current crc state, zero extend to 64 bit to have a clean register.
1840 
1841     StubRoutines::ppc64::generate_load_crc_table_addr(_masm, table);
1842     __ kernel_crc32_singleByte(crc, data, dataLen, table, tmp);
1843 
1844     // Restore caller sp for c2i case and return.
1845     __ mr(R1_SP, R21_sender_SP); // Cut the stack back to where the caller started.
1846     __ blr();
1847 
1848     // Generate a vanilla native entry as the slow path.
1849     BLOCK_COMMENT("} CRC32_update");
1850     BIND(slow_path);
1851     __ jump_to_entry(Interpreter::entry_for_kind(Interpreter::native), R11_scratch1);
1852     return start;
1853   }
1854 
1855   return NULL;
1856 }
1857 
1858 // CRC32 Intrinsics.
1859 /**
1860  * Method entry for static native methods:
1861  *   int java.util.zip.CRC32.updateBytes(     int crc, byte[] b,  int off, int len)
1862  *   int java.util.zip.CRC32.updateByteBuffer(int crc, long* buf, int off, int len)
1863  */
1864 address TemplateInterpreterGenerator::generate_CRC32_updateBytes_entry(AbstractInterpreter::MethodKind kind) {
1865   if (UseCRC32Intrinsics) {
1866     address start = __ pc();  // Remember stub start address (is rtn value).
1867     Label slow_path;
1868 
1869     // Safepoint check
1870     const Register sync_state = R11_scratch1;
1871     int sync_state_offs = __ load_const_optimized(sync_state, SafepointSynchronize::address_of_state(), /*temp*/R0, true);
1872     __ lwz(sync_state, sync_state_offs, sync_state);
1873     __ cmpwi(CCR0, sync_state, SafepointSynchronize::_not_synchronized);
1874     __ bne(CCR0, slow_path);
1875 
1876     // We don't generate local frame and don't align stack because
1877     // we not even call stub code (we generate the code inline)
1878     // and there is no safepoint on this path.
1879 
1880     // Load parameters.
1881     // Z_esp is callers operand stack pointer, i.e. it points to the parameters.
1882     const Register argP    = R15_esp;
1883     const Register crc     = R3_ARG1;  // crc value
1884     const Register data    = R4_ARG2;  // address of java byte array
1885     const Register dataLen = R5_ARG3;  // source data len
1886     const Register table   = R6_ARG4;  // address of crc32 table
1887 
1888     const Register t0      = R9;       // scratch registers for crc calculation
1889     const Register t1      = R10;
1890     const Register t2      = R11;
1891     const Register t3      = R12;
1892 
1893     const Register tc0     = R2;       // registers to hold pre-calculated column addresses
1894     const Register tc1     = R7;
1895     const Register tc2     = R8;
1896     const Register tc3     = table;    // table address is reconstructed at the end of kernel_crc32_* emitters
1897 
1898     const Register tmp     = t0;       // Only used very locally to calculate byte buffer address.
1899 
1900     // Arguments are reversed on java expression stack.
1901     // Calculate address of start element.
1902     if (kind == Interpreter::java_util_zip_CRC32_updateByteBuffer) { // Used for "updateByteBuffer direct".
1903       BLOCK_COMMENT("CRC32_updateByteBuffer {");
1904       // crc     @ (SP + 5W) (32bit)
1905       // buf     @ (SP + 3W) (64bit ptr to long array)
1906       // off     @ (SP + 2W) (32bit)
1907       // dataLen @ (SP + 1W) (32bit)
1908       // data = buf + off
1909       __ ld(  data,    3*wordSize, argP);  // start of byte buffer
1910       __ lwa( tmp,     2*wordSize, argP);  // byte buffer offset
1911       __ lwa( dataLen, 1*wordSize, argP);  // #bytes to process
1912       __ lwz( crc,     5*wordSize, argP);  // current crc state
1913       __ add( data, data, tmp);            // Add byte buffer offset.
1914     } else {                                                         // Used for "updateBytes update".
1915       BLOCK_COMMENT("CRC32_updateBytes {");
1916       // crc     @ (SP + 4W) (32bit)
1917       // buf     @ (SP + 3W) (64bit ptr to byte array)
1918       // off     @ (SP + 2W) (32bit)
1919       // dataLen @ (SP + 1W) (32bit)
1920       // data = buf + off + base_offset
1921       __ ld(  data,    3*wordSize, argP);  // start of byte buffer
1922       __ lwa( tmp,     2*wordSize, argP);  // byte buffer offset
1923       __ lwa( dataLen, 1*wordSize, argP);  // #bytes to process
1924       __ add( data, data, tmp);            // add byte buffer offset
1925       __ lwz( crc,     4*wordSize, argP);  // current crc state
1926       __ addi(data, data, arrayOopDesc::base_offset_in_bytes(T_BYTE));
1927     }
1928 
1929     StubRoutines::ppc64::generate_load_crc_table_addr(_masm, table);
1930 
1931     // Performance measurements show the 1word and 2word variants to be almost equivalent,
1932     // with very light advantages for the 1word variant. We chose the 1word variant for
1933     // code compactness.
1934     __ kernel_crc32_1word(crc, data, dataLen, table, t0, t1, t2, t3, tc0, tc1, tc2, tc3);
1935 
1936     // Restore caller sp for c2i case and return.
1937     __ mr(R1_SP, R21_sender_SP); // Cut the stack back to where the caller started.
1938     __ blr();
1939 
1940     // Generate a vanilla native entry as the slow path.
1941     BLOCK_COMMENT("} CRC32_updateBytes(Buffer)");
1942     BIND(slow_path);
1943     __ jump_to_entry(Interpreter::entry_for_kind(Interpreter::native), R11_scratch1);
1944     return start;
1945   }
1946 
1947   return NULL;
1948 }
1949 
1950 // Not supported
1951 address TemplateInterpreterGenerator::generate_CRC32C_updateBytes_entry(AbstractInterpreter::MethodKind kind) {
1952   return NULL;
1953 }
1954 
1955 // =============================================================================
1956 // Exceptions
1957 
1958 void TemplateInterpreterGenerator::generate_throw_exception() {
1959   Register Rexception    = R17_tos,
1960            Rcontinuation = R3_RET;
1961 
1962   // --------------------------------------------------------------------------
1963   // Entry point if an method returns with a pending exception (rethrow).
1964   Interpreter::_rethrow_exception_entry = __ pc();
1965   {
1966     __ restore_interpreter_state(R11_scratch1); // Sets R11_scratch1 = fp.
1967     __ ld(R12_scratch2, _ijava_state_neg(top_frame_sp), R11_scratch1);
1968     __ resize_frame_absolute(R12_scratch2, R11_scratch1, R0);
1969 
1970     // Compiled code destroys templateTableBase, reload.
1971     __ load_const_optimized(R25_templateTableBase, (address)Interpreter::dispatch_table((TosState)0), R11_scratch1);
1972   }
1973 
1974   // Entry point if a interpreted method throws an exception (throw).
1975   Interpreter::_throw_exception_entry = __ pc();
1976   {
1977     __ mr(Rexception, R3_RET);
1978 
1979     __ verify_thread();
1980     __ verify_oop(Rexception);
1981 
1982     // Expression stack must be empty before entering the VM in case of an exception.
1983     __ empty_expression_stack();
1984     // Find exception handler address and preserve exception oop.
1985     // Call C routine to find handler and jump to it.
1986     __ call_VM(Rexception, CAST_FROM_FN_PTR(address, InterpreterRuntime::exception_handler_for_exception), Rexception);
1987     __ mtctr(Rcontinuation);
1988     // Push exception for exception handler bytecodes.
1989     __ push_ptr(Rexception);
1990 
1991     // Jump to exception handler (may be remove activation entry!).
1992     __ bctr();
1993   }
1994 
1995   // If the exception is not handled in the current frame the frame is
1996   // removed and the exception is rethrown (i.e. exception
1997   // continuation is _rethrow_exception).
1998   //
1999   // Note: At this point the bci is still the bxi for the instruction
2000   // which caused the exception and the expression stack is
2001   // empty. Thus, for any VM calls at this point, GC will find a legal
2002   // oop map (with empty expression stack).
2003 
2004   // In current activation
2005   // tos: exception
2006   // bcp: exception bcp
2007 
2008   // --------------------------------------------------------------------------
2009   // JVMTI PopFrame support
2010 
2011   Interpreter::_remove_activation_preserving_args_entry = __ pc();
2012   {
2013     // Set the popframe_processing bit in popframe_condition indicating that we are
2014     // currently handling popframe, so that call_VMs that may happen later do not
2015     // trigger new popframe handling cycles.
2016     __ lwz(R11_scratch1, in_bytes(JavaThread::popframe_condition_offset()), R16_thread);
2017     __ ori(R11_scratch1, R11_scratch1, JavaThread::popframe_processing_bit);
2018     __ stw(R11_scratch1, in_bytes(JavaThread::popframe_condition_offset()), R16_thread);
2019 
2020     // Empty the expression stack, as in normal exception handling.
2021     __ empty_expression_stack();
2022     __ unlock_if_synchronized_method(vtos, /* throw_monitor_exception */ false, /* install_monitor_exception */ false);
2023 
2024     // Check to see whether we are returning to a deoptimized frame.
2025     // (The PopFrame call ensures that the caller of the popped frame is
2026     // either interpreted or compiled and deoptimizes it if compiled.)
2027     // Note that we don't compare the return PC against the
2028     // deoptimization blob's unpack entry because of the presence of
2029     // adapter frames in C2.
2030     Label Lcaller_not_deoptimized;
2031     Register return_pc = R3_ARG1;
2032     __ ld(return_pc, 0, R1_SP);
2033     __ ld(return_pc, _abi(lr), return_pc);
2034     __ call_VM_leaf(CAST_FROM_FN_PTR(address, InterpreterRuntime::interpreter_contains), return_pc);
2035     __ cmpdi(CCR0, R3_RET, 0);
2036     __ bne(CCR0, Lcaller_not_deoptimized);
2037 
2038     // The deoptimized case.
2039     // In this case, we can't call dispatch_next() after the frame is
2040     // popped, but instead must save the incoming arguments and restore
2041     // them after deoptimization has occurred.
2042     __ ld(R4_ARG2, in_bytes(Method::const_offset()), R19_method);
2043     __ lhz(R4_ARG2 /* number of params */, in_bytes(ConstMethod::size_of_parameters_offset()), R4_ARG2);
2044     __ slwi(R4_ARG2, R4_ARG2, Interpreter::logStackElementSize);
2045     __ addi(R5_ARG3, R18_locals, Interpreter::stackElementSize);
2046     __ subf(R5_ARG3, R4_ARG2, R5_ARG3);
2047     // Save these arguments.
2048     __ call_VM_leaf(CAST_FROM_FN_PTR(address, Deoptimization::popframe_preserve_args), R16_thread, R4_ARG2, R5_ARG3);
2049 
2050     // Inform deoptimization that it is responsible for restoring these arguments.
2051     __ load_const_optimized(R11_scratch1, JavaThread::popframe_force_deopt_reexecution_bit);
2052     __ stw(R11_scratch1, in_bytes(JavaThread::popframe_condition_offset()), R16_thread);
2053 
2054     // Return from the current method into the deoptimization blob. Will eventually
2055     // end up in the deopt interpeter entry, deoptimization prepared everything that
2056     // we will reexecute the call that called us.
2057     __ merge_frames(/*top_frame_sp*/ R21_sender_SP, /*reload return_pc*/ return_pc, R11_scratch1, R12_scratch2);
2058     __ mtlr(return_pc);
2059     __ blr();
2060 
2061     // The non-deoptimized case.
2062     __ bind(Lcaller_not_deoptimized);
2063 
2064     // Clear the popframe condition flag.
2065     __ li(R0, 0);
2066     __ stw(R0, in_bytes(JavaThread::popframe_condition_offset()), R16_thread);
2067 
2068     // Get out of the current method and re-execute the call that called us.
2069     __ merge_frames(/*top_frame_sp*/ R21_sender_SP, /*return_pc*/ noreg, R11_scratch1, R12_scratch2);
2070     __ restore_interpreter_state(R11_scratch1);
2071     __ ld(R12_scratch2, _ijava_state_neg(top_frame_sp), R11_scratch1);
2072     __ resize_frame_absolute(R12_scratch2, R11_scratch1, R0);
2073     if (ProfileInterpreter) {
2074       __ set_method_data_pointer_for_bcp();
2075       __ ld(R11_scratch1, 0, R1_SP);
2076       __ std(R28_mdx, _ijava_state_neg(mdx), R11_scratch1);
2077     }
2078 #if INCLUDE_JVMTI
2079     Label L_done;
2080 
2081     __ lbz(R11_scratch1, 0, R14_bcp);
2082     __ cmpwi(CCR0, R11_scratch1, Bytecodes::_invokestatic);
2083     __ bne(CCR0, L_done);
2084 
2085     // The member name argument must be restored if _invokestatic is re-executed after a PopFrame call.
2086     // Detect such a case in the InterpreterRuntime function and return the member name argument, or NULL.
2087     __ ld(R4_ARG2, 0, R18_locals);
2088     __ MacroAssembler::call_VM(R4_ARG2, CAST_FROM_FN_PTR(address, InterpreterRuntime::member_name_arg_or_null), R4_ARG2, R19_method, R14_bcp, false);
2089     __ restore_interpreter_state(R11_scratch1, /*bcp_and_mdx_only*/ true);
2090     __ cmpdi(CCR0, R4_ARG2, 0);
2091     __ beq(CCR0, L_done);
2092     __ std(R4_ARG2, wordSize, R15_esp);
2093     __ bind(L_done);
2094 #endif // INCLUDE_JVMTI
2095     __ dispatch_next(vtos);
2096   }
2097   // end of JVMTI PopFrame support
2098 
2099   // --------------------------------------------------------------------------
2100   // Remove activation exception entry.
2101   // This is jumped to if an interpreted method can't handle an exception itself
2102   // (we come from the throw/rethrow exception entry above). We're going to call
2103   // into the VM to find the exception handler in the caller, pop the current
2104   // frame and return the handler we calculated.
2105   Interpreter::_remove_activation_entry = __ pc();
2106   {
2107     __ pop_ptr(Rexception);
2108     __ verify_thread();
2109     __ verify_oop(Rexception);
2110     __ std(Rexception, in_bytes(JavaThread::vm_result_offset()), R16_thread);
2111 
2112     __ unlock_if_synchronized_method(vtos, /* throw_monitor_exception */ false, true);
2113     __ notify_method_exit(false, vtos, InterpreterMacroAssembler::SkipNotifyJVMTI, false);
2114 
2115     __ get_vm_result(Rexception);
2116 
2117     // We are done with this activation frame; find out where to go next.
2118     // The continuation point will be an exception handler, which expects
2119     // the following registers set up:
2120     //
2121     // RET:  exception oop
2122     // ARG2: Issuing PC (see generate_exception_blob()), only used if the caller is compiled.
2123 
2124     Register return_pc = R31; // Needs to survive the runtime call.
2125     __ ld(return_pc, 0, R1_SP);
2126     __ ld(return_pc, _abi(lr), return_pc);
2127     __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::exception_handler_for_return_address), R16_thread, return_pc);
2128 
2129     // Remove the current activation.
2130     __ merge_frames(/*top_frame_sp*/ R21_sender_SP, /*return_pc*/ noreg, R11_scratch1, R12_scratch2);
2131 
2132     __ mr(R4_ARG2, return_pc);
2133     __ mtlr(R3_RET);
2134     __ mr(R3_RET, Rexception);
2135     __ blr();
2136   }
2137 }
2138 
2139 // JVMTI ForceEarlyReturn support.
2140 // Returns "in the middle" of a method with a "fake" return value.
2141 address TemplateInterpreterGenerator::generate_earlyret_entry_for(TosState state) {
2142 
2143   Register Rscratch1 = R11_scratch1,
2144            Rscratch2 = R12_scratch2;
2145 
2146   address entry = __ pc();
2147   __ empty_expression_stack();
2148 
2149   __ load_earlyret_value(state, Rscratch1);
2150 
2151   __ ld(Rscratch1, in_bytes(JavaThread::jvmti_thread_state_offset()), R16_thread);
2152   // Clear the earlyret state.
2153   __ li(R0, 0);
2154   __ stw(R0, in_bytes(JvmtiThreadState::earlyret_state_offset()), Rscratch1);
2155 
2156   __ remove_activation(state, false, false);
2157   // Copied from TemplateTable::_return.
2158   // Restoration of lr done by remove_activation.
2159   switch (state) {
2160     // Narrow result if state is itos but result type is smaller.
2161     case btos:
2162     case ztos:
2163     case ctos:
2164     case stos:
2165     case itos: __ narrow(R17_tos); /* fall through */
2166     case ltos:
2167     case atos: __ mr(R3_RET, R17_tos); break;
2168     case ftos:
2169     case dtos: __ fmr(F1_RET, F15_ftos); break;
2170     case vtos: // This might be a constructor. Final fields (and volatile fields on PPC64) need
2171                // to get visible before the reference to the object gets stored anywhere.
2172                __ membar(Assembler::StoreStore); break;
2173     default  : ShouldNotReachHere();
2174   }
2175   __ blr();
2176 
2177   return entry;
2178 } // end of ForceEarlyReturn support
2179 
2180 //-----------------------------------------------------------------------------
2181 // Helper for vtos entry point generation
2182 
2183 void TemplateInterpreterGenerator::set_vtos_entry_points(Template* t,
2184                                                          address&amp; bep,
2185                                                          address&amp; cep,
2186                                                          address&amp; sep,
2187                                                          address&amp; aep,
2188                                                          address&amp; iep,
2189                                                          address&amp; lep,
2190                                                          address&amp; fep,
2191                                                          address&amp; dep,
2192                                                          address&amp; vep) {
2193   assert(t-&gt;is_valid() &amp;&amp; t-&gt;tos_in() == vtos, "illegal template");
2194   Label L;
2195 
2196   aep = __ pc();  __ push_ptr();  __ b(L);
2197   fep = __ pc();  __ push_f();    __ b(L);
2198   dep = __ pc();  __ push_d();    __ b(L);
2199   lep = __ pc();  __ push_l();    __ b(L);
2200   __ align(32, 12, 24); // align L
2201   bep = cep = sep =
2202   iep = __ pc();  __ push_i();
2203   vep = __ pc();
2204   __ bind(L);
2205   generate_and_dispatch(t);
2206 }
2207 
2208 //-----------------------------------------------------------------------------
2209 
2210 // Non-product code
2211 #ifndef PRODUCT
2212 address TemplateInterpreterGenerator::generate_trace_code(TosState state) {
2213   //__ flush_bundle();
2214   address entry = __ pc();
2215 
2216   const char *bname = NULL;
2217   uint tsize = 0;
2218   switch(state) {
2219   case ftos:
2220     bname = "trace_code_ftos {";
2221     tsize = 2;
2222     break;
2223   case btos:
2224     bname = "trace_code_btos {";
2225     tsize = 2;
2226     break;
2227   case ztos:
2228     bname = "trace_code_ztos {";
2229     tsize = 2;
2230     break;
2231   case ctos:
2232     bname = "trace_code_ctos {";
2233     tsize = 2;
2234     break;
2235   case stos:
2236     bname = "trace_code_stos {";
2237     tsize = 2;
2238     break;
2239   case itos:
2240     bname = "trace_code_itos {";
2241     tsize = 2;
2242     break;
2243   case ltos:
2244     bname = "trace_code_ltos {";
2245     tsize = 3;
2246     break;
2247   case atos:
2248     bname = "trace_code_atos {";
2249     tsize = 2;
2250     break;
2251   case vtos:
2252     // Note: In case of vtos, the topmost of stack value could be a int or doubl
2253     // In case of a double (2 slots) we won't see the 2nd stack value.
2254     // Maybe we simply should print the topmost 3 stack slots to cope with the problem.
2255     bname = "trace_code_vtos {";
2256     tsize = 2;
2257 
2258     break;
2259   case dtos:
2260     bname = "trace_code_dtos {";
2261     tsize = 3;
2262     break;
2263   default:
2264     ShouldNotReachHere();
2265   }
2266   BLOCK_COMMENT(bname);
2267 
2268   // Support short-cut for TraceBytecodesAt.
2269   // Don't call into the VM if we don't want to trace to speed up things.
2270   Label Lskip_vm_call;
2271   if (TraceBytecodesAt &gt; 0 &amp;&amp; TraceBytecodesAt &lt; max_intx) {
2272     int offs1 = __ load_const_optimized(R11_scratch1, (address) &amp;TraceBytecodesAt, R0, true);
2273     int offs2 = __ load_const_optimized(R12_scratch2, (address) &amp;BytecodeCounter::_counter_value, R0, true);
2274     __ ld(R11_scratch1, offs1, R11_scratch1);
2275     __ lwa(R12_scratch2, offs2, R12_scratch2);
2276     __ cmpd(CCR0, R12_scratch2, R11_scratch1);
2277     __ blt(CCR0, Lskip_vm_call);
2278   }
2279 
2280   __ push(state);
2281   // Load 2 topmost expression stack values.
2282   __ ld(R6_ARG4, tsize*Interpreter::stackElementSize, R15_esp);
2283   __ ld(R5_ARG3, Interpreter::stackElementSize, R15_esp);
2284   __ mflr(R31);
2285   __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::trace_bytecode), /* unused */ R4_ARG2, R5_ARG3, R6_ARG4, false);
2286   __ mtlr(R31);
2287   __ pop(state);
2288 
2289   if (TraceBytecodesAt &gt; 0 &amp;&amp; TraceBytecodesAt &lt; max_intx) {
2290     __ bind(Lskip_vm_call);
2291   }
2292   __ blr();
2293   BLOCK_COMMENT("} trace_code");
2294   return entry;
2295 }
2296 
2297 void TemplateInterpreterGenerator::count_bytecode() {
2298   int offs = __ load_const_optimized(R11_scratch1, (address) &amp;BytecodeCounter::_counter_value, R12_scratch2, true);
2299   __ lwz(R12_scratch2, offs, R11_scratch1);
2300   __ addi(R12_scratch2, R12_scratch2, 1);
2301   __ stw(R12_scratch2, offs, R11_scratch1);
2302 }
2303 
2304 void TemplateInterpreterGenerator::histogram_bytecode(Template* t) {
2305   int offs = __ load_const_optimized(R11_scratch1, (address) &amp;BytecodeHistogram::_counters[t-&gt;bytecode()], R12_scratch2, true);
2306   __ lwz(R12_scratch2, offs, R11_scratch1);
2307   __ addi(R12_scratch2, R12_scratch2, 1);
2308   __ stw(R12_scratch2, offs, R11_scratch1);
2309 }
2310 
2311 void TemplateInterpreterGenerator::histogram_bytecode_pair(Template* t) {
2312   const Register addr = R11_scratch1,
2313                  tmp  = R12_scratch2;
2314   // Get index, shift out old bytecode, bring in new bytecode, and store it.
2315   // _index = (_index &gt;&gt; log2_number_of_codes) |
2316   //          (bytecode &lt;&lt; log2_number_of_codes);
2317   int offs1 = __ load_const_optimized(addr, (address)&amp;BytecodePairHistogram::_index, tmp, true);
2318   __ lwz(tmp, offs1, addr);
2319   __ srwi(tmp, tmp, BytecodePairHistogram::log2_number_of_codes);
2320   __ ori(tmp, tmp, ((int) t-&gt;bytecode()) &lt;&lt; BytecodePairHistogram::log2_number_of_codes);
2321   __ stw(tmp, offs1, addr);
2322 
2323   // Bump bucket contents.
2324   // _counters[_index] ++;
2325   int offs2 = __ load_const_optimized(addr, (address)&amp;BytecodePairHistogram::_counters, R0, true);
2326   __ sldi(tmp, tmp, LogBytesPerInt);
2327   __ add(addr, tmp, addr);
2328   __ lwz(tmp, offs2, addr);
2329   __ addi(tmp, tmp, 1);
2330   __ stw(tmp, offs2, addr);
2331 }
2332 
2333 void TemplateInterpreterGenerator::trace_bytecode(Template* t) {
2334   // Call a little run-time stub to avoid blow-up for each bytecode.
2335   // The run-time runtime saves the right registers, depending on
2336   // the tosca in-state for the given template.
2337 
2338   assert(Interpreter::trace_code(t-&gt;tos_in()) != NULL,
2339          "entry must have been generated");
2340 
2341   // Note: we destroy LR here.
2342   __ bl(Interpreter::trace_code(t-&gt;tos_in()));
2343 }
2344 
2345 void TemplateInterpreterGenerator::stop_interpreter_at() {
2346   Label L;
2347   int offs1 = __ load_const_optimized(R11_scratch1, (address) &amp;StopInterpreterAt, R0, true);
2348   int offs2 = __ load_const_optimized(R12_scratch2, (address) &amp;BytecodeCounter::_counter_value, R0, true);
2349   __ ld(R11_scratch1, offs1, R11_scratch1);
2350   __ lwa(R12_scratch2, offs2, R12_scratch2);
2351   __ cmpd(CCR0, R12_scratch2, R11_scratch1);
2352   __ bne(CCR0, L);
2353   __ illtrap();
2354   __ bind(L);
2355 }
2356 
2357 #endif // !PRODUCT
</pre></body></html>
