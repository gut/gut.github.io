<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta charset="utf-8">
<meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

    <script type="text/javascript" src="../../../../ancnav.js"></script>
    </head>
    <body id="SUNWwebrev" onkeypress="keypress(event);">
    <a name="0"></a>
    <pre>rev 8345 : Backport of "<a href="https://bugs.openjdk.java.net/browse/JDK-8145913">8145913</a>: PPC64: add Montgomery multiply intrinsic"</pre><hr></hr>
<pre>
   1 /*
   2  * Copyright (c) 1997, 2013, Oracle and/or its affiliates. All rights reserved.
   3  * Copyright 2012, 2014 SAP AG. All rights reserved.
   4  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   5  *
   6  * This code is free software; you can redistribute it and/or modify it
   7  * under the terms of the GNU General Public License version 2 only, as
   8  * published by the Free Software Foundation.
   9  *
  10  * This code is distributed in the hope that it will be useful, but WITHOUT
  11  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  12  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  13  * version 2 for more details (a copy is included in the LICENSE file that
  14  * accompanied this code).
  15  *
  16  * You should have received a copy of the GNU General Public License version
  17  * 2 along with this work; if not, write to the Free Software Foundation,
  18  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  19  *
  20  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  21  * or visit www.oracle.com if you need additional information or have any
  22  * questions.
  23  *
  24  */
  25 
  26 #include "precompiled.hpp"
  27 #include "asm/macroAssembler.inline.hpp"
  28 #include "code/debugInfoRec.hpp"
  29 #include "code/icBuffer.hpp"
  30 #include "code/vtableStubs.hpp"
  31 #include "interpreter/interpreter.hpp"
  32 #include "oops/compiledICHolder.hpp"
  33 #include "prims/jvmtiRedefineClassesTrace.hpp"
  34 #include "runtime/sharedRuntime.hpp"
  35 #include "runtime/vframeArray.hpp"
  36 #include "vmreg_ppc.inline.hpp"
  37 #include "adfiles/ad_ppc_64.hpp"
  38 #ifdef COMPILER1
  39 #include "c1/c1_Runtime1.hpp"
  40 #endif
  41 #ifdef COMPILER2
  42 #include "opto/runtime.hpp"
  43 #endif
  44 
<a name="1" id="anc1"></a>

  45 #define __ masm-&gt;
  46 
  47 #ifdef PRODUCT
  48 #define BLOCK_COMMENT(str) // nothing
  49 #else
  50 #define BLOCK_COMMENT(str) __ block_comment(str)
  51 #endif
  52 
  53 #define BIND(label) bind(label); BLOCK_COMMENT(#label ":")
  54 
  55 
  56 class RegisterSaver {
  57  // Used for saving volatile registers.
  58  public:
  59 
  60   // Support different return pc locations.
  61   enum ReturnPCLocation {
  62     return_pc_is_lr,
  63     return_pc_is_r4,
  64     return_pc_is_thread_saved_exception_pc
  65   };
  66 
  67   static OopMap* push_frame_reg_args_and_save_live_registers(MacroAssembler* masm,
  68                          int* out_frame_size_in_bytes,
  69                          bool generate_oop_map,
  70                          int return_pc_adjustment,
  71                          ReturnPCLocation return_pc_location);
  72   static void    restore_live_registers_and_pop_frame(MacroAssembler* masm,
  73                          int frame_size_in_bytes,
  74                          bool restore_ctr);
  75 
  76   static void push_frame_and_save_argument_registers(MacroAssembler* masm,
  77                          Register r_temp,
  78                          int frame_size,
  79                          int total_args,
  80                          const VMRegPair *regs, const VMRegPair *regs2 = NULL);
  81   static void restore_argument_registers_and_pop_frame(MacroAssembler*masm,
  82                          int frame_size,
  83                          int total_args,
  84                          const VMRegPair *regs, const VMRegPair *regs2 = NULL);
  85 
  86   // During deoptimization only the result registers need to be restored
  87   // all the other values have already been extracted.
  88   static void restore_result_registers(MacroAssembler* masm, int frame_size_in_bytes);
  89 
  90   // Constants and data structures:
  91 
  92   typedef enum {
  93     int_reg           = 0,
  94     float_reg         = 1,
  95     special_reg       = 2
  96   } RegisterType;
  97 
  98   typedef enum {
  99     reg_size          = 8,
 100     half_reg_size     = reg_size / 2,
 101   } RegisterConstants;
 102 
 103   typedef struct {
 104     RegisterType        reg_type;
 105     int                 reg_num;
 106     VMReg               vmreg;
 107   } LiveRegType;
 108 };
 109 
 110 
 111 #define RegisterSaver_LiveSpecialReg(regname) \
 112   { RegisterSaver::special_reg, regname-&gt;encoding(), regname-&gt;as_VMReg() }
 113 
 114 #define RegisterSaver_LiveIntReg(regname) \
 115   { RegisterSaver::int_reg,     regname-&gt;encoding(), regname-&gt;as_VMReg() }
 116 
 117 #define RegisterSaver_LiveFloatReg(regname) \
 118   { RegisterSaver::float_reg,   regname-&gt;encoding(), regname-&gt;as_VMReg() }
 119 
 120 static const RegisterSaver::LiveRegType RegisterSaver_LiveRegs[] = {
 121   // Live registers which get spilled to the stack. Register
 122   // positions in this array correspond directly to the stack layout.
 123 
 124   //
 125   // live special registers:
 126   //
 127   RegisterSaver_LiveSpecialReg(SR_CTR),
 128   //
 129   // live float registers:
 130   //
 131   RegisterSaver_LiveFloatReg( F0  ),
 132   RegisterSaver_LiveFloatReg( F1  ),
 133   RegisterSaver_LiveFloatReg( F2  ),
 134   RegisterSaver_LiveFloatReg( F3  ),
 135   RegisterSaver_LiveFloatReg( F4  ),
 136   RegisterSaver_LiveFloatReg( F5  ),
 137   RegisterSaver_LiveFloatReg( F6  ),
 138   RegisterSaver_LiveFloatReg( F7  ),
 139   RegisterSaver_LiveFloatReg( F8  ),
 140   RegisterSaver_LiveFloatReg( F9  ),
 141   RegisterSaver_LiveFloatReg( F10 ),
 142   RegisterSaver_LiveFloatReg( F11 ),
 143   RegisterSaver_LiveFloatReg( F12 ),
 144   RegisterSaver_LiveFloatReg( F13 ),
 145   RegisterSaver_LiveFloatReg( F14 ),
 146   RegisterSaver_LiveFloatReg( F15 ),
 147   RegisterSaver_LiveFloatReg( F16 ),
 148   RegisterSaver_LiveFloatReg( F17 ),
 149   RegisterSaver_LiveFloatReg( F18 ),
 150   RegisterSaver_LiveFloatReg( F19 ),
 151   RegisterSaver_LiveFloatReg( F20 ),
 152   RegisterSaver_LiveFloatReg( F21 ),
 153   RegisterSaver_LiveFloatReg( F22 ),
 154   RegisterSaver_LiveFloatReg( F23 ),
 155   RegisterSaver_LiveFloatReg( F24 ),
 156   RegisterSaver_LiveFloatReg( F25 ),
 157   RegisterSaver_LiveFloatReg( F26 ),
 158   RegisterSaver_LiveFloatReg( F27 ),
 159   RegisterSaver_LiveFloatReg( F28 ),
 160   RegisterSaver_LiveFloatReg( F29 ),
 161   RegisterSaver_LiveFloatReg( F30 ),
 162   RegisterSaver_LiveFloatReg( F31 ),
 163   //
 164   // live integer registers:
 165   //
 166   RegisterSaver_LiveIntReg(   R0  ),
 167   //RegisterSaver_LiveIntReg( R1  ), // stack pointer
 168   RegisterSaver_LiveIntReg(   R2  ),
 169   RegisterSaver_LiveIntReg(   R3  ),
 170   RegisterSaver_LiveIntReg(   R4  ),
 171   RegisterSaver_LiveIntReg(   R5  ),
 172   RegisterSaver_LiveIntReg(   R6  ),
 173   RegisterSaver_LiveIntReg(   R7  ),
 174   RegisterSaver_LiveIntReg(   R8  ),
 175   RegisterSaver_LiveIntReg(   R9  ),
 176   RegisterSaver_LiveIntReg(   R10 ),
 177   RegisterSaver_LiveIntReg(   R11 ),
 178   RegisterSaver_LiveIntReg(   R12 ),
 179   //RegisterSaver_LiveIntReg( R13 ), // system thread id
 180   RegisterSaver_LiveIntReg(   R14 ),
 181   RegisterSaver_LiveIntReg(   R15 ),
 182   RegisterSaver_LiveIntReg(   R16 ),
 183   RegisterSaver_LiveIntReg(   R17 ),
 184   RegisterSaver_LiveIntReg(   R18 ),
 185   RegisterSaver_LiveIntReg(   R19 ),
 186   RegisterSaver_LiveIntReg(   R20 ),
 187   RegisterSaver_LiveIntReg(   R21 ),
 188   RegisterSaver_LiveIntReg(   R22 ),
 189   RegisterSaver_LiveIntReg(   R23 ),
 190   RegisterSaver_LiveIntReg(   R24 ),
 191   RegisterSaver_LiveIntReg(   R25 ),
 192   RegisterSaver_LiveIntReg(   R26 ),
 193   RegisterSaver_LiveIntReg(   R27 ),
 194   RegisterSaver_LiveIntReg(   R28 ),
 195   RegisterSaver_LiveIntReg(   R29 ),
 196   RegisterSaver_LiveIntReg(   R31 ),
 197   RegisterSaver_LiveIntReg(   R30 ), // r30 must be the last register
 198 };
 199 
 200 OopMap* RegisterSaver::push_frame_reg_args_and_save_live_registers(MacroAssembler* masm,
 201                          int* out_frame_size_in_bytes,
 202                          bool generate_oop_map,
 203                          int return_pc_adjustment,
 204                          ReturnPCLocation return_pc_location) {
 205   // Push an abi_reg_args-frame and store all registers which may be live.
 206   // If requested, create an OopMap: Record volatile registers as
 207   // callee-save values in an OopMap so their save locations will be
 208   // propagated to the RegisterMap of the caller frame during
 209   // StackFrameStream construction (needed for deoptimization; see
 210   // compiledVFrame::create_stack_value).
 211   // If return_pc_adjustment != 0 adjust the return pc by return_pc_adjustment.
 212 
 213   int i;
 214   int offset;
 215 
 216   // calcualte frame size
 217   const int regstosave_num       = sizeof(RegisterSaver_LiveRegs) /
 218                                    sizeof(RegisterSaver::LiveRegType);
 219   const int register_save_size   = regstosave_num * reg_size;
 220   const int frame_size_in_bytes  = round_to(register_save_size, frame::alignment_in_bytes)
 221                                    + frame::abi_reg_args_size;
 222   *out_frame_size_in_bytes       = frame_size_in_bytes;
 223   const int frame_size_in_slots  = frame_size_in_bytes / sizeof(jint);
 224   const int register_save_offset = frame_size_in_bytes - register_save_size;
 225 
 226   // OopMap frame size is in c2 stack slots (sizeof(jint)) not bytes or words.
 227   OopMap* map = generate_oop_map ? new OopMap(frame_size_in_slots, 0) : NULL;
 228 
 229   BLOCK_COMMENT("push_frame_reg_args_and_save_live_registers {");
 230 
 231   // Save r30 in the last slot of the not yet pushed frame so that we
 232   // can use it as scratch reg.
 233   __ std(R30, -reg_size, R1_SP);
 234   assert(-reg_size == register_save_offset - frame_size_in_bytes + ((regstosave_num-1)*reg_size),
 235          "consistency check");
 236 
 237   // save the flags
 238   // Do the save_LR_CR by hand and adjust the return pc if requested.
 239   __ mfcr(R30);
 240   __ std(R30, _abi(cr), R1_SP);
 241   switch (return_pc_location) {
 242     case return_pc_is_lr:    __ mflr(R30);           break;
 243     case return_pc_is_r4:    __ mr(R30, R4);     break;
 244     case return_pc_is_thread_saved_exception_pc:
 245                                  __ ld(R30, thread_(saved_exception_pc)); break;
 246     default: ShouldNotReachHere();
 247   }
 248   if (return_pc_adjustment != 0)
 249     __ addi(R30, R30, return_pc_adjustment);
 250   __ std(R30, _abi(lr), R1_SP);
 251 
 252   // push a new frame
 253   __ push_frame(frame_size_in_bytes, R30);
 254 
 255   // save all registers (ints and floats)
 256   offset = register_save_offset;
 257   for (int i = 0; i &lt; regstosave_num; i++) {
 258     int reg_num  = RegisterSaver_LiveRegs[i].reg_num;
 259     int reg_type = RegisterSaver_LiveRegs[i].reg_type;
 260 
 261     switch (reg_type) {
 262       case RegisterSaver::int_reg: {
 263         if (reg_num != 30) { // We spilled R30 right at the beginning.
 264           __ std(as_Register(reg_num), offset, R1_SP);
 265         }
 266         break;
 267       }
 268       case RegisterSaver::float_reg: {
 269         __ stfd(as_FloatRegister(reg_num), offset, R1_SP);
 270         break;
 271       }
 272       case RegisterSaver::special_reg: {
 273         if (reg_num == SR_CTR_SpecialRegisterEnumValue) {
 274           __ mfctr(R30);
 275           __ std(R30, offset, R1_SP);
 276         } else {
 277           Unimplemented();
 278         }
 279         break;
 280       }
 281       default:
 282         ShouldNotReachHere();
 283     }
 284 
 285     if (generate_oop_map) {
 286       map-&gt;set_callee_saved(VMRegImpl::stack2reg(offset&gt;&gt;2),
 287                             RegisterSaver_LiveRegs[i].vmreg);
 288       map-&gt;set_callee_saved(VMRegImpl::stack2reg((offset + half_reg_size)&gt;&gt;2),
 289                             RegisterSaver_LiveRegs[i].vmreg-&gt;next());
 290     }
 291     offset += reg_size;
 292   }
 293 
 294   BLOCK_COMMENT("} push_frame_reg_args_and_save_live_registers");
 295 
 296   // And we're done.
 297   return map;
 298 }
 299 
 300 
 301 // Pop the current frame and restore all the registers that we
 302 // saved.
 303 void RegisterSaver::restore_live_registers_and_pop_frame(MacroAssembler* masm,
 304                                                          int frame_size_in_bytes,
 305                                                          bool restore_ctr) {
 306   int i;
 307   int offset;
 308   const int regstosave_num       = sizeof(RegisterSaver_LiveRegs) /
 309                                    sizeof(RegisterSaver::LiveRegType);
 310   const int register_save_size   = regstosave_num * reg_size;
 311   const int register_save_offset = frame_size_in_bytes - register_save_size;
 312 
 313   BLOCK_COMMENT("restore_live_registers_and_pop_frame {");
 314 
 315   // restore all registers (ints and floats)
 316   offset = register_save_offset;
 317   for (int i = 0; i &lt; regstosave_num; i++) {
 318     int reg_num  = RegisterSaver_LiveRegs[i].reg_num;
 319     int reg_type = RegisterSaver_LiveRegs[i].reg_type;
 320 
 321     switch (reg_type) {
 322       case RegisterSaver::int_reg: {
 323         if (reg_num != 30) // R30 restored at the end, it's the tmp reg!
 324           __ ld(as_Register(reg_num), offset, R1_SP);
 325         break;
 326       }
 327       case RegisterSaver::float_reg: {
 328         __ lfd(as_FloatRegister(reg_num), offset, R1_SP);
 329         break;
 330       }
 331       case RegisterSaver::special_reg: {
 332         if (reg_num == SR_CTR_SpecialRegisterEnumValue) {
 333           if (restore_ctr) { // Nothing to do here if ctr already contains the next address.
 334             __ ld(R30, offset, R1_SP);
 335             __ mtctr(R30);
 336           }
 337         } else {
 338           Unimplemented();
 339         }
 340         break;
 341       }
 342       default:
 343         ShouldNotReachHere();
 344     }
 345     offset += reg_size;
 346   }
 347 
 348   // pop the frame
 349   __ pop_frame();
 350 
 351   // restore the flags
 352   __ restore_LR_CR(R30);
 353 
 354   // restore scratch register's value
 355   __ ld(R30, -reg_size, R1_SP);
 356 
 357   BLOCK_COMMENT("} restore_live_registers_and_pop_frame");
 358 }
 359 
 360 void RegisterSaver::push_frame_and_save_argument_registers(MacroAssembler* masm, Register r_temp,
 361                                                            int frame_size,int total_args, const VMRegPair *regs,
 362                                                            const VMRegPair *regs2) {
 363   __ push_frame(frame_size, r_temp);
 364   int st_off = frame_size - wordSize;
 365   for (int i = 0; i &lt; total_args; i++) {
 366     VMReg r_1 = regs[i].first();
 367     VMReg r_2 = regs[i].second();
 368     if (!r_1-&gt;is_valid()) {
 369       assert(!r_2-&gt;is_valid(), "");
 370       continue;
 371     }
 372     if (r_1-&gt;is_Register()) {
 373       Register r = r_1-&gt;as_Register();
 374       __ std(r, st_off, R1_SP);
 375       st_off -= wordSize;
 376     } else if (r_1-&gt;is_FloatRegister()) {
 377       FloatRegister f = r_1-&gt;as_FloatRegister();
 378       __ stfd(f, st_off, R1_SP);
 379       st_off -= wordSize;
 380     }
 381   }
 382   if (regs2 != NULL) {
 383     for (int i = 0; i &lt; total_args; i++) {
 384       VMReg r_1 = regs2[i].first();
 385       VMReg r_2 = regs2[i].second();
 386       if (!r_1-&gt;is_valid()) {
 387         assert(!r_2-&gt;is_valid(), "");
 388         continue;
 389       }
 390       if (r_1-&gt;is_Register()) {
 391         Register r = r_1-&gt;as_Register();
 392         __ std(r, st_off, R1_SP);
 393         st_off -= wordSize;
 394       } else if (r_1-&gt;is_FloatRegister()) {
 395         FloatRegister f = r_1-&gt;as_FloatRegister();
 396         __ stfd(f, st_off, R1_SP);
 397         st_off -= wordSize;
 398       }
 399     }
 400   }
 401 }
 402 
 403 void RegisterSaver::restore_argument_registers_and_pop_frame(MacroAssembler*masm, int frame_size,
 404                                                              int total_args, const VMRegPair *regs,
 405                                                              const VMRegPair *regs2) {
 406   int st_off = frame_size - wordSize;
 407   for (int i = 0; i &lt; total_args; i++) {
 408     VMReg r_1 = regs[i].first();
 409     VMReg r_2 = regs[i].second();
 410     if (r_1-&gt;is_Register()) {
 411       Register r = r_1-&gt;as_Register();
 412       __ ld(r, st_off, R1_SP);
 413       st_off -= wordSize;
 414     } else if (r_1-&gt;is_FloatRegister()) {
 415       FloatRegister f = r_1-&gt;as_FloatRegister();
 416       __ lfd(f, st_off, R1_SP);
 417       st_off -= wordSize;
 418     }
 419   }
 420   if (regs2 != NULL)
 421     for (int i = 0; i &lt; total_args; i++) {
 422       VMReg r_1 = regs2[i].first();
 423       VMReg r_2 = regs2[i].second();
 424       if (r_1-&gt;is_Register()) {
 425         Register r = r_1-&gt;as_Register();
 426         __ ld(r, st_off, R1_SP);
 427         st_off -= wordSize;
 428       } else if (r_1-&gt;is_FloatRegister()) {
 429         FloatRegister f = r_1-&gt;as_FloatRegister();
 430         __ lfd(f, st_off, R1_SP);
 431         st_off -= wordSize;
 432       }
 433     }
 434   __ pop_frame();
 435 }
 436 
 437 // Restore the registers that might be holding a result.
 438 void RegisterSaver::restore_result_registers(MacroAssembler* masm, int frame_size_in_bytes) {
 439   int i;
 440   int offset;
 441   const int regstosave_num       = sizeof(RegisterSaver_LiveRegs) /
 442                                    sizeof(RegisterSaver::LiveRegType);
 443   const int register_save_size   = regstosave_num * reg_size;
 444   const int register_save_offset = frame_size_in_bytes - register_save_size;
 445 
 446   // restore all result registers (ints and floats)
 447   offset = register_save_offset;
 448   for (int i = 0; i &lt; regstosave_num; i++) {
 449     int reg_num  = RegisterSaver_LiveRegs[i].reg_num;
 450     int reg_type = RegisterSaver_LiveRegs[i].reg_type;
 451     switch (reg_type) {
 452       case RegisterSaver::int_reg: {
 453         if (as_Register(reg_num)==R3_RET) // int result_reg
 454           __ ld(as_Register(reg_num), offset, R1_SP);
 455         break;
 456       }
 457       case RegisterSaver::float_reg: {
 458         if (as_FloatRegister(reg_num)==F1_RET) // float result_reg
 459           __ lfd(as_FloatRegister(reg_num), offset, R1_SP);
 460         break;
 461       }
 462       case RegisterSaver::special_reg: {
 463         // Special registers don't hold a result.
 464         break;
 465       }
 466       default:
 467         ShouldNotReachHere();
 468     }
 469     offset += reg_size;
 470   }
 471 }
 472 
 473 // Is vector's size (in bytes) bigger than a size saved by default?
 474 bool SharedRuntime::is_wide_vector(int size) {
 475   ResourceMark rm;
 476   // Note, MaxVectorSize == 8 on PPC64.
 477   assert(size &lt;= 8, err_msg_res("%d bytes vectors are not supported", size));
 478   return size &gt; 8;
 479 }
 480 #ifdef COMPILER2
 481 static int reg2slot(VMReg r) {
 482   return r-&gt;reg2stack() + SharedRuntime::out_preserve_stack_slots();
 483 }
 484 
 485 static int reg2offset(VMReg r) {
 486   return (r-&gt;reg2stack() + SharedRuntime::out_preserve_stack_slots()) * VMRegImpl::stack_slot_size;
 487 }
 488 #endif
 489 
 490 // ---------------------------------------------------------------------------
 491 // Read the array of BasicTypes from a signature, and compute where the
 492 // arguments should go. Values in the VMRegPair regs array refer to 4-byte
 493 // quantities. Values less than VMRegImpl::stack0 are registers, those above
 494 // refer to 4-byte stack slots. All stack slots are based off of the stack pointer
 495 // as framesizes are fixed.
 496 // VMRegImpl::stack0 refers to the first slot 0(sp).
 497 // and VMRegImpl::stack0+1 refers to the memory word 4-bytes higher. Register
 498 // up to RegisterImpl::number_of_registers) are the 64-bit
 499 // integer registers.
 500 
 501 // Note: the INPUTS in sig_bt are in units of Java argument words, which are
 502 // either 32-bit or 64-bit depending on the build. The OUTPUTS are in 32-bit
 503 // units regardless of build. Of course for i486 there is no 64 bit build
 504 
 505 // The Java calling convention is a "shifted" version of the C ABI.
 506 // By skipping the first C ABI register we can call non-static jni methods
 507 // with small numbers of arguments without having to shuffle the arguments
 508 // at all. Since we control the java ABI we ought to at least get some
 509 // advantage out of it.
 510 
 511 const VMReg java_iarg_reg[8] = {
 512   R3-&gt;as_VMReg(),
 513   R4-&gt;as_VMReg(),
 514   R5-&gt;as_VMReg(),
 515   R6-&gt;as_VMReg(),
 516   R7-&gt;as_VMReg(),
 517   R8-&gt;as_VMReg(),
 518   R9-&gt;as_VMReg(),
 519   R10-&gt;as_VMReg()
 520 };
 521 
 522 const VMReg java_farg_reg[13] = {
 523   F1-&gt;as_VMReg(),
 524   F2-&gt;as_VMReg(),
 525   F3-&gt;as_VMReg(),
 526   F4-&gt;as_VMReg(),
 527   F5-&gt;as_VMReg(),
 528   F6-&gt;as_VMReg(),
 529   F7-&gt;as_VMReg(),
 530   F8-&gt;as_VMReg(),
 531   F9-&gt;as_VMReg(),
 532   F10-&gt;as_VMReg(),
 533   F11-&gt;as_VMReg(),
 534   F12-&gt;as_VMReg(),
 535   F13-&gt;as_VMReg()
 536 };
 537 
 538 const int num_java_iarg_registers = sizeof(java_iarg_reg) / sizeof(java_iarg_reg[0]);
 539 const int num_java_farg_registers = sizeof(java_farg_reg) / sizeof(java_farg_reg[0]);
 540 
 541 int SharedRuntime::java_calling_convention(const BasicType *sig_bt,
 542                                            VMRegPair *regs,
 543                                            int total_args_passed,
 544                                            int is_outgoing) {
 545   // C2c calling conventions for compiled-compiled calls.
 546   // Put 8 ints/longs into registers _AND_ 13 float/doubles into
 547   // registers _AND_ put the rest on the stack.
 548 
 549   const int inc_stk_for_intfloat   = 1; // 1 slots for ints and floats
 550   const int inc_stk_for_longdouble = 2; // 2 slots for longs and doubles
 551 
 552   int i;
 553   VMReg reg;
 554   int stk = 0;
 555   int ireg = 0;
 556   int freg = 0;
 557 
 558   // We put the first 8 arguments into registers and the rest on the
 559   // stack, float arguments are already in their argument registers
 560   // due to c2c calling conventions (see calling_convention).
 561   for (int i = 0; i &lt; total_args_passed; ++i) {
 562     switch(sig_bt[i]) {
 563     case T_BOOLEAN:
 564     case T_CHAR:
 565     case T_BYTE:
 566     case T_SHORT:
 567     case T_INT:
 568       if (ireg &lt; num_java_iarg_registers) {
 569         // Put int/ptr in register
 570         reg = java_iarg_reg[ireg];
 571         ++ireg;
 572       } else {
 573         // Put int/ptr on stack.
 574         reg = VMRegImpl::stack2reg(stk);
 575         stk += inc_stk_for_intfloat;
 576       }
 577       regs[i].set1(reg);
 578       break;
 579     case T_LONG:
 580       assert(sig_bt[i+1] == T_VOID, "expecting half");
 581       if (ireg &lt; num_java_iarg_registers) {
 582         // Put long in register.
 583         reg = java_iarg_reg[ireg];
 584         ++ireg;
 585       } else {
 586         // Put long on stack. They must be aligned to 2 slots.
 587         if (stk &amp; 0x1) ++stk;
 588         reg = VMRegImpl::stack2reg(stk);
 589         stk += inc_stk_for_longdouble;
 590       }
 591       regs[i].set2(reg);
 592       break;
 593     case T_OBJECT:
 594     case T_ARRAY:
 595     case T_ADDRESS:
 596       if (ireg &lt; num_java_iarg_registers) {
 597         // Put ptr in register.
 598         reg = java_iarg_reg[ireg];
 599         ++ireg;
 600       } else {
 601         // Put ptr on stack. Objects must be aligned to 2 slots too,
 602         // because "64-bit pointers record oop-ishness on 2 aligned
 603         // adjacent registers." (see OopFlow::build_oop_map).
 604         if (stk &amp; 0x1) ++stk;
 605         reg = VMRegImpl::stack2reg(stk);
 606         stk += inc_stk_for_longdouble;
 607       }
 608       regs[i].set2(reg);
 609       break;
 610     case T_FLOAT:
 611       if (freg &lt; num_java_farg_registers) {
 612         // Put float in register.
 613         reg = java_farg_reg[freg];
 614         ++freg;
 615       } else {
 616         // Put float on stack.
 617         reg = VMRegImpl::stack2reg(stk);
 618         stk += inc_stk_for_intfloat;
 619       }
 620       regs[i].set1(reg);
 621       break;
 622     case T_DOUBLE:
 623       assert(sig_bt[i+1] == T_VOID, "expecting half");
 624       if (freg &lt; num_java_farg_registers) {
 625         // Put double in register.
 626         reg = java_farg_reg[freg];
 627         ++freg;
 628       } else {
 629         // Put double on stack. They must be aligned to 2 slots.
 630         if (stk &amp; 0x1) ++stk;
 631         reg = VMRegImpl::stack2reg(stk);
 632         stk += inc_stk_for_longdouble;
 633       }
 634       regs[i].set2(reg);
 635       break;
 636     case T_VOID:
 637       // Do not count halves.
 638       regs[i].set_bad();
 639       break;
 640     default:
 641       ShouldNotReachHere();
 642     }
 643   }
 644   return round_to(stk, 2);
 645 }
 646 
 647 #ifdef COMPILER2
 648 // Calling convention for calling C code.
 649 int SharedRuntime::c_calling_convention(const BasicType *sig_bt,
 650                                         VMRegPair *regs,
 651                                         VMRegPair *regs2,
 652                                         int total_args_passed) {
 653   // Calling conventions for C runtime calls and calls to JNI native methods.
 654   //
 655   // PPC64 convention: Hoist the first 8 int/ptr/long's in the first 8
 656   // int regs, leaving int regs undefined if the arg is flt/dbl. Hoist
 657   // the first 13 flt/dbl's in the first 13 fp regs but additionally
 658   // copy flt/dbl to the stack if they are beyond the 8th argument.
 659 
 660   const VMReg iarg_reg[8] = {
 661     R3-&gt;as_VMReg(),
 662     R4-&gt;as_VMReg(),
 663     R5-&gt;as_VMReg(),
 664     R6-&gt;as_VMReg(),
 665     R7-&gt;as_VMReg(),
 666     R8-&gt;as_VMReg(),
 667     R9-&gt;as_VMReg(),
 668     R10-&gt;as_VMReg()
 669   };
 670 
 671   const VMReg farg_reg[13] = {
 672     F1-&gt;as_VMReg(),
 673     F2-&gt;as_VMReg(),
 674     F3-&gt;as_VMReg(),
 675     F4-&gt;as_VMReg(),
 676     F5-&gt;as_VMReg(),
 677     F6-&gt;as_VMReg(),
 678     F7-&gt;as_VMReg(),
 679     F8-&gt;as_VMReg(),
 680     F9-&gt;as_VMReg(),
 681     F10-&gt;as_VMReg(),
 682     F11-&gt;as_VMReg(),
 683     F12-&gt;as_VMReg(),
 684     F13-&gt;as_VMReg()
 685   };
 686 
 687   // Check calling conventions consistency.
 688   assert(sizeof(iarg_reg) / sizeof(iarg_reg[0]) == Argument::n_int_register_parameters_c &amp;&amp;
 689          sizeof(farg_reg) / sizeof(farg_reg[0]) == Argument::n_float_register_parameters_c,
 690          "consistency");
 691 
 692   // `Stk' counts stack slots. Due to alignment, 32 bit values occupy
 693   // 2 such slots, like 64 bit values do.
 694   const int inc_stk_for_intfloat   = 2; // 2 slots for ints and floats
 695   const int inc_stk_for_longdouble = 2; // 2 slots for longs and doubles
 696 
 697   int i;
 698   VMReg reg;
 699   // Leave room for C-compatible ABI_REG_ARGS.
 700   int stk = (frame::abi_reg_args_size - frame::jit_out_preserve_size) / VMRegImpl::stack_slot_size;
 701   int arg = 0;
 702   int freg = 0;
 703 
 704   // Avoid passing C arguments in the wrong stack slots.
 705 #if defined(ABI_ELFv2)
 706   assert((SharedRuntime::out_preserve_stack_slots() + stk) * VMRegImpl::stack_slot_size == 96,
 707          "passing C arguments in wrong stack slots");
 708 #else
 709   assert((SharedRuntime::out_preserve_stack_slots() + stk) * VMRegImpl::stack_slot_size == 112,
 710          "passing C arguments in wrong stack slots");
 711 #endif
 712   // We fill-out regs AND regs2 if an argument must be passed in a
 713   // register AND in a stack slot. If regs2 is NULL in such a
 714   // situation, we bail-out with a fatal error.
 715   for (int i = 0; i &lt; total_args_passed; ++i, ++arg) {
 716     // Initialize regs2 to BAD.
 717     if (regs2 != NULL) regs2[i].set_bad();
 718 
 719     switch(sig_bt[i]) {
 720 
 721     //
 722     // If arguments 0-7 are integers, they are passed in integer registers.
 723     // Argument i is placed in iarg_reg[i].
 724     //
 725     case T_BOOLEAN:
 726     case T_CHAR:
 727     case T_BYTE:
 728     case T_SHORT:
 729     case T_INT:
 730       // We must cast ints to longs and use full 64 bit stack slots
 731       // here. We do the cast in GraphKit::gen_stub() and just guard
 732       // here against loosing that change.
 733       assert(CCallingConventionRequiresIntsAsLongs,
 734              "argument of type int should be promoted to type long");
 735       guarantee(i &gt; 0 &amp;&amp; sig_bt[i-1] == T_LONG,
 736                 "argument of type (bt) should have been promoted to type (T_LONG,bt) for bt in "
 737                 "{T_BOOLEAN, T_CHAR, T_BYTE, T_SHORT, T_INT}");
 738       // Do not count halves.
 739       regs[i].set_bad();
 740       --arg;
 741       break;
 742     case T_LONG:
 743       guarantee(sig_bt[i+1] == T_VOID    ||
 744                 sig_bt[i+1] == T_BOOLEAN || sig_bt[i+1] == T_CHAR  ||
 745                 sig_bt[i+1] == T_BYTE    || sig_bt[i+1] == T_SHORT ||
 746                 sig_bt[i+1] == T_INT,
 747                 "expecting type (T_LONG,half) or type (T_LONG,bt) with bt in {T_BOOLEAN, T_CHAR, T_BYTE, T_SHORT, T_INT}");
 748     case T_OBJECT:
 749     case T_ARRAY:
 750     case T_ADDRESS:
 751     case T_METADATA:
 752       // Oops are already boxed if required (JNI).
 753       if (arg &lt; Argument::n_int_register_parameters_c) {
 754         reg = iarg_reg[arg];
 755       } else {
 756         reg = VMRegImpl::stack2reg(stk);
 757         stk += inc_stk_for_longdouble;
 758       }
 759       regs[i].set2(reg);
 760       break;
 761 
 762     //
 763     // Floats are treated differently from int regs:  The first 13 float arguments
 764     // are passed in registers (not the float args among the first 13 args).
 765     // Thus argument i is NOT passed in farg_reg[i] if it is float.  It is passed
 766     // in farg_reg[j] if argument i is the j-th float argument of this call.
 767     //
 768     case T_FLOAT:
 769 #if defined(LINUX)
 770       // Linux uses ELF ABI. Both original ELF and ELFv2 ABIs have float
 771       // in the least significant word of an argument slot.
 772 #if defined(VM_LITTLE_ENDIAN)
 773 #define FLOAT_WORD_OFFSET_IN_SLOT 0
 774 #else
 775 #define FLOAT_WORD_OFFSET_IN_SLOT 1
 776 #endif
 777 #elif defined(AIX)
 778       // Although AIX runs on big endian CPU, float is in the most
 779       // significant word of an argument slot.
 780 #define FLOAT_WORD_OFFSET_IN_SLOT 0
 781 #else
 782 #error "unknown OS"
 783 #endif
 784       if (freg &lt; Argument::n_float_register_parameters_c) {
 785         // Put float in register ...
 786         reg = farg_reg[freg];
 787         ++freg;
 788 
 789         // Argument i for i &gt; 8 is placed on the stack even if it's
 790         // placed in a register (if it's a float arg). Aix disassembly
 791         // shows that xlC places these float args on the stack AND in
 792         // a register. This is not documented, but we follow this
 793         // convention, too.
 794         if (arg &gt;= Argument::n_regs_not_on_stack_c) {
 795           // ... and on the stack.
 796           guarantee(regs2 != NULL, "must pass float in register and stack slot");
 797           VMReg reg2 = VMRegImpl::stack2reg(stk + FLOAT_WORD_OFFSET_IN_SLOT);
 798           regs2[i].set1(reg2);
 799           stk += inc_stk_for_intfloat;
 800         }
 801 
 802       } else {
 803         // Put float on stack.
 804         reg = VMRegImpl::stack2reg(stk + FLOAT_WORD_OFFSET_IN_SLOT);
 805         stk += inc_stk_for_intfloat;
 806       }
 807       regs[i].set1(reg);
 808       break;
 809     case T_DOUBLE:
 810       assert(sig_bt[i+1] == T_VOID, "expecting half");
 811       if (freg &lt; Argument::n_float_register_parameters_c) {
 812         // Put double in register ...
 813         reg = farg_reg[freg];
 814         ++freg;
 815 
 816         // Argument i for i &gt; 8 is placed on the stack even if it's
 817         // placed in a register (if it's a double arg). Aix disassembly
 818         // shows that xlC places these float args on the stack AND in
 819         // a register. This is not documented, but we follow this
 820         // convention, too.
 821         if (arg &gt;= Argument::n_regs_not_on_stack_c) {
 822           // ... and on the stack.
 823           guarantee(regs2 != NULL, "must pass float in register and stack slot");
 824           VMReg reg2 = VMRegImpl::stack2reg(stk);
 825           regs2[i].set2(reg2);
 826           stk += inc_stk_for_longdouble;
 827         }
 828       } else {
 829         // Put double on stack.
 830         reg = VMRegImpl::stack2reg(stk);
 831         stk += inc_stk_for_longdouble;
 832       }
 833       regs[i].set2(reg);
 834       break;
 835 
 836     case T_VOID:
 837       // Do not count halves.
 838       regs[i].set_bad();
 839       --arg;
 840       break;
 841     default:
 842       ShouldNotReachHere();
 843     }
 844   }
 845 
 846   return round_to(stk, 2);
 847 }
 848 #endif // COMPILER2
 849 
 850 static address gen_c2i_adapter(MacroAssembler *masm,
 851                             int total_args_passed,
 852                             int comp_args_on_stack,
 853                             const BasicType *sig_bt,
 854                             const VMRegPair *regs,
 855                             Label&amp; call_interpreter,
 856                             const Register&amp; ientry) {
 857 
 858   address c2i_entrypoint;
 859 
 860   const Register sender_SP = R21_sender_SP; // == R21_tmp1
 861   const Register code      = R22_tmp2;
 862   //const Register ientry  = R23_tmp3;
 863   const Register value_regs[] = { R24_tmp4, R25_tmp5, R26_tmp6 };
 864   const int num_value_regs = sizeof(value_regs) / sizeof(Register);
 865   int value_regs_index = 0;
 866 
 867   const Register return_pc = R27_tmp7;
 868   const Register tmp       = R28_tmp8;
 869 
 870   assert_different_registers(sender_SP, code, ientry, return_pc, tmp);
 871 
 872   // Adapter needs TOP_IJAVA_FRAME_ABI.
 873   const int adapter_size = frame::top_ijava_frame_abi_size +
 874                            round_to(total_args_passed * wordSize, frame::alignment_in_bytes);
 875 
 876   // regular (verified) c2i entry point
 877   c2i_entrypoint = __ pc();
 878 
 879   // Does compiled code exists? If yes, patch the caller's callsite.
 880   __ ld(code, method_(code));
 881   __ cmpdi(CCR0, code, 0);
 882   __ ld(ientry, method_(interpreter_entry)); // preloaded
 883   __ beq(CCR0, call_interpreter);
 884 
 885 
 886   // Patch caller's callsite, method_(code) was not NULL which means that
 887   // compiled code exists.
 888   __ mflr(return_pc);
 889   __ std(return_pc, _abi(lr), R1_SP);
 890   RegisterSaver::push_frame_and_save_argument_registers(masm, tmp, adapter_size, total_args_passed, regs);
 891 
 892   __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::fixup_callers_callsite), R19_method, return_pc);
 893 
 894   RegisterSaver::restore_argument_registers_and_pop_frame(masm, adapter_size, total_args_passed, regs);
 895   __ ld(return_pc, _abi(lr), R1_SP);
 896   __ ld(ientry, method_(interpreter_entry)); // preloaded
 897   __ mtlr(return_pc);
 898 
 899 
 900   // Call the interpreter.
 901   __ BIND(call_interpreter);
 902   __ mtctr(ientry);
 903 
 904   // Get a copy of the current SP for loading caller's arguments.
 905   __ mr(sender_SP, R1_SP);
 906 
 907   // Add space for the adapter.
 908   __ resize_frame(-adapter_size, R12_scratch2);
 909 
 910   int st_off = adapter_size - wordSize;
 911 
 912   // Write the args into the outgoing interpreter space.
 913   for (int i = 0; i &lt; total_args_passed; i++) {
 914     VMReg r_1 = regs[i].first();
 915     VMReg r_2 = regs[i].second();
 916     if (!r_1-&gt;is_valid()) {
 917       assert(!r_2-&gt;is_valid(), "");
 918       continue;
 919     }
 920     if (r_1-&gt;is_stack()) {
 921       Register tmp_reg = value_regs[value_regs_index];
 922       value_regs_index = (value_regs_index + 1) % num_value_regs;
 923       // The calling convention produces OptoRegs that ignore the out
 924       // preserve area (JIT's ABI). We must account for it here.
 925       int ld_off = (r_1-&gt;reg2stack() + SharedRuntime::out_preserve_stack_slots()) * VMRegImpl::stack_slot_size;
 926       if (!r_2-&gt;is_valid()) {
 927         __ lwz(tmp_reg, ld_off, sender_SP);
 928       } else {
 929         __ ld(tmp_reg, ld_off, sender_SP);
 930       }
 931       // Pretend stack targets were loaded into tmp_reg.
 932       r_1 = tmp_reg-&gt;as_VMReg();
 933     }
 934 
 935     if (r_1-&gt;is_Register()) {
 936       Register r = r_1-&gt;as_Register();
 937       if (!r_2-&gt;is_valid()) {
 938         __ stw(r, st_off, R1_SP);
 939         st_off-=wordSize;
 940       } else {
 941         // Longs are given 2 64-bit slots in the interpreter, but the
 942         // data is passed in only 1 slot.
 943         if (sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {
 944           DEBUG_ONLY( __ li(tmp, 0); __ std(tmp, st_off, R1_SP); )
 945           st_off-=wordSize;
 946         }
 947         __ std(r, st_off, R1_SP);
 948         st_off-=wordSize;
 949       }
 950     } else {
 951       assert(r_1-&gt;is_FloatRegister(), "");
 952       FloatRegister f = r_1-&gt;as_FloatRegister();
 953       if (!r_2-&gt;is_valid()) {
 954         __ stfs(f, st_off, R1_SP);
 955         st_off-=wordSize;
 956       } else {
 957         // In 64bit, doubles are given 2 64-bit slots in the interpreter, but the
 958         // data is passed in only 1 slot.
 959         // One of these should get known junk...
 960         DEBUG_ONLY( __ li(tmp, 0); __ std(tmp, st_off, R1_SP); )
 961         st_off-=wordSize;
 962         __ stfd(f, st_off, R1_SP);
 963         st_off-=wordSize;
 964       }
 965     }
 966   }
 967 
 968   // Jump to the interpreter just as if interpreter was doing it.
 969 
 970 #ifdef CC_INTERP
 971   const Register tos = R17_tos;
 972 #else
 973   const Register tos = R15_esp;
 974   __ load_const_optimized(R25_templateTableBase, (address)Interpreter::dispatch_table((TosState)0), R11_scratch1);
 975 #endif
 976 
 977   // load TOS
 978   __ addi(tos, R1_SP, st_off);
 979 
 980   // Frame_manager expects initial_caller_sp (= SP without resize by c2i) in R21_tmp1.
 981   assert(sender_SP == R21_sender_SP, "passing initial caller's SP in wrong register");
 982   __ bctr();
 983 
 984   return c2i_entrypoint;
 985 }
 986 
 987 static void gen_i2c_adapter(MacroAssembler *masm,
 988                             int total_args_passed,
 989                             int comp_args_on_stack,
 990                             const BasicType *sig_bt,
 991                             const VMRegPair *regs) {
 992 
 993   // Load method's entry-point from method.
 994   __ ld(R12_scratch2, in_bytes(Method::from_compiled_offset()), R19_method);
 995   __ mtctr(R12_scratch2);
 996 
 997   // We will only enter here from an interpreted frame and never from after
 998   // passing thru a c2i. Azul allowed this but we do not. If we lose the
 999   // race and use a c2i we will remain interpreted for the race loser(s).
1000   // This removes all sorts of headaches on the x86 side and also eliminates
1001   // the possibility of having c2i -&gt; i2c -&gt; c2i -&gt; ... endless transitions.
1002 
1003   // Note: r13 contains the senderSP on entry. We must preserve it since
1004   // we may do a i2c -&gt; c2i transition if we lose a race where compiled
1005   // code goes non-entrant while we get args ready.
1006   // In addition we use r13 to locate all the interpreter args as
1007   // we must align the stack to 16 bytes on an i2c entry else we
1008   // lose alignment we expect in all compiled code and register
1009   // save code can segv when fxsave instructions find improperly
1010   // aligned stack pointer.
1011 
1012 #ifdef CC_INTERP
1013   const Register ld_ptr = R17_tos;
1014 #else
1015   const Register ld_ptr = R15_esp;
1016 #endif
1017 
1018   const Register value_regs[] = { R22_tmp2, R23_tmp3, R24_tmp4, R25_tmp5, R26_tmp6 };
1019   const int num_value_regs = sizeof(value_regs) / sizeof(Register);
1020   int value_regs_index = 0;
1021 
1022   int ld_offset = total_args_passed*wordSize;
1023 
1024   // Cut-out for having no stack args. Since up to 2 int/oop args are passed
1025   // in registers, we will occasionally have no stack args.
1026   int comp_words_on_stack = 0;
1027   if (comp_args_on_stack) {
1028     // Sig words on the stack are greater-than VMRegImpl::stack0. Those in
1029     // registers are below. By subtracting stack0, we either get a negative
1030     // number (all values in registers) or the maximum stack slot accessed.
1031 
1032     // Convert 4-byte c2 stack slots to words.
1033     comp_words_on_stack = round_to(comp_args_on_stack*VMRegImpl::stack_slot_size, wordSize)&gt;&gt;LogBytesPerWord;
1034     // Round up to miminum stack alignment, in wordSize.
1035     comp_words_on_stack = round_to(comp_words_on_stack, 2);
1036     __ resize_frame(-comp_words_on_stack * wordSize, R11_scratch1);
1037   }
1038 
1039   // Now generate the shuffle code.  Pick up all register args and move the
1040   // rest through register value=Z_R12.
1041   BLOCK_COMMENT("Shuffle arguments");
1042   for (int i = 0; i &lt; total_args_passed; i++) {
1043     if (sig_bt[i] == T_VOID) {
1044       assert(i &gt; 0 &amp;&amp; (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), "missing half");
1045       continue;
1046     }
1047 
1048     // Pick up 0, 1 or 2 words from ld_ptr.
1049     assert(!regs[i].second()-&gt;is_valid() || regs[i].first()-&gt;next() == regs[i].second(),
1050             "scrambled load targets?");
1051     VMReg r_1 = regs[i].first();
1052     VMReg r_2 = regs[i].second();
1053     if (!r_1-&gt;is_valid()) {
1054       assert(!r_2-&gt;is_valid(), "");
1055       continue;
1056     }
1057     if (r_1-&gt;is_FloatRegister()) {
1058       if (!r_2-&gt;is_valid()) {
1059         __ lfs(r_1-&gt;as_FloatRegister(), ld_offset, ld_ptr);
1060         ld_offset-=wordSize;
1061       } else {
1062         // Skip the unused interpreter slot.
1063         __ lfd(r_1-&gt;as_FloatRegister(), ld_offset-wordSize, ld_ptr);
1064         ld_offset-=2*wordSize;
1065       }
1066     } else {
1067       Register r;
1068       if (r_1-&gt;is_stack()) {
1069         // Must do a memory to memory move thru "value".
1070         r = value_regs[value_regs_index];
1071         value_regs_index = (value_regs_index + 1) % num_value_regs;
1072       } else {
1073         r = r_1-&gt;as_Register();
1074       }
1075       if (!r_2-&gt;is_valid()) {
1076         // Not sure we need to do this but it shouldn't hurt.
1077         if (sig_bt[i] == T_OBJECT || sig_bt[i] == T_ADDRESS || sig_bt[i] == T_ARRAY) {
1078           __ ld(r, ld_offset, ld_ptr);
1079           ld_offset-=wordSize;
1080         } else {
1081           __ lwz(r, ld_offset, ld_ptr);
1082           ld_offset-=wordSize;
1083         }
1084       } else {
1085         // In 64bit, longs are given 2 64-bit slots in the interpreter, but the
1086         // data is passed in only 1 slot.
1087         if (sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {
1088           ld_offset-=wordSize;
1089         }
1090         __ ld(r, ld_offset, ld_ptr);
1091         ld_offset-=wordSize;
1092       }
1093 
1094       if (r_1-&gt;is_stack()) {
1095         // Now store value where the compiler expects it
1096         int st_off = (r_1-&gt;reg2stack() + SharedRuntime::out_preserve_stack_slots())*VMRegImpl::stack_slot_size;
1097 
1098         if (sig_bt[i] == T_INT   || sig_bt[i] == T_FLOAT ||sig_bt[i] == T_BOOLEAN ||
1099             sig_bt[i] == T_SHORT || sig_bt[i] == T_CHAR  || sig_bt[i] == T_BYTE) {
1100           __ stw(r, st_off, R1_SP);
1101         } else {
1102           __ std(r, st_off, R1_SP);
1103         }
1104       }
1105     }
1106   }
1107 
1108   BLOCK_COMMENT("Store method");
1109   // Store method into thread-&gt;callee_target.
1110   // We might end up in handle_wrong_method if the callee is
1111   // deoptimized as we race thru here. If that happens we don't want
1112   // to take a safepoint because the caller frame will look
1113   // interpreted and arguments are now "compiled" so it is much better
1114   // to make this transition invisible to the stack walking
1115   // code. Unfortunately if we try and find the callee by normal means
1116   // a safepoint is possible. So we stash the desired callee in the
1117   // thread and the vm will find there should this case occur.
1118   __ std(R19_method, thread_(callee_target));
1119 
1120   // Jump to the compiled code just as if compiled code was doing it.
1121   __ bctr();
1122 }
1123 
1124 AdapterHandlerEntry* SharedRuntime::generate_i2c2i_adapters(MacroAssembler *masm,
1125                                                             int total_args_passed,
1126                                                             int comp_args_on_stack,
1127                                                             const BasicType *sig_bt,
1128                                                             const VMRegPair *regs,
1129                                                             AdapterFingerPrint* fingerprint) {
1130   address i2c_entry;
1131   address c2i_unverified_entry;
1132   address c2i_entry;
1133 
1134 
1135   // entry: i2c
1136 
1137   __ align(CodeEntryAlignment);
1138   i2c_entry = __ pc();
1139   gen_i2c_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs);
1140 
1141 
1142   // entry: c2i unverified
1143 
1144   __ align(CodeEntryAlignment);
1145   BLOCK_COMMENT("c2i unverified entry");
1146   c2i_unverified_entry = __ pc();
1147 
1148   // inline_cache contains a compiledICHolder
1149   const Register ic             = R19_method;
1150   const Register ic_klass       = R11_scratch1;
1151   const Register receiver_klass = R12_scratch2;
1152   const Register code           = R21_tmp1;
1153   const Register ientry         = R23_tmp3;
1154 
1155   assert_different_registers(ic, ic_klass, receiver_klass, R3_ARG1, code, ientry);
1156   assert(R11_scratch1 == R11, "need prologue scratch register");
1157 
1158   Label call_interpreter;
1159 
1160   assert(!MacroAssembler::needs_explicit_null_check(oopDesc::klass_offset_in_bytes()),
1161          "klass offset should reach into any page");
1162   // Check for NULL argument if we don't have implicit null checks.
1163   if (!ImplicitNullChecks || !os::zero_page_read_protected()) {
1164     if (TrapBasedNullChecks) {
1165       __ trap_null_check(R3_ARG1);
1166     } else {
1167       Label valid;
1168       __ cmpdi(CCR0, R3_ARG1, 0);
1169       __ bne_predict_taken(CCR0, valid);
1170       // We have a null argument, branch to ic_miss_stub.
1171       __ b64_patchable((address)SharedRuntime::get_ic_miss_stub(),
1172                        relocInfo::runtime_call_type);
1173       __ BIND(valid);
1174     }
1175   }
1176   // Assume argument is not NULL, load klass from receiver.
1177   __ load_klass(receiver_klass, R3_ARG1);
1178 
1179   __ ld(ic_klass, CompiledICHolder::holder_klass_offset(), ic);
1180 
1181   if (TrapBasedICMissChecks) {
1182     __ trap_ic_miss_check(receiver_klass, ic_klass);
1183   } else {
1184     Label valid;
1185     __ cmpd(CCR0, receiver_klass, ic_klass);
1186     __ beq_predict_taken(CCR0, valid);
1187     // We have an unexpected klass, branch to ic_miss_stub.
1188     __ b64_patchable((address)SharedRuntime::get_ic_miss_stub(),
1189                      relocInfo::runtime_call_type);
1190     __ BIND(valid);
1191   }
1192 
1193   // Argument is valid and klass is as expected, continue.
1194 
1195   // Extract method from inline cache, verified entry point needs it.
1196   __ ld(R19_method, CompiledICHolder::holder_method_offset(), ic);
1197   assert(R19_method == ic, "the inline cache register is dead here");
1198 
1199   __ ld(code, method_(code));
1200   __ cmpdi(CCR0, code, 0);
1201   __ ld(ientry, method_(interpreter_entry)); // preloaded
1202   __ beq_predict_taken(CCR0, call_interpreter);
1203 
1204   // Branch to ic_miss_stub.
1205   __ b64_patchable((address)SharedRuntime::get_ic_miss_stub(), relocInfo::runtime_call_type);
1206 
1207   // entry: c2i
1208 
1209   c2i_entry = gen_c2i_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs, call_interpreter, ientry);
1210 
1211   return AdapterHandlerLibrary::new_entry(fingerprint, i2c_entry, c2i_entry, c2i_unverified_entry);
1212 }
1213 
1214 #ifdef COMPILER2
1215 // An oop arg. Must pass a handle not the oop itself.
1216 static void object_move(MacroAssembler* masm,
1217                         int frame_size_in_slots,
1218                         OopMap* oop_map, int oop_handle_offset,
1219                         bool is_receiver, int* receiver_offset,
1220                         VMRegPair src, VMRegPair dst,
1221                         Register r_caller_sp, Register r_temp_1, Register r_temp_2) {
1222   assert(!is_receiver || (is_receiver &amp;&amp; (*receiver_offset == -1)),
1223          "receiver has already been moved");
1224 
1225   // We must pass a handle. First figure out the location we use as a handle.
1226 
1227   if (src.first()-&gt;is_stack()) {
1228     // stack to stack or reg
1229 
1230     const Register r_handle = dst.first()-&gt;is_stack() ? r_temp_1 : dst.first()-&gt;as_Register();
1231     Label skip;
1232     const int oop_slot_in_callers_frame = reg2slot(src.first());
1233 
1234     guarantee(!is_receiver, "expecting receiver in register");
1235     oop_map-&gt;set_oop(VMRegImpl::stack2reg(oop_slot_in_callers_frame + frame_size_in_slots));
1236 
1237     __ addi(r_handle, r_caller_sp, reg2offset(src.first()));
1238     __ ld(  r_temp_2, reg2offset(src.first()), r_caller_sp);
1239     __ cmpdi(CCR0, r_temp_2, 0);
1240     __ bne(CCR0, skip);
1241     // Use a NULL handle if oop is NULL.
1242     __ li(r_handle, 0);
1243     __ bind(skip);
1244 
1245     if (dst.first()-&gt;is_stack()) {
1246       // stack to stack
1247       __ std(r_handle, reg2offset(dst.first()), R1_SP);
1248     } else {
1249       // stack to reg
1250       // Nothing to do, r_handle is already the dst register.
1251     }
1252   } else {
1253     // reg to stack or reg
1254     const Register r_oop      = src.first()-&gt;as_Register();
1255     const Register r_handle   = dst.first()-&gt;is_stack() ? r_temp_1 : dst.first()-&gt;as_Register();
1256     const int oop_slot        = (r_oop-&gt;encoding()-R3_ARG1-&gt;encoding()) * VMRegImpl::slots_per_word
1257                                 + oop_handle_offset; // in slots
1258     const int oop_offset = oop_slot * VMRegImpl::stack_slot_size;
1259     Label skip;
1260 
1261     if (is_receiver) {
1262       *receiver_offset = oop_offset;
1263     }
1264     oop_map-&gt;set_oop(VMRegImpl::stack2reg(oop_slot));
1265 
1266     __ std( r_oop,    oop_offset, R1_SP);
1267     __ addi(r_handle, R1_SP, oop_offset);
1268 
1269     __ cmpdi(CCR0, r_oop, 0);
1270     __ bne(CCR0, skip);
1271     // Use a NULL handle if oop is NULL.
1272     __ li(r_handle, 0);
1273     __ bind(skip);
1274 
1275     if (dst.first()-&gt;is_stack()) {
1276       // reg to stack
1277       __ std(r_handle, reg2offset(dst.first()), R1_SP);
1278     } else {
1279       // reg to reg
1280       // Nothing to do, r_handle is already the dst register.
1281     }
1282   }
1283 }
1284 
1285 static void int_move(MacroAssembler*masm,
1286                      VMRegPair src, VMRegPair dst,
1287                      Register r_caller_sp, Register r_temp) {
1288   assert(src.first()-&gt;is_valid() &amp;&amp; src.second() == src.first()-&gt;next(), "incoming must be long-int");
1289   assert(dst.first()-&gt;is_valid() &amp;&amp; dst.second() == dst.first()-&gt;next(), "outgoing must be long");
1290 
1291   if (src.first()-&gt;is_stack()) {
1292     if (dst.first()-&gt;is_stack()) {
1293       // stack to stack
1294       __ lwa(r_temp, reg2offset(src.first()), r_caller_sp);
1295       __ std(r_temp, reg2offset(dst.first()), R1_SP);
1296     } else {
1297       // stack to reg
1298       __ lwa(dst.first()-&gt;as_Register(), reg2offset(src.first()), r_caller_sp);
1299     }
1300   } else if (dst.first()-&gt;is_stack()) {
1301     // reg to stack
1302     __ extsw(r_temp, src.first()-&gt;as_Register());
1303     __ std(r_temp, reg2offset(dst.first()), R1_SP);
1304   } else {
1305     // reg to reg
1306     __ extsw(dst.first()-&gt;as_Register(), src.first()-&gt;as_Register());
1307   }
1308 }
1309 
1310 static void long_move(MacroAssembler*masm,
1311                       VMRegPair src, VMRegPair dst,
1312                       Register r_caller_sp, Register r_temp) {
1313   assert(src.first()-&gt;is_valid() &amp;&amp; src.second() == src.first()-&gt;next(), "incoming must be long");
1314   assert(dst.first()-&gt;is_valid() &amp;&amp; dst.second() == dst.first()-&gt;next(), "outgoing must be long");
1315 
1316   if (src.first()-&gt;is_stack()) {
1317     if (dst.first()-&gt;is_stack()) {
1318       // stack to stack
1319       __ ld( r_temp, reg2offset(src.first()), r_caller_sp);
1320       __ std(r_temp, reg2offset(dst.first()), R1_SP);
1321     } else {
1322       // stack to reg
1323       __ ld(dst.first()-&gt;as_Register(), reg2offset(src.first()), r_caller_sp);
1324     }
1325   } else if (dst.first()-&gt;is_stack()) {
1326     // reg to stack
1327     __ std(src.first()-&gt;as_Register(), reg2offset(dst.first()), R1_SP);
1328   } else {
1329     // reg to reg
1330     if (dst.first()-&gt;as_Register() != src.first()-&gt;as_Register())
1331       __ mr(dst.first()-&gt;as_Register(), src.first()-&gt;as_Register());
1332   }
1333 }
1334 
1335 static void float_move(MacroAssembler*masm,
1336                        VMRegPair src, VMRegPair dst,
1337                        Register r_caller_sp, Register r_temp) {
1338   assert(src.first()-&gt;is_valid() &amp;&amp; !src.second()-&gt;is_valid(), "incoming must be float");
1339   assert(dst.first()-&gt;is_valid() &amp;&amp; !dst.second()-&gt;is_valid(), "outgoing must be float");
1340 
1341   if (src.first()-&gt;is_stack()) {
1342     if (dst.first()-&gt;is_stack()) {
1343       // stack to stack
1344       __ lwz(r_temp, reg2offset(src.first()), r_caller_sp);
1345       __ stw(r_temp, reg2offset(dst.first()), R1_SP);
1346     } else {
1347       // stack to reg
1348       __ lfs(dst.first()-&gt;as_FloatRegister(), reg2offset(src.first()), r_caller_sp);
1349     }
1350   } else if (dst.first()-&gt;is_stack()) {
1351     // reg to stack
1352     __ stfs(src.first()-&gt;as_FloatRegister(), reg2offset(dst.first()), R1_SP);
1353   } else {
1354     // reg to reg
1355     if (dst.first()-&gt;as_FloatRegister() != src.first()-&gt;as_FloatRegister())
1356       __ fmr(dst.first()-&gt;as_FloatRegister(), src.first()-&gt;as_FloatRegister());
1357   }
1358 }
1359 
1360 static void double_move(MacroAssembler*masm,
1361                         VMRegPair src, VMRegPair dst,
1362                         Register r_caller_sp, Register r_temp) {
1363   assert(src.first()-&gt;is_valid() &amp;&amp; src.second() == src.first()-&gt;next(), "incoming must be double");
1364   assert(dst.first()-&gt;is_valid() &amp;&amp; dst.second() == dst.first()-&gt;next(), "outgoing must be double");
1365 
1366   if (src.first()-&gt;is_stack()) {
1367     if (dst.first()-&gt;is_stack()) {
1368       // stack to stack
1369       __ ld( r_temp, reg2offset(src.first()), r_caller_sp);
1370       __ std(r_temp, reg2offset(dst.first()), R1_SP);
1371     } else {
1372       // stack to reg
1373       __ lfd(dst.first()-&gt;as_FloatRegister(), reg2offset(src.first()), r_caller_sp);
1374     }
1375   } else if (dst.first()-&gt;is_stack()) {
1376     // reg to stack
1377     __ stfd(src.first()-&gt;as_FloatRegister(), reg2offset(dst.first()), R1_SP);
1378   } else {
1379     // reg to reg
1380     if (dst.first()-&gt;as_FloatRegister() != src.first()-&gt;as_FloatRegister())
1381       __ fmr(dst.first()-&gt;as_FloatRegister(), src.first()-&gt;as_FloatRegister());
1382   }
1383 }
1384 
1385 void SharedRuntime::save_native_result(MacroAssembler *masm, BasicType ret_type, int frame_slots) {
1386   switch (ret_type) {
1387     case T_BOOLEAN:
1388     case T_CHAR:
1389     case T_BYTE:
1390     case T_SHORT:
1391     case T_INT:
1392       __ stw (R3_RET,  frame_slots*VMRegImpl::stack_slot_size, R1_SP);
1393       break;
1394     case T_ARRAY:
1395     case T_OBJECT:
1396     case T_LONG:
1397       __ std (R3_RET,  frame_slots*VMRegImpl::stack_slot_size, R1_SP);
1398       break;
1399     case T_FLOAT:
1400       __ stfs(F1_RET, frame_slots*VMRegImpl::stack_slot_size, R1_SP);
1401       break;
1402     case T_DOUBLE:
1403       __ stfd(F1_RET, frame_slots*VMRegImpl::stack_slot_size, R1_SP);
1404       break;
1405     case T_VOID:
1406       break;
1407     default:
1408       ShouldNotReachHere();
1409       break;
1410   }
1411 }
1412 
1413 void SharedRuntime::restore_native_result(MacroAssembler *masm, BasicType ret_type, int frame_slots) {
1414   switch (ret_type) {
1415     case T_BOOLEAN:
1416     case T_CHAR:
1417     case T_BYTE:
1418     case T_SHORT:
1419     case T_INT:
1420       __ lwz(R3_RET,  frame_slots*VMRegImpl::stack_slot_size, R1_SP);
1421       break;
1422     case T_ARRAY:
1423     case T_OBJECT:
1424     case T_LONG:
1425       __ ld (R3_RET,  frame_slots*VMRegImpl::stack_slot_size, R1_SP);
1426       break;
1427     case T_FLOAT:
1428       __ lfs(F1_RET, frame_slots*VMRegImpl::stack_slot_size, R1_SP);
1429       break;
1430     case T_DOUBLE:
1431       __ lfd(F1_RET, frame_slots*VMRegImpl::stack_slot_size, R1_SP);
1432       break;
1433     case T_VOID:
1434       break;
1435     default:
1436       ShouldNotReachHere();
1437       break;
1438   }
1439 }
1440 
1441 static void save_or_restore_arguments(MacroAssembler* masm,
1442                                       const int stack_slots,
1443                                       const int total_in_args,
1444                                       const int arg_save_area,
1445                                       OopMap* map,
1446                                       VMRegPair* in_regs,
1447                                       BasicType* in_sig_bt) {
1448   // If map is non-NULL then the code should store the values,
1449   // otherwise it should load them.
1450   int slot = arg_save_area;
1451   // Save down double word first.
1452   for (int i = 0; i &lt; total_in_args; i++) {
1453     if (in_regs[i].first()-&gt;is_FloatRegister() &amp;&amp; in_sig_bt[i] == T_DOUBLE) {
1454       int offset = slot * VMRegImpl::stack_slot_size;
1455       slot += VMRegImpl::slots_per_word;
1456       assert(slot &lt;= stack_slots, "overflow (after DOUBLE stack slot)");
1457       if (map != NULL) {
1458         __ stfd(in_regs[i].first()-&gt;as_FloatRegister(), offset, R1_SP);
1459       } else {
1460         __ lfd(in_regs[i].first()-&gt;as_FloatRegister(), offset, R1_SP);
1461       }
1462     } else if (in_regs[i].first()-&gt;is_Register() &amp;&amp;
1463         (in_sig_bt[i] == T_LONG || in_sig_bt[i] == T_ARRAY)) {
1464       int offset = slot * VMRegImpl::stack_slot_size;
1465       if (map != NULL) {
1466         __ std(in_regs[i].first()-&gt;as_Register(), offset, R1_SP);
1467         if (in_sig_bt[i] == T_ARRAY) {
1468           map-&gt;set_oop(VMRegImpl::stack2reg(slot));
1469         }
1470       } else {
1471         __ ld(in_regs[i].first()-&gt;as_Register(), offset, R1_SP);
1472       }
1473       slot += VMRegImpl::slots_per_word;
1474       assert(slot &lt;= stack_slots, "overflow (after LONG/ARRAY stack slot)");
1475     }
1476   }
1477   // Save or restore single word registers.
1478   for (int i = 0; i &lt; total_in_args; i++) {
1479     // PPC64: pass ints as longs: must only deal with floats here.
1480     if (in_regs[i].first()-&gt;is_FloatRegister()) {
1481       if (in_sig_bt[i] == T_FLOAT) {
1482         int offset = slot * VMRegImpl::stack_slot_size;
1483         slot++;
1484         assert(slot &lt;= stack_slots, "overflow (after FLOAT stack slot)");
1485         if (map != NULL) {
1486           __ stfs(in_regs[i].first()-&gt;as_FloatRegister(), offset, R1_SP);
1487         } else {
1488           __ lfs(in_regs[i].first()-&gt;as_FloatRegister(), offset, R1_SP);
1489         }
1490       }
1491     } else if (in_regs[i].first()-&gt;is_stack()) {
1492       if (in_sig_bt[i] == T_ARRAY &amp;&amp; map != NULL) {
1493         int offset_in_older_frame = in_regs[i].first()-&gt;reg2stack() + SharedRuntime::out_preserve_stack_slots();
1494         map-&gt;set_oop(VMRegImpl::stack2reg(offset_in_older_frame + stack_slots));
1495       }
1496     }
1497   }
1498 }
1499 
1500 // Check GC_locker::needs_gc and enter the runtime if it's true. This
1501 // keeps a new JNI critical region from starting until a GC has been
1502 // forced. Save down any oops in registers and describe them in an
1503 // OopMap.
1504 static void check_needs_gc_for_critical_native(MacroAssembler* masm,
1505                                                const int stack_slots,
1506                                                const int total_in_args,
1507                                                const int arg_save_area,
1508                                                OopMapSet* oop_maps,
1509                                                VMRegPair* in_regs,
1510                                                BasicType* in_sig_bt,
1511                                                Register tmp_reg ) {
1512   __ block_comment("check GC_locker::needs_gc");
1513   Label cont;
1514   __ lbz(tmp_reg, (RegisterOrConstant)(intptr_t)GC_locker::needs_gc_address());
1515   __ cmplwi(CCR0, tmp_reg, 0);
1516   __ beq(CCR0, cont);
1517 
1518   // Save down any values that are live in registers and call into the
1519   // runtime to halt for a GC.
1520   OopMap* map = new OopMap(stack_slots * 2, 0 /* arg_slots*/);
1521   save_or_restore_arguments(masm, stack_slots, total_in_args,
1522                             arg_save_area, map, in_regs, in_sig_bt);
1523 
1524   __ mr(R3_ARG1, R16_thread);
1525   __ set_last_Java_frame(R1_SP, noreg);
1526 
1527   __ block_comment("block_for_jni_critical");
1528   address entry_point = CAST_FROM_FN_PTR(address, SharedRuntime::block_for_jni_critical);
1529 #if defined(ABI_ELFv2)
1530   __ call_c(entry_point, relocInfo::runtime_call_type);
1531 #else
1532   __ call_c(CAST_FROM_FN_PTR(FunctionDescriptor*, entry_point), relocInfo::runtime_call_type);
1533 #endif
1534   address start           = __ pc() - __ offset(),
1535           calls_return_pc = __ last_calls_return_pc();
1536   oop_maps-&gt;add_gc_map(calls_return_pc - start, map);
1537 
1538   __ reset_last_Java_frame();
1539 
1540   // Reload all the register arguments.
1541   save_or_restore_arguments(masm, stack_slots, total_in_args,
1542                             arg_save_area, NULL, in_regs, in_sig_bt);
1543 
1544   __ BIND(cont);
1545 
1546 #ifdef ASSERT
1547   if (StressCriticalJNINatives) {
1548     // Stress register saving.
1549     OopMap* map = new OopMap(stack_slots * 2, 0 /* arg_slots*/);
1550     save_or_restore_arguments(masm, stack_slots, total_in_args,
1551                               arg_save_area, map, in_regs, in_sig_bt);
1552     // Destroy argument registers.
1553     for (int i = 0; i &lt; total_in_args; i++) {
1554       if (in_regs[i].first()-&gt;is_Register()) {
1555         const Register reg = in_regs[i].first()-&gt;as_Register();
1556         __ neg(reg, reg);
1557       } else if (in_regs[i].first()-&gt;is_FloatRegister()) {
1558         __ fneg(in_regs[i].first()-&gt;as_FloatRegister(), in_regs[i].first()-&gt;as_FloatRegister());
1559       }
1560     }
1561 
1562     save_or_restore_arguments(masm, stack_slots, total_in_args,
1563                               arg_save_area, NULL, in_regs, in_sig_bt);
1564   }
1565 #endif
1566 }
1567 
1568 static void move_ptr(MacroAssembler* masm, VMRegPair src, VMRegPair dst, Register r_caller_sp, Register r_temp) {
1569   if (src.first()-&gt;is_stack()) {
1570     if (dst.first()-&gt;is_stack()) {
1571       // stack to stack
1572       __ ld(r_temp, reg2offset(src.first()), r_caller_sp);
1573       __ std(r_temp, reg2offset(dst.first()), R1_SP);
1574     } else {
1575       // stack to reg
1576       __ ld(dst.first()-&gt;as_Register(), reg2offset(src.first()), r_caller_sp);
1577     }
1578   } else if (dst.first()-&gt;is_stack()) {
1579     // reg to stack
1580     __ std(src.first()-&gt;as_Register(), reg2offset(dst.first()), R1_SP);
1581   } else {
1582     if (dst.first() != src.first()) {
1583       __ mr(dst.first()-&gt;as_Register(), src.first()-&gt;as_Register());
1584     }
1585   }
1586 }
1587 
1588 // Unpack an array argument into a pointer to the body and the length
1589 // if the array is non-null, otherwise pass 0 for both.
1590 static void unpack_array_argument(MacroAssembler* masm, VMRegPair reg, BasicType in_elem_type,
1591                                   VMRegPair body_arg, VMRegPair length_arg, Register r_caller_sp,
1592                                   Register tmp_reg, Register tmp2_reg) {
1593   assert(!body_arg.first()-&gt;is_Register() || body_arg.first()-&gt;as_Register() != tmp_reg,
1594          "possible collision");
1595   assert(!length_arg.first()-&gt;is_Register() || length_arg.first()-&gt;as_Register() != tmp_reg,
1596          "possible collision");
1597 
1598   // Pass the length, ptr pair.
1599   Label set_out_args;
1600   VMRegPair tmp, tmp2;
1601   tmp.set_ptr(tmp_reg-&gt;as_VMReg());
1602   tmp2.set_ptr(tmp2_reg-&gt;as_VMReg());
1603   if (reg.first()-&gt;is_stack()) {
1604     // Load the arg up from the stack.
1605     move_ptr(masm, reg, tmp, r_caller_sp, /*unused*/ R0);
1606     reg = tmp;
1607   }
1608   __ li(tmp2_reg, 0); // Pass zeros if Array=null.
1609   if (tmp_reg != reg.first()-&gt;as_Register()) __ li(tmp_reg, 0);
1610   __ cmpdi(CCR0, reg.first()-&gt;as_Register(), 0);
1611   __ beq(CCR0, set_out_args);
1612   __ lwa(tmp2_reg, arrayOopDesc::length_offset_in_bytes(), reg.first()-&gt;as_Register());
1613   __ addi(tmp_reg, reg.first()-&gt;as_Register(), arrayOopDesc::base_offset_in_bytes(in_elem_type));
1614   __ bind(set_out_args);
1615   move_ptr(masm, tmp, body_arg, r_caller_sp, /*unused*/ R0);
1616   move_ptr(masm, tmp2, length_arg, r_caller_sp, /*unused*/ R0); // Same as move32_64 on PPC64.
1617 }
1618 
1619 static void verify_oop_args(MacroAssembler* masm,
1620                             methodHandle method,
1621                             const BasicType* sig_bt,
1622                             const VMRegPair* regs) {
1623   Register temp_reg = R19_method;  // not part of any compiled calling seq
1624   if (VerifyOops) {
1625     for (int i = 0; i &lt; method-&gt;size_of_parameters(); i++) {
1626       if (sig_bt[i] == T_OBJECT ||
1627           sig_bt[i] == T_ARRAY) {
1628         VMReg r = regs[i].first();
1629         assert(r-&gt;is_valid(), "bad oop arg");
1630         if (r-&gt;is_stack()) {
1631           __ ld(temp_reg, reg2offset(r), R1_SP);
1632           __ verify_oop(temp_reg);
1633         } else {
1634           __ verify_oop(r-&gt;as_Register());
1635         }
1636       }
1637     }
1638   }
1639 }
1640 
1641 static void gen_special_dispatch(MacroAssembler* masm,
1642                                  methodHandle method,
1643                                  const BasicType* sig_bt,
1644                                  const VMRegPair* regs) {
1645   verify_oop_args(masm, method, sig_bt, regs);
1646   vmIntrinsics::ID iid = method-&gt;intrinsic_id();
1647 
1648   // Now write the args into the outgoing interpreter space
1649   bool     has_receiver   = false;
1650   Register receiver_reg   = noreg;
1651   int      member_arg_pos = -1;
1652   Register member_reg     = noreg;
1653   int      ref_kind       = MethodHandles::signature_polymorphic_intrinsic_ref_kind(iid);
1654   if (ref_kind != 0) {
1655     member_arg_pos = method-&gt;size_of_parameters() - 1;  // trailing MemberName argument
1656     member_reg = R19_method;  // known to be free at this point
1657     has_receiver = MethodHandles::ref_kind_has_receiver(ref_kind);
1658   } else if (iid == vmIntrinsics::_invokeBasic) {
1659     has_receiver = true;
1660   } else {
1661     fatal(err_msg_res("unexpected intrinsic id %d", iid));
1662   }
1663 
1664   if (member_reg != noreg) {
1665     // Load the member_arg into register, if necessary.
1666     SharedRuntime::check_member_name_argument_is_last_argument(method, sig_bt, regs);
1667     VMReg r = regs[member_arg_pos].first();
1668     if (r-&gt;is_stack()) {
1669       __ ld(member_reg, reg2offset(r), R1_SP);
1670     } else {
1671       // no data motion is needed
1672       member_reg = r-&gt;as_Register();
1673     }
1674   }
1675 
1676   if (has_receiver) {
1677     // Make sure the receiver is loaded into a register.
1678     assert(method-&gt;size_of_parameters() &gt; 0, "oob");
1679     assert(sig_bt[0] == T_OBJECT, "receiver argument must be an object");
1680     VMReg r = regs[0].first();
1681     assert(r-&gt;is_valid(), "bad receiver arg");
1682     if (r-&gt;is_stack()) {
1683       // Porting note:  This assumes that compiled calling conventions always
1684       // pass the receiver oop in a register.  If this is not true on some
1685       // platform, pick a temp and load the receiver from stack.
1686       fatal("receiver always in a register");
1687       receiver_reg = R11_scratch1;  // TODO (hs24): is R11_scratch1 really free at this point?
1688       __ ld(receiver_reg, reg2offset(r), R1_SP);
1689     } else {
1690       // no data motion is needed
1691       receiver_reg = r-&gt;as_Register();
1692     }
1693   }
1694 
1695   // Figure out which address we are really jumping to:
1696   MethodHandles::generate_method_handle_dispatch(masm, iid,
1697                                                  receiver_reg, member_reg, /*for_compiler_entry:*/ true);
1698 }
1699 
1700 #endif // COMPILER2
1701 
1702 // ---------------------------------------------------------------------------
1703 // Generate a native wrapper for a given method. The method takes arguments
1704 // in the Java compiled code convention, marshals them to the native
1705 // convention (handlizes oops, etc), transitions to native, makes the call,
1706 // returns to java state (possibly blocking), unhandlizes any result and
1707 // returns.
1708 //
1709 // Critical native functions are a shorthand for the use of
1710 // GetPrimtiveArrayCritical and disallow the use of any other JNI
1711 // functions.  The wrapper is expected to unpack the arguments before
1712 // passing them to the callee and perform checks before and after the
1713 // native call to ensure that they GC_locker
1714 // lock_critical/unlock_critical semantics are followed.  Some other
1715 // parts of JNI setup are skipped like the tear down of the JNI handle
1716 // block and the check for pending exceptions it's impossible for them
1717 // to be thrown.
1718 //
1719 // They are roughly structured like this:
1720 //   if (GC_locker::needs_gc())
1721 //     SharedRuntime::block_for_jni_critical();
1722 //   tranistion to thread_in_native
1723 //   unpack arrray arguments and call native entry point
1724 //   check for safepoint in progress
1725 //   check if any thread suspend flags are set
1726 //     call into JVM and possible unlock the JNI critical
1727 //     if a GC was suppressed while in the critical native.
1728 //   transition back to thread_in_Java
1729 //   return to caller
1730 //
1731 nmethod *SharedRuntime::generate_native_wrapper(MacroAssembler *masm,
1732                                                 methodHandle method,
1733                                                 int compile_id,
1734                                                 BasicType *in_sig_bt,
1735                                                 VMRegPair *in_regs,
1736                                                 BasicType ret_type) {
1737 #ifdef COMPILER2
1738   if (method-&gt;is_method_handle_intrinsic()) {
1739     vmIntrinsics::ID iid = method-&gt;intrinsic_id();
1740     intptr_t start = (intptr_t)__ pc();
1741     int vep_offset = ((intptr_t)__ pc()) - start;
1742     gen_special_dispatch(masm,
1743                          method,
1744                          in_sig_bt,
1745                          in_regs);
1746     int frame_complete = ((intptr_t)__ pc()) - start;  // not complete, period
1747     __ flush();
1748     int stack_slots = SharedRuntime::out_preserve_stack_slots();  // no out slots at all, actually
1749     return nmethod::new_native_nmethod(method,
1750                                        compile_id,
1751                                        masm-&gt;code(),
1752                                        vep_offset,
1753                                        frame_complete,
1754                                        stack_slots / VMRegImpl::slots_per_word,
1755                                        in_ByteSize(-1),
1756                                        in_ByteSize(-1),
1757                                        (OopMapSet*)NULL);
1758   }
1759 
1760   bool is_critical_native = true;
1761   address native_func = method-&gt;critical_native_function();
1762   if (native_func == NULL) {
1763     native_func = method-&gt;native_function();
1764     is_critical_native = false;
1765   }
1766   assert(native_func != NULL, "must have function");
1767 
1768   // First, create signature for outgoing C call
1769   // --------------------------------------------------------------------------
1770 
1771   int total_in_args = method-&gt;size_of_parameters();
1772   // We have received a description of where all the java args are located
1773   // on entry to the wrapper. We need to convert these args to where
1774   // the jni function will expect them. To figure out where they go
1775   // we convert the java signature to a C signature by inserting
1776   // the hidden arguments as arg[0] and possibly arg[1] (static method)
1777   //
1778   // Additionally, on ppc64 we must convert integers to longs in the C
1779   // signature. We do this in advance in order to have no trouble with
1780   // indexes into the bt-arrays.
1781   // So convert the signature and registers now, and adjust the total number
1782   // of in-arguments accordingly.
1783   int i2l_argcnt = convert_ints_to_longints_argcnt(total_in_args, in_sig_bt); // PPC64: pass ints as longs.
1784 
1785   // Calculate the total number of C arguments and create arrays for the
1786   // signature and the outgoing registers.
1787   // On ppc64, we have two arrays for the outgoing registers, because
1788   // some floating-point arguments must be passed in registers _and_
1789   // in stack locations.
1790   bool method_is_static = method-&gt;is_static();
1791   int  total_c_args     = i2l_argcnt;
1792 
1793   if (!is_critical_native) {
1794     int n_hidden_args = method_is_static ? 2 : 1;
1795     total_c_args += n_hidden_args;
1796   } else {
1797     // No JNIEnv*, no this*, but unpacked arrays (base+length).
1798     for (int i = 0; i &lt; total_in_args; i++) {
1799       if (in_sig_bt[i] == T_ARRAY) {
1800         total_c_args += 2; // PPC64: T_LONG, T_INT, T_ADDRESS (see convert_ints_to_longints and c_calling_convention)
1801       }
1802     }
1803   }
1804 
1805   BasicType *out_sig_bt = NEW_RESOURCE_ARRAY(BasicType, total_c_args);
1806   VMRegPair *out_regs   = NEW_RESOURCE_ARRAY(VMRegPair, total_c_args);
1807   VMRegPair *out_regs2  = NEW_RESOURCE_ARRAY(VMRegPair, total_c_args);
1808   BasicType* in_elem_bt = NULL;
1809 
1810   // Create the signature for the C call:
1811   //   1) add the JNIEnv*
1812   //   2) add the class if the method is static
1813   //   3) copy the rest of the incoming signature (shifted by the number of
1814   //      hidden arguments).
1815 
1816   int argc = 0;
1817   if (!is_critical_native) {
1818     convert_ints_to_longints(i2l_argcnt, total_in_args, in_sig_bt, in_regs); // PPC64: pass ints as longs.
1819 
1820     out_sig_bt[argc++] = T_ADDRESS;
1821     if (method-&gt;is_static()) {
1822       out_sig_bt[argc++] = T_OBJECT;
1823     }
1824 
1825     for (int i = 0; i &lt; total_in_args ; i++ ) {
1826       out_sig_bt[argc++] = in_sig_bt[i];
1827     }
1828   } else {
1829     Thread* THREAD = Thread::current();
1830     in_elem_bt = NEW_RESOURCE_ARRAY(BasicType, i2l_argcnt);
1831     SignatureStream ss(method-&gt;signature());
1832     int o = 0;
1833     for (int i = 0; i &lt; total_in_args ; i++, o++) {
1834       if (in_sig_bt[i] == T_ARRAY) {
1835         // Arrays are passed as int, elem* pair
1836         Symbol* atype = ss.as_symbol(CHECK_NULL);
1837         const char* at = atype-&gt;as_C_string();
1838         if (strlen(at) == 2) {
1839           assert(at[0] == '[', "must be");
1840           switch (at[1]) {
1841             case 'B': in_elem_bt[o] = T_BYTE; break;
1842             case 'C': in_elem_bt[o] = T_CHAR; break;
1843             case 'D': in_elem_bt[o] = T_DOUBLE; break;
1844             case 'F': in_elem_bt[o] = T_FLOAT; break;
1845             case 'I': in_elem_bt[o] = T_INT; break;
1846             case 'J': in_elem_bt[o] = T_LONG; break;
1847             case 'S': in_elem_bt[o] = T_SHORT; break;
1848             case 'Z': in_elem_bt[o] = T_BOOLEAN; break;
1849             default: ShouldNotReachHere();
1850           }
1851         }
1852       } else {
1853         in_elem_bt[o] = T_VOID;
1854         switch(in_sig_bt[i]) { // PPC64: pass ints as longs.
1855           case T_BOOLEAN:
1856           case T_CHAR:
1857           case T_BYTE:
1858           case T_SHORT:
1859           case T_INT: in_elem_bt[++o] = T_VOID; break;
1860           default: break;
1861         }
1862       }
1863       if (in_sig_bt[i] != T_VOID) {
1864         assert(in_sig_bt[i] == ss.type(), "must match");
1865         ss.next();
1866       }
1867     }
1868     assert(i2l_argcnt==o, "must match");
1869 
1870     convert_ints_to_longints(i2l_argcnt, total_in_args, in_sig_bt, in_regs); // PPC64: pass ints as longs.
1871 
1872     for (int i = 0; i &lt; total_in_args ; i++ ) {
1873       if (in_sig_bt[i] == T_ARRAY) {
1874         // Arrays are passed as int, elem* pair.
1875         out_sig_bt[argc++] = T_LONG; // PPC64: pass ints as longs.
1876         out_sig_bt[argc++] = T_INT;
1877         out_sig_bt[argc++] = T_ADDRESS;
1878       } else {
1879         out_sig_bt[argc++] = in_sig_bt[i];
1880       }
1881     }
1882   }
1883 
1884 
1885   // Compute the wrapper's frame size.
1886   // --------------------------------------------------------------------------
1887 
1888   // Now figure out where the args must be stored and how much stack space
1889   // they require.
1890   //
1891   // Compute framesize for the wrapper. We need to handlize all oops in
1892   // incoming registers.
1893   //
1894   // Calculate the total number of stack slots we will need:
1895   //   1) abi requirements
1896   //   2) outgoing arguments
1897   //   3) space for inbound oop handle area
1898   //   4) space for handlizing a klass if static method
1899   //   5) space for a lock if synchronized method
1900   //   6) workspace for saving return values, int &lt;-&gt; float reg moves, etc.
1901   //   7) alignment
1902   //
1903   // Layout of the native wrapper frame:
1904   // (stack grows upwards, memory grows downwards)
1905   //
1906   // NW     [ABI_REG_ARGS]             &lt;-- 1) R1_SP
1907   //        [outgoing arguments]       &lt;-- 2) R1_SP + out_arg_slot_offset
1908   //        [oopHandle area]           &lt;-- 3) R1_SP + oop_handle_offset (save area for critical natives)
1909   //        klass                      &lt;-- 4) R1_SP + klass_offset
1910   //        lock                       &lt;-- 5) R1_SP + lock_offset
1911   //        [workspace]                &lt;-- 6) R1_SP + workspace_offset
1912   //        [alignment] (optional)     &lt;-- 7)
1913   // caller [JIT_TOP_ABI_48]           &lt;-- r_callers_sp
1914   //
1915   // - *_slot_offset Indicates offset from SP in number of stack slots.
1916   // - *_offset      Indicates offset from SP in bytes.
1917 
1918   int stack_slots = c_calling_convention(out_sig_bt, out_regs, out_regs2, total_c_args) // 1+2)
1919                   + SharedRuntime::out_preserve_stack_slots(); // See c_calling_convention.
1920 
1921   // Now the space for the inbound oop handle area.
1922   int total_save_slots = num_java_iarg_registers * VMRegImpl::slots_per_word;
1923   if (is_critical_native) {
1924     // Critical natives may have to call out so they need a save area
1925     // for register arguments.
1926     int double_slots = 0;
1927     int single_slots = 0;
1928     for (int i = 0; i &lt; total_in_args; i++) {
1929       if (in_regs[i].first()-&gt;is_Register()) {
1930         const Register reg = in_regs[i].first()-&gt;as_Register();
1931         switch (in_sig_bt[i]) {
1932           case T_BOOLEAN:
1933           case T_BYTE:
1934           case T_SHORT:
1935           case T_CHAR:
1936           case T_INT:  /*single_slots++;*/ break; // PPC64: pass ints as longs.
1937           case T_ARRAY:
1938           case T_LONG: double_slots++; break;
1939           default:  ShouldNotReachHere();
1940         }
1941       } else if (in_regs[i].first()-&gt;is_FloatRegister()) {
1942         switch (in_sig_bt[i]) {
1943           case T_FLOAT:  single_slots++; break;
1944           case T_DOUBLE: double_slots++; break;
1945           default:  ShouldNotReachHere();
1946         }
1947       }
1948     }
1949     total_save_slots = double_slots * 2 + round_to(single_slots, 2); // round to even
1950   }
1951 
1952   int oop_handle_slot_offset = stack_slots;
1953   stack_slots += total_save_slots;                                                // 3)
1954 
1955   int klass_slot_offset = 0;
1956   int klass_offset      = -1;
1957   if (method_is_static &amp;&amp; !is_critical_native) {                                  // 4)
1958     klass_slot_offset  = stack_slots;
1959     klass_offset       = klass_slot_offset * VMRegImpl::stack_slot_size;
1960     stack_slots       += VMRegImpl::slots_per_word;
1961   }
1962 
1963   int lock_slot_offset = 0;
1964   int lock_offset      = -1;
1965   if (method-&gt;is_synchronized()) {                                                // 5)
1966     lock_slot_offset   = stack_slots;
1967     lock_offset        = lock_slot_offset * VMRegImpl::stack_slot_size;
1968     stack_slots       += VMRegImpl::slots_per_word;
1969   }
1970 
1971   int workspace_slot_offset = stack_slots;                                        // 6)
1972   stack_slots         += 2;
1973 
1974   // Now compute actual number of stack words we need.
1975   // Rounding to make stack properly aligned.
1976   stack_slots = round_to(stack_slots,                                             // 7)
1977                          frame::alignment_in_bytes / VMRegImpl::stack_slot_size);
1978   int frame_size_in_bytes = stack_slots * VMRegImpl::stack_slot_size;
1979 
1980 
1981   // Now we can start generating code.
1982   // --------------------------------------------------------------------------
1983 
1984   intptr_t start_pc = (intptr_t)__ pc();
1985   intptr_t vep_start_pc;
1986   intptr_t frame_done_pc;
1987   intptr_t oopmap_pc;
1988 
1989   Label    ic_miss;
1990   Label    handle_pending_exception;
1991 
1992   Register r_callers_sp = R21;
1993   Register r_temp_1     = R22;
1994   Register r_temp_2     = R23;
1995   Register r_temp_3     = R24;
1996   Register r_temp_4     = R25;
1997   Register r_temp_5     = R26;
1998   Register r_temp_6     = R27;
1999   Register r_return_pc  = R28;
2000 
2001   Register r_carg1_jnienv        = noreg;
2002   Register r_carg2_classorobject = noreg;
2003   if (!is_critical_native) {
2004     r_carg1_jnienv        = out_regs[0].first()-&gt;as_Register();
2005     r_carg2_classorobject = out_regs[1].first()-&gt;as_Register();
2006   }
2007 
2008 
2009   // Generate the Unverified Entry Point (UEP).
2010   // --------------------------------------------------------------------------
2011   assert(start_pc == (intptr_t)__ pc(), "uep must be at start");
2012 
2013   // Check ic: object class == cached class?
2014   if (!method_is_static) {
2015   Register ic = as_Register(Matcher::inline_cache_reg_encode());
2016   Register receiver_klass = r_temp_1;
2017 
2018   __ cmpdi(CCR0, R3_ARG1, 0);
2019   __ beq(CCR0, ic_miss);
2020   __ verify_oop(R3_ARG1);
2021   __ load_klass(receiver_klass, R3_ARG1);
2022 
2023   __ cmpd(CCR0, receiver_klass, ic);
2024   __ bne(CCR0, ic_miss);
2025   }
2026 
2027 
2028   // Generate the Verified Entry Point (VEP).
2029   // --------------------------------------------------------------------------
2030   vep_start_pc = (intptr_t)__ pc();
2031 
2032   __ save_LR_CR(r_temp_1);
2033   __ generate_stack_overflow_check(frame_size_in_bytes); // Check before creating frame.
2034   __ mr(r_callers_sp, R1_SP);                       // Remember frame pointer.
2035   __ push_frame(frame_size_in_bytes, r_temp_1);          // Push the c2n adapter's frame.
2036   frame_done_pc = (intptr_t)__ pc();
2037 
2038   // Native nmethod wrappers never take possesion of the oop arguments.
2039   // So the caller will gc the arguments.
2040   // The only thing we need an oopMap for is if the call is static.
2041   //
2042   // An OopMap for lock (and class if static), and one for the VM call itself.
2043   OopMapSet *oop_maps = new OopMapSet();
2044   OopMap    *oop_map  = new OopMap(stack_slots * 2, 0 /* arg_slots*/);
2045 
2046   if (is_critical_native) {
2047     check_needs_gc_for_critical_native(masm, stack_slots, total_in_args, oop_handle_slot_offset, oop_maps, in_regs, in_sig_bt, r_temp_1);
2048   }
2049 
2050   // Move arguments from register/stack to register/stack.
2051   // --------------------------------------------------------------------------
2052   //
2053   // We immediately shuffle the arguments so that for any vm call we have
2054   // to make from here on out (sync slow path, jvmti, etc.) we will have
2055   // captured the oops from our caller and have a valid oopMap for them.
2056   //
2057   // Natives require 1 or 2 extra arguments over the normal ones: the JNIEnv*
2058   // (derived from JavaThread* which is in R16_thread) and, if static,
2059   // the class mirror instead of a receiver. This pretty much guarantees that
2060   // register layout will not match. We ignore these extra arguments during
2061   // the shuffle. The shuffle is described by the two calling convention
2062   // vectors we have in our possession. We simply walk the java vector to
2063   // get the source locations and the c vector to get the destinations.
2064 
2065   // Record sp-based slot for receiver on stack for non-static methods.
2066   int receiver_offset = -1;
2067 
2068   // We move the arguments backward because the floating point registers
2069   // destination will always be to a register with a greater or equal
2070   // register number or the stack.
2071   //   in  is the index of the incoming Java arguments
2072   //   out is the index of the outgoing C arguments
2073 
2074 #ifdef ASSERT
2075   bool reg_destroyed[RegisterImpl::number_of_registers];
2076   bool freg_destroyed[FloatRegisterImpl::number_of_registers];
2077   for (int r = 0 ; r &lt; RegisterImpl::number_of_registers ; r++) {
2078     reg_destroyed[r] = false;
2079   }
2080   for (int f = 0 ; f &lt; FloatRegisterImpl::number_of_registers ; f++) {
2081     freg_destroyed[f] = false;
2082   }
2083 #endif // ASSERT
2084 
2085   for (int in = total_in_args - 1, out = total_c_args - 1; in &gt;= 0 ; in--, out--) {
2086 
2087 #ifdef ASSERT
2088     if (in_regs[in].first()-&gt;is_Register()) {
2089       assert(!reg_destroyed[in_regs[in].first()-&gt;as_Register()-&gt;encoding()], "ack!");
2090     } else if (in_regs[in].first()-&gt;is_FloatRegister()) {
2091       assert(!freg_destroyed[in_regs[in].first()-&gt;as_FloatRegister()-&gt;encoding()], "ack!");
2092     }
2093     if (out_regs[out].first()-&gt;is_Register()) {
2094       reg_destroyed[out_regs[out].first()-&gt;as_Register()-&gt;encoding()] = true;
2095     } else if (out_regs[out].first()-&gt;is_FloatRegister()) {
2096       freg_destroyed[out_regs[out].first()-&gt;as_FloatRegister()-&gt;encoding()] = true;
2097     }
2098     if (out_regs2[out].first()-&gt;is_Register()) {
2099       reg_destroyed[out_regs2[out].first()-&gt;as_Register()-&gt;encoding()] = true;
2100     } else if (out_regs2[out].first()-&gt;is_FloatRegister()) {
2101       freg_destroyed[out_regs2[out].first()-&gt;as_FloatRegister()-&gt;encoding()] = true;
2102     }
2103 #endif // ASSERT
2104 
2105     switch (in_sig_bt[in]) {
2106       case T_BOOLEAN:
2107       case T_CHAR:
2108       case T_BYTE:
2109       case T_SHORT:
2110       case T_INT:
2111         guarantee(in &gt; 0 &amp;&amp; in_sig_bt[in-1] == T_LONG,
2112                   "expecting type (T_LONG,bt) for bt in {T_BOOLEAN, T_CHAR, T_BYTE, T_SHORT, T_INT}");
2113         break;
2114       case T_LONG:
2115         if (in_sig_bt[in+1] == T_VOID) {
2116           long_move(masm, in_regs[in], out_regs[out], r_callers_sp, r_temp_1);
2117         } else {
2118           guarantee(in_sig_bt[in+1] == T_BOOLEAN || in_sig_bt[in+1] == T_CHAR  ||
2119                     in_sig_bt[in+1] == T_BYTE    || in_sig_bt[in+1] == T_SHORT ||
2120                     in_sig_bt[in+1] == T_INT,
2121                  "expecting type (T_LONG,bt) for bt in {T_BOOLEAN, T_CHAR, T_BYTE, T_SHORT, T_INT}");
2122           int_move(masm, in_regs[in], out_regs[out], r_callers_sp, r_temp_1);
2123         }
2124         break;
2125       case T_ARRAY:
2126         if (is_critical_native) {
2127           int body_arg = out;
2128           out -= 2; // Point to length arg. PPC64: pass ints as longs.
2129           unpack_array_argument(masm, in_regs[in], in_elem_bt[in], out_regs[body_arg], out_regs[out],
2130                                 r_callers_sp, r_temp_1, r_temp_2);
2131           break;
2132         }
2133       case T_OBJECT:
2134         assert(!is_critical_native, "no oop arguments");
2135         object_move(masm, stack_slots,
2136                     oop_map, oop_handle_slot_offset,
2137                     ((in == 0) &amp;&amp; (!method_is_static)), &amp;receiver_offset,
2138                     in_regs[in], out_regs[out],
2139                     r_callers_sp, r_temp_1, r_temp_2);
2140         break;
2141       case T_VOID:
2142         break;
2143       case T_FLOAT:
2144         float_move(masm, in_regs[in], out_regs[out], r_callers_sp, r_temp_1);
2145         if (out_regs2[out].first()-&gt;is_valid()) {
2146           float_move(masm, in_regs[in], out_regs2[out], r_callers_sp, r_temp_1);
2147         }
2148         break;
2149       case T_DOUBLE:
2150         double_move(masm, in_regs[in], out_regs[out], r_callers_sp, r_temp_1);
2151         if (out_regs2[out].first()-&gt;is_valid()) {
2152           double_move(masm, in_regs[in], out_regs2[out], r_callers_sp, r_temp_1);
2153         }
2154         break;
2155       case T_ADDRESS:
2156         fatal("found type (T_ADDRESS) in java args");
2157         break;
2158       default:
2159         ShouldNotReachHere();
2160         break;
2161     }
2162   }
2163 
2164   // Pre-load a static method's oop into ARG2.
2165   // Used both by locking code and the normal JNI call code.
2166   if (method_is_static &amp;&amp; !is_critical_native) {
2167     __ set_oop_constant(JNIHandles::make_local(method-&gt;method_holder()-&gt;java_mirror()),
2168                         r_carg2_classorobject);
2169 
2170     // Now handlize the static class mirror in carg2. It's known not-null.
2171     __ std(r_carg2_classorobject, klass_offset, R1_SP);
2172     oop_map-&gt;set_oop(VMRegImpl::stack2reg(klass_slot_offset));
2173     __ addi(r_carg2_classorobject, R1_SP, klass_offset);
2174   }
2175 
2176   // Get JNIEnv* which is first argument to native.
2177   if (!is_critical_native) {
2178     __ addi(r_carg1_jnienv, R16_thread, in_bytes(JavaThread::jni_environment_offset()));
2179   }
2180 
2181   // NOTE:
2182   //
2183   // We have all of the arguments setup at this point.
2184   // We MUST NOT touch any outgoing regs from this point on.
2185   // So if we must call out we must push a new frame.
2186 
2187   // Get current pc for oopmap, and load it patchable relative to global toc.
2188   oopmap_pc = (intptr_t) __ pc();
2189   __ calculate_address_from_global_toc(r_return_pc, (address)oopmap_pc, true, true, true, true);
2190 
2191   // We use the same pc/oopMap repeatedly when we call out.
2192   oop_maps-&gt;add_gc_map(oopmap_pc - start_pc, oop_map);
2193 
2194   // r_return_pc now has the pc loaded that we will use when we finally call
2195   // to native.
2196 
2197   // Make sure that thread is non-volatile; it crosses a bunch of VM calls below.
2198   assert(R16_thread-&gt;is_nonvolatile(), "thread must be in non-volatile register");
2199 
2200 
2201 # if 0
2202   // DTrace method entry
2203 # endif
2204 
2205   // Lock a synchronized method.
2206   // --------------------------------------------------------------------------
2207 
2208   if (method-&gt;is_synchronized()) {
2209     assert(!is_critical_native, "unhandled");
2210     ConditionRegister r_flag = CCR1;
2211     Register          r_oop  = r_temp_4;
2212     const Register    r_box  = r_temp_5;
2213     Label             done, locked;
2214 
2215     // Load the oop for the object or class. r_carg2_classorobject contains
2216     // either the handlized oop from the incoming arguments or the handlized
2217     // class mirror (if the method is static).
2218     __ ld(r_oop, 0, r_carg2_classorobject);
2219 
2220     // Get the lock box slot's address.
2221     __ addi(r_box, R1_SP, lock_offset);
2222 
2223 #   ifdef ASSERT
2224     if (UseBiasedLocking) {
2225       // Making the box point to itself will make it clear it went unused
2226       // but also be obviously invalid.
2227       __ std(r_box, 0, r_box);
2228     }
2229 #   endif // ASSERT
2230 
2231     // Try fastpath for locking.
2232     // fast_lock kills r_temp_1, r_temp_2, r_temp_3.
2233     __ compiler_fast_lock_object(r_flag, r_oop, r_box, r_temp_1, r_temp_2, r_temp_3);
2234     __ beq(r_flag, locked);
2235 
2236     // None of the above fast optimizations worked so we have to get into the
2237     // slow case of monitor enter. Inline a special case of call_VM that
2238     // disallows any pending_exception.
2239 
2240     // Save argument registers and leave room for C-compatible ABI_REG_ARGS.
2241     int frame_size = frame::abi_reg_args_size +
2242                      round_to(total_c_args * wordSize, frame::alignment_in_bytes);
2243     __ mr(R11_scratch1, R1_SP);
2244     RegisterSaver::push_frame_and_save_argument_registers(masm, R12_scratch2, frame_size, total_c_args, out_regs, out_regs2);
2245 
2246     // Do the call.
2247     __ set_last_Java_frame(R11_scratch1, r_return_pc);
2248     assert(r_return_pc-&gt;is_nonvolatile(), "expecting return pc to be in non-volatile register");
2249     __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::complete_monitor_locking_C), r_oop, r_box, R16_thread);
2250     __ reset_last_Java_frame();
2251 
2252     RegisterSaver::restore_argument_registers_and_pop_frame(masm, frame_size, total_c_args, out_regs, out_regs2);
2253 
2254     __ asm_assert_mem8_is_zero(thread_(pending_exception),
2255        "no pending exception allowed on exit from SharedRuntime::complete_monitor_locking_C", 0);
2256 
2257     __ bind(locked);
2258   }
2259 
2260 
2261   // Publish thread state
2262   // --------------------------------------------------------------------------
2263 
2264   // Use that pc we placed in r_return_pc a while back as the current frame anchor.
2265   __ set_last_Java_frame(R1_SP, r_return_pc);
2266 
2267   // Transition from _thread_in_Java to _thread_in_native.
2268   __ li(R0, _thread_in_native);
2269   __ release();
2270   // TODO: PPC port assert(4 == JavaThread::sz_thread_state(), "unexpected field size");
2271   __ stw(R0, thread_(thread_state));
2272   if (UseMembar) {
2273     __ fence();
2274   }
2275 
2276 
2277   // The JNI call
2278   // --------------------------------------------------------------------------
2279 #if defined(ABI_ELFv2)
2280   __ call_c(native_func, relocInfo::runtime_call_type);
2281 #else
2282   FunctionDescriptor* fd_native_method = (FunctionDescriptor*) native_func;
2283   __ call_c(fd_native_method, relocInfo::runtime_call_type);
2284 #endif
2285 
2286 
2287   // Now, we are back from the native code.
2288 
2289 
2290   // Unpack the native result.
2291   // --------------------------------------------------------------------------
2292 
2293   // For int-types, we do any needed sign-extension required.
2294   // Care must be taken that the return values (R3_RET and F1_RET)
2295   // will survive any VM calls for blocking or unlocking.
2296   // An OOP result (handle) is done specially in the slow-path code.
2297 
2298   switch (ret_type) {
2299     case T_VOID:    break;        // Nothing to do!
2300     case T_FLOAT:   break;        // Got it where we want it (unless slow-path).
2301     case T_DOUBLE:  break;        // Got it where we want it (unless slow-path).
2302     case T_LONG:    break;        // Got it where we want it (unless slow-path).
2303     case T_OBJECT:  break;        // Really a handle.
2304                                   // Cannot de-handlize until after reclaiming jvm_lock.
2305     case T_ARRAY:   break;
2306 
2307     case T_BOOLEAN: {             // 0 -&gt; false(0); !0 -&gt; true(1)
2308       Label skip_modify;
2309       __ cmpwi(CCR0, R3_RET, 0);
2310       __ beq(CCR0, skip_modify);
2311       __ li(R3_RET, 1);
2312       __ bind(skip_modify);
2313       break;
2314       }
2315     case T_BYTE: {                // sign extension
2316       __ extsb(R3_RET, R3_RET);
2317       break;
2318       }
2319     case T_CHAR: {                // unsigned result
2320       __ andi(R3_RET, R3_RET, 0xffff);
2321       break;
2322       }
2323     case T_SHORT: {               // sign extension
2324       __ extsh(R3_RET, R3_RET);
2325       break;
2326       }
2327     case T_INT:                   // nothing to do
2328       break;
2329     default:
2330       ShouldNotReachHere();
2331       break;
2332   }
2333 
2334 
2335   // Publish thread state
2336   // --------------------------------------------------------------------------
2337 
2338   // Switch thread to "native transition" state before reading the
2339   // synchronization state. This additional state is necessary because reading
2340   // and testing the synchronization state is not atomic w.r.t. GC, as this
2341   // scenario demonstrates:
2342   //   - Java thread A, in _thread_in_native state, loads _not_synchronized
2343   //     and is preempted.
2344   //   - VM thread changes sync state to synchronizing and suspends threads
2345   //     for GC.
2346   //   - Thread A is resumed to finish this native method, but doesn't block
2347   //     here since it didn't see any synchronization in progress, and escapes.
2348 
2349   // Transition from _thread_in_native to _thread_in_native_trans.
2350   __ li(R0, _thread_in_native_trans);
2351   __ release();
2352   // TODO: PPC port assert(4 == JavaThread::sz_thread_state(), "unexpected field size");
2353   __ stw(R0, thread_(thread_state));
2354 
2355 
2356   // Must we block?
2357   // --------------------------------------------------------------------------
2358 
2359   // Block, if necessary, before resuming in _thread_in_Java state.
2360   // In order for GC to work, don't clear the last_Java_sp until after blocking.
2361   Label after_transition;
2362   {
2363     Label no_block, sync;
2364 
2365     if (os::is_MP()) {
2366       if (UseMembar) {
2367         // Force this write out before the read below.
2368         __ fence();
2369       } else {
2370         // Write serialization page so VM thread can do a pseudo remote membar.
2371         // We use the current thread pointer to calculate a thread specific
2372         // offset to write to within the page. This minimizes bus traffic
2373         // due to cache line collision.
2374         __ serialize_memory(R16_thread, r_temp_4, r_temp_5);
2375       }
2376     }
2377 
2378     Register sync_state_addr = r_temp_4;
2379     Register sync_state      = r_temp_5;
2380     Register suspend_flags   = r_temp_6;
2381 
2382     __ load_const(sync_state_addr, SafepointSynchronize::address_of_state(), /*temp*/ sync_state);
2383 
2384     // TODO: PPC port assert(4 == SafepointSynchronize::sz_state(), "unexpected field size");
2385     __ lwz(sync_state, 0, sync_state_addr);
2386 
2387     // TODO: PPC port assert(4 == Thread::sz_suspend_flags(), "unexpected field size");
2388     __ lwz(suspend_flags, thread_(suspend_flags));
2389 
2390     __ acquire();
2391 
2392     Label do_safepoint;
2393     // No synchronization in progress nor yet synchronized.
2394     __ cmpwi(CCR0, sync_state, SafepointSynchronize::_not_synchronized);
2395     // Not suspended.
2396     __ cmpwi(CCR1, suspend_flags, 0);
2397 
2398     __ bne(CCR0, sync);
2399     __ beq(CCR1, no_block);
2400 
2401     // Block. Save any potential method result value before the operation and
2402     // use a leaf call to leave the last_Java_frame setup undisturbed. Doing this
2403     // lets us share the oopMap we used when we went native rather than create
2404     // a distinct one for this pc.
2405     __ bind(sync);
2406 
2407     address entry_point = is_critical_native
2408       ? CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans_and_transition)
2409       : CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans);
2410     save_native_result(masm, ret_type, workspace_slot_offset);
2411     __ call_VM_leaf(entry_point, R16_thread);
2412     restore_native_result(masm, ret_type, workspace_slot_offset);
2413 
2414     if (is_critical_native) {
2415       __ b(after_transition); // No thread state transition here.
2416     }
2417     __ bind(no_block);
2418   }
2419 
2420   // Publish thread state.
2421   // --------------------------------------------------------------------------
2422 
2423   // Thread state is thread_in_native_trans. Any safepoint blocking has
2424   // already happened so we can now change state to _thread_in_Java.
2425 
2426   // Transition from _thread_in_native_trans to _thread_in_Java.
2427   __ li(R0, _thread_in_Java);
2428   __ release();
2429   // TODO: PPC port assert(4 == JavaThread::sz_thread_state(), "unexpected field size");
2430   __ stw(R0, thread_(thread_state));
2431   if (UseMembar) {
2432     __ fence();
2433   }
2434   __ bind(after_transition);
2435 
2436   // Reguard any pages if necessary.
2437   // --------------------------------------------------------------------------
2438 
2439   Label no_reguard;
2440   __ lwz(r_temp_1, thread_(stack_guard_state));
2441   __ cmpwi(CCR0, r_temp_1, JavaThread::stack_guard_yellow_disabled);
2442   __ bne(CCR0, no_reguard);
2443 
2444   save_native_result(masm, ret_type, workspace_slot_offset);
2445   __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::reguard_yellow_pages));
2446   restore_native_result(masm, ret_type, workspace_slot_offset);
2447 
2448   __ bind(no_reguard);
2449 
2450 
2451   // Unlock
2452   // --------------------------------------------------------------------------
2453 
2454   if (method-&gt;is_synchronized()) {
2455 
2456     ConditionRegister r_flag   = CCR1;
2457     const Register r_oop       = r_temp_4;
2458     const Register r_box       = r_temp_5;
2459     const Register r_exception = r_temp_6;
2460     Label done;
2461 
2462     // Get oop and address of lock object box.
2463     if (method_is_static) {
2464       assert(klass_offset != -1, "");
2465       __ ld(r_oop, klass_offset, R1_SP);
2466     } else {
2467       assert(receiver_offset != -1, "");
2468       __ ld(r_oop, receiver_offset, R1_SP);
2469     }
2470     __ addi(r_box, R1_SP, lock_offset);
2471 
2472     // Try fastpath for unlocking.
2473     __ compiler_fast_unlock_object(r_flag, r_oop, r_box, r_temp_1, r_temp_2, r_temp_3);
2474     __ beq(r_flag, done);
2475 
2476     // Save and restore any potential method result value around the unlocking operation.
2477     save_native_result(masm, ret_type, workspace_slot_offset);
2478 
2479     // Must save pending exception around the slow-path VM call. Since it's a
2480     // leaf call, the pending exception (if any) can be kept in a register.
2481     __ ld(r_exception, thread_(pending_exception));
2482     assert(r_exception-&gt;is_nonvolatile(), "exception register must be non-volatile");
2483     __ li(R0, 0);
2484     __ std(R0, thread_(pending_exception));
2485 
2486     // Slow case of monitor enter.
2487     // Inline a special case of call_VM that disallows any pending_exception.
2488     __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::complete_monitor_unlocking_C), r_oop, r_box);
2489 
2490     __ asm_assert_mem8_is_zero(thread_(pending_exception),
2491        "no pending exception allowed on exit from SharedRuntime::complete_monitor_unlocking_C", 0);
2492 
2493     restore_native_result(masm, ret_type, workspace_slot_offset);
2494 
2495     // Check_forward_pending_exception jump to forward_exception if any pending
2496     // exception is set. The forward_exception routine expects to see the
2497     // exception in pending_exception and not in a register. Kind of clumsy,
2498     // since all folks who branch to forward_exception must have tested
2499     // pending_exception first and hence have it in a register already.
2500     __ std(r_exception, thread_(pending_exception));
2501 
2502     __ bind(done);
2503   }
2504 
2505 # if 0
2506   // DTrace method exit
2507 # endif
2508 
2509   // Clear "last Java frame" SP and PC.
2510   // --------------------------------------------------------------------------
2511 
2512   __ reset_last_Java_frame();
2513 
2514   // Unpack oop result.
2515   // --------------------------------------------------------------------------
2516 
2517   if (ret_type == T_OBJECT || ret_type == T_ARRAY) {
2518     Label skip_unboxing;
2519     __ cmpdi(CCR0, R3_RET, 0);
2520     __ beq(CCR0, skip_unboxing);
2521     __ ld(R3_RET, 0, R3_RET);
2522     __ bind(skip_unboxing);
2523     __ verify_oop(R3_RET);
2524   }
2525 
2526 
2527   // Reset handle block.
2528   // --------------------------------------------------------------------------
2529   if (!is_critical_native) {
2530   __ ld(r_temp_1, thread_(active_handles));
2531   // TODO: PPC port assert(4 == JNIHandleBlock::top_size_in_bytes(), "unexpected field size");
2532   __ li(r_temp_2, 0);
2533   __ stw(r_temp_2, JNIHandleBlock::top_offset_in_bytes(), r_temp_1);
2534 
2535 
2536   // Check for pending exceptions.
2537   // --------------------------------------------------------------------------
2538   __ ld(r_temp_2, thread_(pending_exception));
2539   __ cmpdi(CCR0, r_temp_2, 0);
2540   __ bne(CCR0, handle_pending_exception);
2541   }
2542 
2543   // Return
2544   // --------------------------------------------------------------------------
2545 
2546   __ pop_frame();
2547   __ restore_LR_CR(R11);
2548   __ blr();
2549 
2550 
2551   // Handler for pending exceptions (out-of-line).
2552   // --------------------------------------------------------------------------
2553 
2554   // Since this is a native call, we know the proper exception handler
2555   // is the empty function. We just pop this frame and then jump to
2556   // forward_exception_entry.
2557   if (!is_critical_native) {
2558   __ align(InteriorEntryAlignment);
2559   __ bind(handle_pending_exception);
2560 
2561   __ pop_frame();
2562   __ restore_LR_CR(R11);
2563   __ b64_patchable((address)StubRoutines::forward_exception_entry(),
2564                        relocInfo::runtime_call_type);
2565   }
2566 
2567   // Handler for a cache miss (out-of-line).
2568   // --------------------------------------------------------------------------
2569 
2570   if (!method_is_static) {
2571   __ align(InteriorEntryAlignment);
2572   __ bind(ic_miss);
2573 
2574   __ b64_patchable((address)SharedRuntime::get_ic_miss_stub(),
2575                        relocInfo::runtime_call_type);
2576   }
2577 
2578   // Done.
2579   // --------------------------------------------------------------------------
2580 
2581   __ flush();
2582 
2583   nmethod *nm = nmethod::new_native_nmethod(method,
2584                                             compile_id,
2585                                             masm-&gt;code(),
2586                                             vep_start_pc-start_pc,
2587                                             frame_done_pc-start_pc,
2588                                             stack_slots / VMRegImpl::slots_per_word,
2589                                             (method_is_static ? in_ByteSize(klass_offset) : in_ByteSize(receiver_offset)),
2590                                             in_ByteSize(lock_offset),
2591                                             oop_maps);
2592 
2593   if (is_critical_native) {
2594     nm-&gt;set_lazy_critical_native(true);
2595   }
2596 
2597   return nm;
2598 #else
2599   ShouldNotReachHere();
2600   return NULL;
2601 #endif // COMPILER2
2602 }
2603 
2604 // This function returns the adjust size (in number of words) to a c2i adapter
2605 // activation for use during deoptimization.
2606 int Deoptimization::last_frame_adjust(int callee_parameters, int callee_locals) {
2607   return round_to((callee_locals - callee_parameters) * Interpreter::stackElementWords, frame::alignment_in_bytes);
2608 }
2609 
2610 uint SharedRuntime::out_preserve_stack_slots() {
2611 #ifdef COMPILER2
2612   return frame::jit_out_preserve_size / VMRegImpl::stack_slot_size;
2613 #else
2614   return 0;
2615 #endif
2616 }
2617 
2618 #ifdef COMPILER2
2619 // Frame generation for deopt and uncommon trap blobs.
2620 static void push_skeleton_frame(MacroAssembler* masm, bool deopt,
2621                                 /* Read */
2622                                 Register unroll_block_reg,
2623                                 /* Update */
2624                                 Register frame_sizes_reg,
2625                                 Register number_of_frames_reg,
2626                                 Register pcs_reg,
2627                                 /* Invalidate */
2628                                 Register frame_size_reg,
2629                                 Register pc_reg) {
2630 
2631   __ ld(pc_reg, 0, pcs_reg);
2632   __ ld(frame_size_reg, 0, frame_sizes_reg);
2633   __ std(pc_reg, _abi(lr), R1_SP);
2634   __ push_frame(frame_size_reg, R0/*tmp*/);
2635 #ifdef CC_INTERP
2636   __ std(R1_SP, _parent_ijava_frame_abi(initial_caller_sp), R1_SP);
2637 #else
2638 #ifdef ASSERT
2639   __ load_const_optimized(pc_reg, 0x5afe);
2640   __ std(pc_reg, _ijava_state_neg(ijava_reserved), R1_SP);
2641 #endif
2642   __ std(R1_SP, _ijava_state_neg(sender_sp), R1_SP);
2643 #endif // CC_INTERP
2644   __ addi(number_of_frames_reg, number_of_frames_reg, -1);
2645   __ addi(frame_sizes_reg, frame_sizes_reg, wordSize);
2646   __ addi(pcs_reg, pcs_reg, wordSize);
2647 }
2648 
2649 // Loop through the UnrollBlock info and create new frames.
2650 static void push_skeleton_frames(MacroAssembler* masm, bool deopt,
2651                                  /* read */
2652                                  Register unroll_block_reg,
2653                                  /* invalidate */
2654                                  Register frame_sizes_reg,
2655                                  Register number_of_frames_reg,
2656                                  Register pcs_reg,
2657                                  Register frame_size_reg,
2658                                  Register pc_reg) {
2659   Label loop;
2660 
2661  // _number_of_frames is of type int (deoptimization.hpp)
2662   __ lwa(number_of_frames_reg,
2663              Deoptimization::UnrollBlock::number_of_frames_offset_in_bytes(),
2664              unroll_block_reg);
2665   __ ld(pcs_reg,
2666             Deoptimization::UnrollBlock::frame_pcs_offset_in_bytes(),
2667             unroll_block_reg);
2668   __ ld(frame_sizes_reg,
2669             Deoptimization::UnrollBlock::frame_sizes_offset_in_bytes(),
2670             unroll_block_reg);
2671 
2672   // stack: (caller_of_deoptee, ...).
2673 
2674   // At this point we either have an interpreter frame or a compiled
2675   // frame on top of stack. If it is a compiled frame we push a new c2i
2676   // adapter here
2677 
2678   // Memorize top-frame stack-pointer.
2679   __ mr(frame_size_reg/*old_sp*/, R1_SP);
2680 
2681   // Resize interpreter top frame OR C2I adapter.
2682 
2683   // At this moment, the top frame (which is the caller of the deoptee) is
2684   // an interpreter frame or a newly pushed C2I adapter or an entry frame.
2685   // The top frame has a TOP_IJAVA_FRAME_ABI and the frame contains the
2686   // outgoing arguments.
2687   //
2688   // In order to push the interpreter frame for the deoptee, we need to
2689   // resize the top frame such that we are able to place the deoptee's
2690   // locals in the frame.
2691   // Additionally, we have to turn the top frame's TOP_IJAVA_FRAME_ABI
2692   // into a valid PARENT_IJAVA_FRAME_ABI.
2693 
2694   __ lwa(R11_scratch1,
2695              Deoptimization::UnrollBlock::caller_adjustment_offset_in_bytes(),
2696              unroll_block_reg);
2697   __ neg(R11_scratch1, R11_scratch1);
2698 
2699   // R11_scratch1 contains size of locals for frame resizing.
2700   // R12_scratch2 contains top frame's lr.
2701 
2702   // Resize frame by complete frame size prevents TOC from being
2703   // overwritten by locals. A more stack space saving way would be
2704   // to copy the TOC to its location in the new abi.
2705   __ addi(R11_scratch1, R11_scratch1, - frame::parent_ijava_frame_abi_size);
2706 
2707   // now, resize the frame
2708   __ resize_frame(R11_scratch1, pc_reg/*tmp*/);
2709 
2710   // In the case where we have resized a c2i frame above, the optional
2711   // alignment below the locals has size 32 (why?).
2712   __ std(R12_scratch2, _abi(lr), R1_SP);
2713 
2714   // Initialize initial_caller_sp.
2715 #ifdef CC_INTERP
2716   __ std(frame_size_reg/*old_sp*/, _parent_ijava_frame_abi(initial_caller_sp), R1_SP);
2717 #else
2718 #ifdef ASSERT
2719  __ load_const_optimized(pc_reg, 0x5afe);
2720  __ std(pc_reg, _ijava_state_neg(ijava_reserved), R1_SP);
2721 #endif
2722  __ std(frame_size_reg, _ijava_state_neg(sender_sp), R1_SP);
2723 #endif // CC_INTERP
2724 
2725 #ifdef ASSERT
2726   // Make sure that there is at least one entry in the array.
2727   __ cmpdi(CCR0, number_of_frames_reg, 0);
2728   __ asm_assert_ne("array_size must be &gt; 0", 0x205);
2729 #endif
2730 
2731   // Now push the new interpreter frames.
2732   //
2733   __ bind(loop);
2734   // Allocate a new frame, fill in the pc.
2735   push_skeleton_frame(masm, deopt,
2736                       unroll_block_reg,
2737                       frame_sizes_reg,
2738                       number_of_frames_reg,
2739                       pcs_reg,
2740                       frame_size_reg,
2741                       pc_reg);
2742   __ cmpdi(CCR0, number_of_frames_reg, 0);
2743   __ bne(CCR0, loop);
2744 
2745   // Get the return address pointing into the frame manager.
2746   __ ld(R0, 0, pcs_reg);
2747   // Store it in the top interpreter frame.
2748   __ std(R0, _abi(lr), R1_SP);
2749   // Initialize frame_manager_lr of interpreter top frame.
2750 #ifdef CC_INTERP
2751   __ std(R0, _top_ijava_frame_abi(frame_manager_lr), R1_SP);
2752 #endif
2753 }
2754 #endif
2755 
2756 void SharedRuntime::generate_deopt_blob() {
2757   // Allocate space for the code
2758   ResourceMark rm;
2759   // Setup code generation tools
2760   CodeBuffer buffer("deopt_blob", 2048, 1024);
2761   InterpreterMacroAssembler* masm = new InterpreterMacroAssembler(&amp;buffer);
2762   Label exec_mode_initialized;
2763   int frame_size_in_words;
2764   OopMap* map = NULL;
2765   OopMapSet *oop_maps = new OopMapSet();
2766 
2767   // size of ABI112 plus spill slots for R3_RET and F1_RET.
2768   const int frame_size_in_bytes = frame::abi_reg_args_spill_size;
2769   const int frame_size_in_slots = frame_size_in_bytes / sizeof(jint);
2770   int first_frame_size_in_bytes = 0; // frame size of "unpack frame" for call to fetch_unroll_info.
2771 
2772   const Register exec_mode_reg = R21_tmp1;
2773 
2774   const address start = __ pc();
2775 
2776 #ifdef COMPILER2
2777   // --------------------------------------------------------------------------
2778   // Prolog for non exception case!
2779 
2780   // We have been called from the deopt handler of the deoptee.
2781   //
2782   // deoptee:
2783   //                      ...
2784   //                      call X
2785   //                      ...
2786   //  deopt_handler:      call_deopt_stub
2787   //  cur. return pc  --&gt; ...
2788   //
2789   // So currently SR_LR points behind the call in the deopt handler.
2790   // We adjust it such that it points to the start of the deopt handler.
2791   // The return_pc has been stored in the frame of the deoptee and
2792   // will replace the address of the deopt_handler in the call
2793   // to Deoptimization::fetch_unroll_info below.
2794   // We can't grab a free register here, because all registers may
2795   // contain live values, so let the RegisterSaver do the adjustment
2796   // of the return pc.
2797   const int return_pc_adjustment_no_exception = -HandlerImpl::size_deopt_handler();
2798 
2799   // Push the "unpack frame"
2800   // Save everything in sight.
2801   map = RegisterSaver::push_frame_reg_args_and_save_live_registers(masm,
2802                                                                    &amp;first_frame_size_in_bytes,
2803                                                                    /*generate_oop_map=*/ true,
2804                                                                    return_pc_adjustment_no_exception,
2805                                                                    RegisterSaver::return_pc_is_lr);
2806   assert(map != NULL, "OopMap must have been created");
2807 
2808   __ li(exec_mode_reg, Deoptimization::Unpack_deopt);
2809   // Save exec mode for unpack_frames.
2810   __ b(exec_mode_initialized);
2811 
2812   // --------------------------------------------------------------------------
2813   // Prolog for exception case
2814 
2815   // An exception is pending.
2816   // We have been called with a return (interpreter) or a jump (exception blob).
2817   //
2818   // - R3_ARG1: exception oop
2819   // - R4_ARG2: exception pc
2820 
2821   int exception_offset = __ pc() - start;
2822 
2823   BLOCK_COMMENT("Prolog for exception case");
2824 
2825   // The RegisterSaves doesn't need to adjust the return pc for this situation.
2826   const int return_pc_adjustment_exception = 0;
2827 
2828   // Push the "unpack frame".
2829   // Save everything in sight.
2830   assert(R4 == R4_ARG2, "exception pc must be in r4");
2831   RegisterSaver::push_frame_reg_args_and_save_live_registers(masm,
2832                                                              &amp;first_frame_size_in_bytes,
2833                                                              /*generate_oop_map=*/ false,
2834                                                              return_pc_adjustment_exception,
2835                                                              RegisterSaver::return_pc_is_r4);
2836 
2837   // Deopt during an exception. Save exec mode for unpack_frames.
2838   __ li(exec_mode_reg, Deoptimization::Unpack_exception);
2839 
2840   // Store exception oop and pc in thread (location known to GC).
2841   // This is needed since the call to "fetch_unroll_info()" may safepoint.
2842   __ std(R3_ARG1, in_bytes(JavaThread::exception_oop_offset()), R16_thread);
2843   __ std(R4_ARG2, in_bytes(JavaThread::exception_pc_offset()),  R16_thread);
2844 
2845   // fall through
2846 
2847   // --------------------------------------------------------------------------
2848   __ BIND(exec_mode_initialized);
2849 
2850   {
2851   const Register unroll_block_reg = R22_tmp2;
2852 
2853   // We need to set `last_Java_frame' because `fetch_unroll_info' will
2854   // call `last_Java_frame()'. The value of the pc in the frame is not
2855   // particularly important. It just needs to identify this blob.
2856   __ set_last_Java_frame(R1_SP, noreg);
2857 
2858   // With EscapeAnalysis turned on, this call may safepoint!
2859   __ call_VM_leaf(CAST_FROM_FN_PTR(address, Deoptimization::fetch_unroll_info), R16_thread);
2860   address calls_return_pc = __ last_calls_return_pc();
2861   // Set an oopmap for the call site that describes all our saved registers.
2862   oop_maps-&gt;add_gc_map(calls_return_pc - start, map);
2863 
2864   __ reset_last_Java_frame();
2865   // Save the return value.
2866   __ mr(unroll_block_reg, R3_RET);
2867 
2868   // Restore only the result registers that have been saved
2869   // by save_volatile_registers(...).
2870   RegisterSaver::restore_result_registers(masm, first_frame_size_in_bytes);
2871 
2872   // In excp_deopt_mode, restore and clear exception oop which we
2873   // stored in the thread during exception entry above. The exception
2874   // oop will be the return value of this stub.
2875   Label skip_restore_excp;
2876   __ cmpdi(CCR0, exec_mode_reg, Deoptimization::Unpack_exception);
2877   __ bne(CCR0, skip_restore_excp);
2878   __ ld(R3_RET, in_bytes(JavaThread::exception_oop_offset()), R16_thread);
2879   __ ld(R4_ARG2, in_bytes(JavaThread::exception_pc_offset()), R16_thread);
2880   __ li(R0, 0);
2881   __ std(R0, in_bytes(JavaThread::exception_pc_offset()),  R16_thread);
2882   __ std(R0, in_bytes(JavaThread::exception_oop_offset()), R16_thread);
2883   __ BIND(skip_restore_excp);
2884 
2885   // reload narrro_oop_base
2886   if (UseCompressedOops &amp;&amp; Universe::narrow_oop_base() != 0) {
2887     __ load_const_optimized(R30, Universe::narrow_oop_base());
2888   }
2889 
2890   __ pop_frame();
2891 
2892   // stack: (deoptee, optional i2c, caller of deoptee, ...).
2893 
2894   // pop the deoptee's frame
2895   __ pop_frame();
2896 
2897   // stack: (caller_of_deoptee, ...).
2898 
2899   // Loop through the `UnrollBlock' info and create interpreter frames.
2900   push_skeleton_frames(masm, true/*deopt*/,
2901                        unroll_block_reg,
2902                        R23_tmp3,
2903                        R24_tmp4,
2904                        R25_tmp5,
2905                        R26_tmp6,
2906                        R27_tmp7);
2907 
2908   // stack: (skeletal interpreter frame, ..., optional skeletal
2909   // interpreter frame, optional c2i, caller of deoptee, ...).
2910   }
2911 
2912   // push an `unpack_frame' taking care of float / int return values.
2913   __ push_frame(frame_size_in_bytes, R0/*tmp*/);
2914 
2915   // stack: (unpack frame, skeletal interpreter frame, ..., optional
2916   // skeletal interpreter frame, optional c2i, caller of deoptee,
2917   // ...).
2918 
2919   // Spill live volatile registers since we'll do a call.
2920   __ std( R3_RET, _abi_reg_args_spill(spill_ret),  R1_SP);
2921   __ stfd(F1_RET, _abi_reg_args_spill(spill_fret), R1_SP);
2922 
2923   // Let the unpacker layout information in the skeletal frames just
2924   // allocated.
2925   __ get_PC_trash_LR(R3_RET);
2926   __ set_last_Java_frame(/*sp*/R1_SP, /*pc*/R3_RET);
2927   // This is a call to a LEAF method, so no oop map is required.
2928   __ call_VM_leaf(CAST_FROM_FN_PTR(address, Deoptimization::unpack_frames),
2929                   R16_thread/*thread*/, exec_mode_reg/*exec_mode*/);
2930   __ reset_last_Java_frame();
2931 
2932   // Restore the volatiles saved above.
2933   __ ld( R3_RET, _abi_reg_args_spill(spill_ret),  R1_SP);
2934   __ lfd(F1_RET, _abi_reg_args_spill(spill_fret), R1_SP);
2935 
2936   // Pop the unpack frame.
2937   __ pop_frame();
2938   __ restore_LR_CR(R0);
2939 
2940   // stack: (top interpreter frame, ..., optional interpreter frame,
2941   // optional c2i, caller of deoptee, ...).
2942 
2943   // Initialize R14_state.
2944 #ifdef CC_INTERP
2945   __ ld(R14_state, 0, R1_SP);
2946   __ addi(R14_state, R14_state, -frame::interpreter_frame_cinterpreterstate_size_in_bytes());
2947   // Also inititialize R15_prev_state.
2948   __ restore_prev_state();
2949 #else
2950   __ restore_interpreter_state(R11_scratch1);
2951   __ load_const_optimized(R25_templateTableBase, (address)Interpreter::dispatch_table((TosState)0), R11_scratch1);
2952 #endif // CC_INTERP
2953 
2954 
2955   // Return to the interpreter entry point.
2956   __ blr();
2957   __ flush();
2958 #else // COMPILER2
2959   __ unimplemented("deopt blob needed only with compiler");
2960   int exception_offset = __ pc() - start;
2961 #endif // COMPILER2
2962 
2963   _deopt_blob = DeoptimizationBlob::create(&amp;buffer, oop_maps, 0, exception_offset, 0, first_frame_size_in_bytes / wordSize);
2964 }
2965 
2966 #ifdef COMPILER2
2967 void SharedRuntime::generate_uncommon_trap_blob() {
2968   // Allocate space for the code.
2969   ResourceMark rm;
2970   // Setup code generation tools.
2971   CodeBuffer buffer("uncommon_trap_blob", 2048, 1024);
2972   InterpreterMacroAssembler* masm = new InterpreterMacroAssembler(&amp;buffer);
2973   address start = __ pc();
2974 
2975   Register unroll_block_reg = R21_tmp1;
2976   Register klass_index_reg  = R22_tmp2;
2977   Register unc_trap_reg     = R23_tmp3;
2978 
2979   OopMapSet* oop_maps = new OopMapSet();
2980   int frame_size_in_bytes = frame::abi_reg_args_size;
2981   OopMap* map = new OopMap(frame_size_in_bytes / sizeof(jint), 0);
2982 
2983   // stack: (deoptee, optional i2c, caller_of_deoptee, ...).
2984 
2985   // Push a dummy `unpack_frame' and call
2986   // `Deoptimization::uncommon_trap' to pack the compiled frame into a
2987   // vframe array and return the `UnrollBlock' information.
2988 
2989   // Save LR to compiled frame.
2990   __ save_LR_CR(R11_scratch1);
2991 
2992   // Push an "uncommon_trap" frame.
2993   __ push_frame_reg_args(0, R11_scratch1);
2994 
2995   // stack: (unpack frame, deoptee, optional i2c, caller_of_deoptee, ...).
2996 
2997   // Set the `unpack_frame' as last_Java_frame.
2998   // `Deoptimization::uncommon_trap' expects it and considers its
2999   // sender frame as the deoptee frame.
3000   // Remember the offset of the instruction whose address will be
3001   // moved to R11_scratch1.
3002   address gc_map_pc = __ get_PC_trash_LR(R11_scratch1);
3003 
3004   __ set_last_Java_frame(/*sp*/R1_SP, /*pc*/R11_scratch1);
3005 
3006   __ mr(klass_index_reg, R3);
3007   __ call_VM_leaf(CAST_FROM_FN_PTR(address, Deoptimization::uncommon_trap),
3008                   R16_thread, klass_index_reg);
3009 
3010   // Set an oopmap for the call site.
3011   oop_maps-&gt;add_gc_map(gc_map_pc - start, map);
3012 
3013   __ reset_last_Java_frame();
3014 
3015   // Pop the `unpack frame'.
3016   __ pop_frame();
3017 
3018   // stack: (deoptee, optional i2c, caller_of_deoptee, ...).
3019 
3020   // Save the return value.
3021   __ mr(unroll_block_reg, R3_RET);
3022 
3023   // Pop the uncommon_trap frame.
3024   __ pop_frame();
3025 
3026   // stack: (caller_of_deoptee, ...).
3027 
3028   // Allocate new interpreter frame(s) and possibly a c2i adapter
3029   // frame.
3030   push_skeleton_frames(masm, false/*deopt*/,
3031                        unroll_block_reg,
3032                        R22_tmp2,
3033                        R23_tmp3,
3034                        R24_tmp4,
3035                        R25_tmp5,
3036                        R26_tmp6);
3037 
3038   // stack: (skeletal interpreter frame, ..., optional skeletal
3039   // interpreter frame, optional c2i, caller of deoptee, ...).
3040 
3041   // Push a dummy `unpack_frame' taking care of float return values.
3042   // Call `Deoptimization::unpack_frames' to layout information in the
3043   // interpreter frames just created.
3044 
3045   // Push a simple "unpack frame" here.
3046   __ push_frame_reg_args(0, R11_scratch1);
3047 
3048   // stack: (unpack frame, skeletal interpreter frame, ..., optional
3049   // skeletal interpreter frame, optional c2i, caller of deoptee,
3050   // ...).
3051 
3052   // Set the "unpack_frame" as last_Java_frame.
3053   __ get_PC_trash_LR(R11_scratch1);
3054   __ set_last_Java_frame(/*sp*/R1_SP, /*pc*/R11_scratch1);
3055 
3056   // Indicate it is the uncommon trap case.
3057   __ li(unc_trap_reg, Deoptimization::Unpack_uncommon_trap);
3058   // Let the unpacker layout information in the skeletal frames just
3059   // allocated.
3060   __ call_VM_leaf(CAST_FROM_FN_PTR(address, Deoptimization::unpack_frames),
3061                   R16_thread, unc_trap_reg);
3062 
3063   __ reset_last_Java_frame();
3064   // Pop the `unpack frame'.
3065   __ pop_frame();
3066   // Restore LR from top interpreter frame.
3067   __ restore_LR_CR(R11_scratch1);
3068 
3069   // stack: (top interpreter frame, ..., optional interpreter frame,
3070   // optional c2i, caller of deoptee, ...).
3071 
3072 #ifdef CC_INTERP
3073   // Initialize R14_state, ...
3074   __ ld(R11_scratch1, 0, R1_SP);
3075   __ addi(R14_state, R11_scratch1, -frame::interpreter_frame_cinterpreterstate_size_in_bytes());
3076   // also initialize R15_prev_state.
3077   __ restore_prev_state();
3078 #else
3079   __ restore_interpreter_state(R11_scratch1);
3080   __ load_const_optimized(R25_templateTableBase, (address)Interpreter::dispatch_table((TosState)0), R11_scratch1);
3081 #endif // CC_INTERP
3082 
3083   // Return to the interpreter entry point.
3084   __ blr();
3085 
3086   masm-&gt;flush();
3087 
3088   _uncommon_trap_blob = UncommonTrapBlob::create(&amp;buffer, oop_maps, frame_size_in_bytes/wordSize);
3089 }
3090 #endif // COMPILER2
3091 
3092 // Generate a special Compile2Runtime blob that saves all registers, and setup oopmap.
3093 SafepointBlob* SharedRuntime::generate_handler_blob(address call_ptr, int poll_type) {
3094   assert(StubRoutines::forward_exception_entry() != NULL,
3095          "must be generated before");
3096 
3097   ResourceMark rm;
3098   OopMapSet *oop_maps = new OopMapSet();
3099   OopMap* map;
3100 
3101   // Allocate space for the code. Setup code generation tools.
3102   CodeBuffer buffer("handler_blob", 2048, 1024);
3103   MacroAssembler* masm = new MacroAssembler(&amp;buffer);
3104 
3105   address start = __ pc();
3106   int frame_size_in_bytes = 0;
3107 
3108   RegisterSaver::ReturnPCLocation return_pc_location;
3109   bool cause_return = (poll_type == POLL_AT_RETURN);
3110   if (cause_return) {
3111     // Nothing to do here. The frame has already been popped in MachEpilogNode.
3112     // Register LR already contains the return pc.
3113     return_pc_location = RegisterSaver::return_pc_is_lr;
3114   } else {
3115     // Use thread()-&gt;saved_exception_pc() as return pc.
3116     return_pc_location = RegisterSaver::return_pc_is_thread_saved_exception_pc;
3117   }
3118 
3119   // Save registers, fpu state, and flags.
3120   map = RegisterSaver::push_frame_reg_args_and_save_live_registers(masm,
3121                                                                    &amp;frame_size_in_bytes,
3122                                                                    /*generate_oop_map=*/ true,
3123                                                                    /*return_pc_adjustment=*/0,
3124                                                                    return_pc_location);
3125 
3126   // The following is basically a call_VM. However, we need the precise
3127   // address of the call in order to generate an oopmap. Hence, we do all the
3128   // work outselves.
3129   __ set_last_Java_frame(/*sp=*/R1_SP, /*pc=*/noreg);
3130 
3131   // The return address must always be correct so that the frame constructor
3132   // never sees an invalid pc.
3133 
3134   // Do the call
3135   __ call_VM_leaf(call_ptr, R16_thread);
3136   address calls_return_pc = __ last_calls_return_pc();
3137 
3138   // Set an oopmap for the call site. This oopmap will map all
3139   // oop-registers and debug-info registers as callee-saved. This
3140   // will allow deoptimization at this safepoint to find all possible
3141   // debug-info recordings, as well as let GC find all oops.
3142   oop_maps-&gt;add_gc_map(calls_return_pc - start, map);
3143 
3144   Label noException;
3145 
3146   // Clear the last Java frame.
3147   __ reset_last_Java_frame();
3148 
3149   BLOCK_COMMENT("  Check pending exception.");
3150   const Register pending_exception = R0;
3151   __ ld(pending_exception, thread_(pending_exception));
3152   __ cmpdi(CCR0, pending_exception, 0);
3153   __ beq(CCR0, noException);
3154 
3155   // Exception pending
3156   RegisterSaver::restore_live_registers_and_pop_frame(masm,
3157                                                       frame_size_in_bytes,
3158                                                       /*restore_ctr=*/true);
3159 
3160   BLOCK_COMMENT("  Jump to forward_exception_entry.");
3161   // Jump to forward_exception_entry, with the issuing PC in LR
3162   // so it looks like the original nmethod called forward_exception_entry.
3163   __ b64_patchable(StubRoutines::forward_exception_entry(), relocInfo::runtime_call_type);
3164 
3165   // No exception case.
3166   __ BIND(noException);
3167 
3168 
3169   // Normal exit, restore registers and exit.
3170   RegisterSaver::restore_live_registers_and_pop_frame(masm,
3171                                                       frame_size_in_bytes,
3172                                                       /*restore_ctr=*/true);
3173 
3174   __ blr();
3175 
3176   // Make sure all code is generated
3177   masm-&gt;flush();
3178 
3179   // Fill-out other meta info
3180   // CodeBlob frame size is in words.
3181   return SafepointBlob::create(&amp;buffer, oop_maps, frame_size_in_bytes / wordSize);
3182 }
3183 
3184 // generate_resolve_blob - call resolution (static/virtual/opt-virtual/ic-miss)
3185 //
3186 // Generate a stub that calls into the vm to find out the proper destination
3187 // of a java call. All the argument registers are live at this point
3188 // but since this is generic code we don't know what they are and the caller
3189 // must do any gc of the args.
3190 //
3191 RuntimeStub* SharedRuntime::generate_resolve_blob(address destination, const char* name) {
3192 
3193   // allocate space for the code
3194   ResourceMark rm;
3195 
3196   CodeBuffer buffer(name, 1000, 512);
3197   MacroAssembler* masm = new MacroAssembler(&amp;buffer);
3198 
3199   int frame_size_in_bytes;
3200 
3201   OopMapSet *oop_maps = new OopMapSet();
3202   OopMap* map = NULL;
3203 
3204   address start = __ pc();
3205 
3206   map = RegisterSaver::push_frame_reg_args_and_save_live_registers(masm,
3207                                                                    &amp;frame_size_in_bytes,
3208                                                                    /*generate_oop_map*/ true,
3209                                                                    /*return_pc_adjustment*/ 0,
3210                                                                    RegisterSaver::return_pc_is_lr);
3211 
3212   // Use noreg as last_Java_pc, the return pc will be reconstructed
3213   // from the physical frame.
3214   __ set_last_Java_frame(/*sp*/R1_SP, noreg);
3215 
3216   int frame_complete = __ offset();
3217 
3218   // Pass R19_method as 2nd (optional) argument, used by
3219   // counter_overflow_stub.
3220   __ call_VM_leaf(destination, R16_thread, R19_method);
3221   address calls_return_pc = __ last_calls_return_pc();
3222   // Set an oopmap for the call site.
3223   // We need this not only for callee-saved registers, but also for volatile
3224   // registers that the compiler might be keeping live across a safepoint.
3225   // Create the oopmap for the call's return pc.
3226   oop_maps-&gt;add_gc_map(calls_return_pc - start, map);
3227 
3228   // R3_RET contains the address we are going to jump to assuming no exception got installed.
3229 
3230   // clear last_Java_sp
3231   __ reset_last_Java_frame();
3232 
3233   // Check for pending exceptions.
3234   BLOCK_COMMENT("Check for pending exceptions.");
3235   Label pending;
3236   __ ld(R11_scratch1, thread_(pending_exception));
3237   __ cmpdi(CCR0, R11_scratch1, 0);
3238   __ bne(CCR0, pending);
3239 
3240   __ mtctr(R3_RET); // Ctr will not be touched by restore_live_registers_and_pop_frame.
3241 
3242   RegisterSaver::restore_live_registers_and_pop_frame(masm, frame_size_in_bytes, /*restore_ctr*/ false);
3243 
3244   // Get the returned method.
3245   __ get_vm_result_2(R19_method);
3246 
3247   __ bctr();
3248 
3249 
3250   // Pending exception after the safepoint.
3251   __ BIND(pending);
3252 
3253   RegisterSaver::restore_live_registers_and_pop_frame(masm, frame_size_in_bytes, /*restore_ctr*/ true);
3254 
3255   // exception pending =&gt; remove activation and forward to exception handler
3256 
3257   __ li(R11_scratch1, 0);
3258   __ ld(R3_ARG1, thread_(pending_exception));
3259   __ std(R11_scratch1, in_bytes(JavaThread::vm_result_offset()), R16_thread);
3260   __ b64_patchable(StubRoutines::forward_exception_entry(), relocInfo::runtime_call_type);
3261 
3262   // -------------
3263   // Make sure all code is generated.
3264   masm-&gt;flush();
3265 
3266   // return the blob
3267   // frame_size_words or bytes??
3268   return RuntimeStub::new_runtime_stub(name, &amp;buffer, frame_complete, frame_size_in_bytes/wordSize,
3269                                        oop_maps, true);
<a name="2" id="anc2"></a>

















































































































































































































































3270 }
<a name="3" id="anc3"></a><b style="font-size: large; color: red">--- EOF ---</b>















































































</pre><form name="eof"><input name="value" value="3" type="hidden" /></form></body></html>
