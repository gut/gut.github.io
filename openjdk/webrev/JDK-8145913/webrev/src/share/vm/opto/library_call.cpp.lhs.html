<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta charset="utf-8">
<meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

    <script type="text/javascript" src="../../../../ancnav.js"></script>
    </head>
    <body id="SUNWwebrev" onkeypress="keypress(event);">
    <a name="0"></a>
    <pre>rev 8346 : Adaptation due to JDK8's CCallingConventionRequiresIntsAsLongs</pre><hr></hr>
<pre>
   1 /*
   2  * Copyright (c) 1999, 2013, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include "precompiled.hpp"
  26 #include "classfile/systemDictionary.hpp"
  27 #include "classfile/vmSymbols.hpp"
  28 #include "compiler/compileBroker.hpp"
  29 #include "compiler/compileLog.hpp"
  30 #include "oops/objArrayKlass.hpp"
  31 #include "opto/addnode.hpp"
  32 #include "opto/callGenerator.hpp"
  33 #include "opto/cfgnode.hpp"
  34 #include "opto/connode.hpp"
  35 #include "opto/idealKit.hpp"
  36 #include "opto/mathexactnode.hpp"
  37 #include "opto/mulnode.hpp"
  38 #include "opto/parse.hpp"
  39 #include "opto/runtime.hpp"
  40 #include "opto/subnode.hpp"
  41 #include "prims/nativeLookup.hpp"
  42 #include "runtime/sharedRuntime.hpp"
  43 #include "trace/traceMacros.hpp"
  44 
  45 class LibraryIntrinsic : public InlineCallGenerator {
  46   // Extend the set of intrinsics known to the runtime:
  47  public:
  48  private:
  49   bool             _is_virtual;
  50   bool             _does_virtual_dispatch;
  51   int8_t           _predicates_count;  // Intrinsic is predicated by several conditions
  52   int8_t           _last_predicate; // Last generated predicate
  53   vmIntrinsics::ID _intrinsic_id;
  54 
  55  public:
  56   LibraryIntrinsic(ciMethod* m, bool is_virtual, int predicates_count, bool does_virtual_dispatch, vmIntrinsics::ID id)
  57     : InlineCallGenerator(m),
  58       _is_virtual(is_virtual),
  59       _does_virtual_dispatch(does_virtual_dispatch),
  60       _predicates_count((int8_t)predicates_count),
  61       _last_predicate((int8_t)-1),
  62       _intrinsic_id(id)
  63   {
  64   }
  65   virtual bool is_intrinsic() const { return true; }
  66   virtual bool is_virtual()   const { return _is_virtual; }
  67   virtual bool is_predicated() const { return _predicates_count &gt; 0; }
  68   virtual int  predicates_count() const { return _predicates_count; }
  69   virtual bool does_virtual_dispatch()   const { return _does_virtual_dispatch; }
  70   virtual JVMState* generate(JVMState* jvms);
  71   virtual Node* generate_predicate(JVMState* jvms, int predicate);
  72   vmIntrinsics::ID intrinsic_id() const { return _intrinsic_id; }
  73 };
  74 
  75 
  76 // Local helper class for LibraryIntrinsic:
  77 class LibraryCallKit : public GraphKit {
  78  private:
  79   LibraryIntrinsic* _intrinsic;     // the library intrinsic being called
  80   Node*             _result;        // the result node, if any
  81   int               _reexecute_sp;  // the stack pointer when bytecode needs to be reexecuted
  82 
  83   const TypeOopPtr* sharpen_unsafe_type(Compile::AliasType* alias_type, const TypePtr *adr_type, bool is_native_ptr = false);
  84 
  85  public:
  86   LibraryCallKit(JVMState* jvms, LibraryIntrinsic* intrinsic)
  87     : GraphKit(jvms),
  88       _intrinsic(intrinsic),
  89       _result(NULL)
  90   {
  91     // Check if this is a root compile.  In that case we don't have a caller.
  92     if (!jvms-&gt;has_method()) {
  93       _reexecute_sp = sp();
  94     } else {
  95       // Find out how many arguments the interpreter needs when deoptimizing
  96       // and save the stack pointer value so it can used by uncommon_trap.
  97       // We find the argument count by looking at the declared signature.
  98       bool ignored_will_link;
  99       ciSignature* declared_signature = NULL;
 100       ciMethod* ignored_callee = caller()-&gt;get_method_at_bci(bci(), ignored_will_link, &amp;declared_signature);
 101       const int nargs = declared_signature-&gt;arg_size_for_bc(caller()-&gt;java_code_at_bci(bci()));
 102       _reexecute_sp = sp() + nargs;  // "push" arguments back on stack
 103     }
 104   }
 105 
 106   virtual LibraryCallKit* is_LibraryCallKit() const { return (LibraryCallKit*)this; }
 107 
 108   ciMethod*         caller()    const    { return jvms()-&gt;method(); }
 109   int               bci()       const    { return jvms()-&gt;bci(); }
 110   LibraryIntrinsic* intrinsic() const    { return _intrinsic; }
 111   vmIntrinsics::ID  intrinsic_id() const { return _intrinsic-&gt;intrinsic_id(); }
 112   ciMethod*         callee()    const    { return _intrinsic-&gt;method(); }
 113 
 114   bool  try_to_inline(int predicate);
 115   Node* try_to_predicate(int predicate);
 116 
 117   void push_result() {
 118     // Push the result onto the stack.
 119     if (!stopped() &amp;&amp; result() != NULL) {
 120       BasicType bt = result()-&gt;bottom_type()-&gt;basic_type();
 121       push_node(bt, result());
 122     }
 123   }
 124 
 125  private:
 126   void fatal_unexpected_iid(vmIntrinsics::ID iid) {
 127     fatal(err_msg_res("unexpected intrinsic %d: %s", iid, vmIntrinsics::name_at(iid)));
 128   }
 129 
 130   void  set_result(Node* n) { assert(_result == NULL, "only set once"); _result = n; }
 131   void  set_result(RegionNode* region, PhiNode* value);
 132   Node*     result() { return _result; }
 133 
 134   virtual int reexecute_sp() { return _reexecute_sp; }
 135 
 136   // Helper functions to inline natives
 137   Node* generate_guard(Node* test, RegionNode* region, float true_prob);
 138   Node* generate_slow_guard(Node* test, RegionNode* region);
 139   Node* generate_fair_guard(Node* test, RegionNode* region);
 140   Node* generate_negative_guard(Node* index, RegionNode* region,
 141                                 // resulting CastII of index:
 142                                 Node* *pos_index = NULL);
 143   Node* generate_nonpositive_guard(Node* index, bool never_negative,
 144                                    // resulting CastII of index:
 145                                    Node* *pos_index = NULL);
 146   Node* generate_limit_guard(Node* offset, Node* subseq_length,
 147                              Node* array_length,
 148                              RegionNode* region);
 149   Node* generate_current_thread(Node* &amp;tls_output);
 150   address basictype2arraycopy(BasicType t, Node *src_offset, Node *dest_offset,
 151                               bool disjoint_bases, const char* &amp;name, bool dest_uninitialized);
 152   Node* load_mirror_from_klass(Node* klass);
 153   Node* load_klass_from_mirror_common(Node* mirror, bool never_see_null,
 154                                       RegionNode* region, int null_path,
 155                                       int offset);
 156   Node* load_klass_from_mirror(Node* mirror, bool never_see_null,
 157                                RegionNode* region, int null_path) {
 158     int offset = java_lang_Class::klass_offset_in_bytes();
 159     return load_klass_from_mirror_common(mirror, never_see_null,
 160                                          region, null_path,
 161                                          offset);
 162   }
 163   Node* load_array_klass_from_mirror(Node* mirror, bool never_see_null,
 164                                      RegionNode* region, int null_path) {
 165     int offset = java_lang_Class::array_klass_offset_in_bytes();
 166     return load_klass_from_mirror_common(mirror, never_see_null,
 167                                          region, null_path,
 168                                          offset);
 169   }
 170   Node* generate_access_flags_guard(Node* kls,
 171                                     int modifier_mask, int modifier_bits,
 172                                     RegionNode* region);
 173   Node* generate_interface_guard(Node* kls, RegionNode* region);
 174   Node* generate_array_guard(Node* kls, RegionNode* region) {
 175     return generate_array_guard_common(kls, region, false, false);
 176   }
 177   Node* generate_non_array_guard(Node* kls, RegionNode* region) {
 178     return generate_array_guard_common(kls, region, false, true);
 179   }
 180   Node* generate_objArray_guard(Node* kls, RegionNode* region) {
 181     return generate_array_guard_common(kls, region, true, false);
 182   }
 183   Node* generate_non_objArray_guard(Node* kls, RegionNode* region) {
 184     return generate_array_guard_common(kls, region, true, true);
 185   }
 186   Node* generate_array_guard_common(Node* kls, RegionNode* region,
 187                                     bool obj_array, bool not_array);
 188   Node* generate_virtual_guard(Node* obj_klass, RegionNode* slow_region);
 189   CallJavaNode* generate_method_call(vmIntrinsics::ID method_id,
 190                                      bool is_virtual = false, bool is_static = false);
 191   CallJavaNode* generate_method_call_static(vmIntrinsics::ID method_id) {
 192     return generate_method_call(method_id, false, true);
 193   }
 194   CallJavaNode* generate_method_call_virtual(vmIntrinsics::ID method_id) {
 195     return generate_method_call(method_id, true, false);
 196   }
 197   Node * load_field_from_object(Node * fromObj, const char * fieldName, const char * fieldTypeString, bool is_exact, bool is_static);
 198 
 199   Node* make_string_method_node(int opcode, Node* str1_start, Node* cnt1, Node* str2_start, Node* cnt2);
 200   Node* make_string_method_node(int opcode, Node* str1, Node* str2);
 201   bool inline_string_compareTo();
 202   bool inline_string_indexOf();
 203   Node* string_indexOf(Node* string_object, ciTypeArray* target_array, jint offset, jint cache_i, jint md2_i);
 204   bool inline_string_equals();
 205   Node* round_double_node(Node* n);
 206   bool runtime_math(const TypeFunc* call_type, address funcAddr, const char* funcName);
 207   bool inline_math_native(vmIntrinsics::ID id);
 208   bool inline_trig(vmIntrinsics::ID id);
 209   bool inline_math(vmIntrinsics::ID id);
 210   template &lt;typename OverflowOp&gt;
 211   bool inline_math_overflow(Node* arg1, Node* arg2);
 212   void inline_math_mathExact(Node* math, Node* test);
 213   bool inline_math_addExactI(bool is_increment);
 214   bool inline_math_addExactL(bool is_increment);
 215   bool inline_math_multiplyExactI();
 216   bool inline_math_multiplyExactL();
 217   bool inline_math_negateExactI();
 218   bool inline_math_negateExactL();
 219   bool inline_math_subtractExactI(bool is_decrement);
 220   bool inline_math_subtractExactL(bool is_decrement);
 221   bool inline_exp();
 222   bool inline_pow();
 223   Node* finish_pow_exp(Node* result, Node* x, Node* y, const TypeFunc* call_type, address funcAddr, const char* funcName);
 224   bool inline_min_max(vmIntrinsics::ID id);
 225   Node* generate_min_max(vmIntrinsics::ID id, Node* x, Node* y);
 226   // This returns Type::AnyPtr, RawPtr, or OopPtr.
 227   int classify_unsafe_addr(Node* &amp;base, Node* &amp;offset);
 228   Node* make_unsafe_address(Node* base, Node* offset);
 229   // Helper for inline_unsafe_access.
 230   // Generates the guards that check whether the result of
 231   // Unsafe.getObject should be recorded in an SATB log buffer.
 232   void insert_pre_barrier(Node* base_oop, Node* offset, Node* pre_val, bool need_mem_bar);
 233   bool inline_unsafe_access(bool is_native_ptr, bool is_store, BasicType type, bool is_volatile, bool is_unaligned);
 234   bool inline_unsafe_prefetch(bool is_native_ptr, bool is_store, bool is_static);
 235   static bool klass_needs_init_guard(Node* kls);
 236   bool inline_unsafe_allocate();
 237   bool inline_unsafe_copyMemory();
 238   bool inline_native_currentThread();
 239 #ifdef TRACE_HAVE_INTRINSICS
 240   bool inline_native_classID();
 241   bool inline_native_threadID();
 242 #endif
 243   bool inline_native_time_funcs(address method, const char* funcName);
 244   bool inline_native_isInterrupted();
 245   bool inline_native_Class_query(vmIntrinsics::ID id);
 246   bool inline_native_subtype_check();
 247 
 248   bool inline_native_newArray();
 249   bool inline_native_getLength();
 250   bool inline_array_copyOf(bool is_copyOfRange);
 251   bool inline_array_equals();
 252   void copy_to_clone(Node* obj, Node* alloc_obj, Node* obj_size, bool is_array, bool card_mark);
 253   bool inline_native_clone(bool is_virtual);
 254   bool inline_native_Reflection_getCallerClass();
 255   // Helper function for inlining native object hash method
 256   bool inline_native_hashcode(bool is_virtual, bool is_static);
 257   bool inline_native_getClass();
 258 
 259   // Helper functions for inlining arraycopy
 260   bool inline_arraycopy();
 261   void generate_arraycopy(const TypePtr* adr_type,
 262                           BasicType basic_elem_type,
 263                           Node* src,  Node* src_offset,
 264                           Node* dest, Node* dest_offset,
 265                           Node* copy_length,
 266                           bool disjoint_bases = false,
 267                           bool length_never_negative = false,
 268                           RegionNode* slow_region = NULL);
 269   AllocateArrayNode* tightly_coupled_allocation(Node* ptr,
 270                                                 RegionNode* slow_region);
 271   void generate_clear_array(const TypePtr* adr_type,
 272                             Node* dest,
 273                             BasicType basic_elem_type,
 274                             Node* slice_off,
 275                             Node* slice_len,
 276                             Node* slice_end);
 277   bool generate_block_arraycopy(const TypePtr* adr_type,
 278                                 BasicType basic_elem_type,
 279                                 AllocateNode* alloc,
 280                                 Node* src,  Node* src_offset,
 281                                 Node* dest, Node* dest_offset,
 282                                 Node* dest_size, bool dest_uninitialized);
 283   void generate_slow_arraycopy(const TypePtr* adr_type,
 284                                Node* src,  Node* src_offset,
 285                                Node* dest, Node* dest_offset,
 286                                Node* copy_length, bool dest_uninitialized);
 287   Node* generate_checkcast_arraycopy(const TypePtr* adr_type,
 288                                      Node* dest_elem_klass,
 289                                      Node* src,  Node* src_offset,
 290                                      Node* dest, Node* dest_offset,
 291                                      Node* copy_length, bool dest_uninitialized);
 292   Node* generate_generic_arraycopy(const TypePtr* adr_type,
 293                                    Node* src,  Node* src_offset,
 294                                    Node* dest, Node* dest_offset,
 295                                    Node* copy_length, bool dest_uninitialized);
 296   void generate_unchecked_arraycopy(const TypePtr* adr_type,
 297                                     BasicType basic_elem_type,
 298                                     bool disjoint_bases,
 299                                     Node* src,  Node* src_offset,
 300                                     Node* dest, Node* dest_offset,
 301                                     Node* copy_length, bool dest_uninitialized);
 302   typedef enum { LS_xadd, LS_xchg, LS_cmpxchg } LoadStoreKind;
 303   bool inline_unsafe_load_store(BasicType type,  LoadStoreKind kind);
 304   bool inline_unsafe_ordered_store(BasicType type);
 305   bool inline_unsafe_fence(vmIntrinsics::ID id);
 306   bool inline_fp_conversions(vmIntrinsics::ID id);
 307   bool inline_number_methods(vmIntrinsics::ID id);
 308   bool inline_reference_get();
 309   bool inline_aescrypt_Block(vmIntrinsics::ID id);
 310   bool inline_cipherBlockChaining_AESCrypt(vmIntrinsics::ID id);
 311   Node* inline_cipherBlockChaining_AESCrypt_predicate(bool decrypting);
 312   Node* get_key_start_from_aescrypt_object(Node* aescrypt_object);
 313   Node* get_original_key_start_from_aescrypt_object(Node* aescrypt_object);
 314   bool inline_sha_implCompress(vmIntrinsics::ID id);
 315   bool inline_digestBase_implCompressMB(int predicate);
 316   bool inline_sha_implCompressMB(Node* digestBaseObj, ciInstanceKlass* instklass_SHA,
 317                                  bool long_state, address stubAddr, const char *stubName,
 318                                  Node* src_start, Node* ofs, Node* limit);
 319   Node* get_state_from_sha_object(Node *sha_object);
 320   Node* get_state_from_sha5_object(Node *sha_object);
 321   Node* inline_digestBase_implCompressMB_predicate(int predicate);
 322   bool inline_encodeISOArray();
 323   bool inline_updateCRC32();
 324   bool inline_updateBytesCRC32();
 325   bool inline_updateByteBufferCRC32();
 326   bool inline_multiplyToLen();
 327   bool inline_squareToLen();
 328   bool inline_mulAdd();
 329   bool inline_montgomeryMultiply();
 330   bool inline_montgomerySquare();
 331 
 332   bool inline_profileBoolean();
 333 };
 334 
 335 
 336 //---------------------------make_vm_intrinsic----------------------------
 337 CallGenerator* Compile::make_vm_intrinsic(ciMethod* m, bool is_virtual) {
 338   vmIntrinsics::ID id = m-&gt;intrinsic_id();
 339   assert(id != vmIntrinsics::_none, "must be a VM intrinsic");
 340 
 341   ccstr disable_intr = NULL;
 342 
 343   if ((DisableIntrinsic[0] != '\0'
 344        &amp;&amp; strstr(DisableIntrinsic, vmIntrinsics::name_at(id)) != NULL) ||
 345       (method_has_option_value("DisableIntrinsic", disable_intr)
 346        &amp;&amp; strstr(disable_intr, vmIntrinsics::name_at(id)) != NULL)) {
 347     // disabled by a user request on the command line:
 348     // example: -XX:DisableIntrinsic=_hashCode,_getClass
 349     return NULL;
 350   }
 351 
 352   if (!m-&gt;is_loaded()) {
 353     // do not attempt to inline unloaded methods
 354     return NULL;
 355   }
 356 
 357   // Only a few intrinsics implement a virtual dispatch.
 358   // They are expensive calls which are also frequently overridden.
 359   if (is_virtual) {
 360     switch (id) {
 361     case vmIntrinsics::_hashCode:
 362     case vmIntrinsics::_clone:
 363       // OK, Object.hashCode and Object.clone intrinsics come in both flavors
 364       break;
 365     default:
 366       return NULL;
 367     }
 368   }
 369 
 370   // -XX:-InlineNatives disables nearly all intrinsics:
 371   if (!InlineNatives) {
 372     switch (id) {
 373     case vmIntrinsics::_indexOf:
 374     case vmIntrinsics::_compareTo:
 375     case vmIntrinsics::_equals:
 376     case vmIntrinsics::_equalsC:
 377     case vmIntrinsics::_getAndAddInt:
 378     case vmIntrinsics::_getAndAddLong:
 379     case vmIntrinsics::_getAndSetInt:
 380     case vmIntrinsics::_getAndSetLong:
 381     case vmIntrinsics::_getAndSetObject:
 382     case vmIntrinsics::_loadFence:
 383     case vmIntrinsics::_storeFence:
 384     case vmIntrinsics::_fullFence:
 385       break;  // InlineNatives does not control String.compareTo
 386     case vmIntrinsics::_Reference_get:
 387       break;  // InlineNatives does not control Reference.get
 388     default:
 389       return NULL;
 390     }
 391   }
 392 
 393   int predicates = 0;
 394   bool does_virtual_dispatch = false;
 395 
 396   switch (id) {
 397   case vmIntrinsics::_compareTo:
 398     if (!SpecialStringCompareTo)  return NULL;
 399     if (!Matcher::match_rule_supported(Op_StrComp))  return NULL;
 400     break;
 401   case vmIntrinsics::_indexOf:
 402     if (!SpecialStringIndexOf)  return NULL;
 403     break;
 404   case vmIntrinsics::_equals:
 405     if (!SpecialStringEquals)  return NULL;
 406     if (!Matcher::match_rule_supported(Op_StrEquals))  return NULL;
 407     break;
 408   case vmIntrinsics::_equalsC:
 409     if (!SpecialArraysEquals)  return NULL;
 410     if (!Matcher::match_rule_supported(Op_AryEq))  return NULL;
 411     break;
 412   case vmIntrinsics::_arraycopy:
 413     if (!InlineArrayCopy)  return NULL;
 414     break;
 415   case vmIntrinsics::_copyMemory:
 416     if (StubRoutines::unsafe_arraycopy() == NULL)  return NULL;
 417     if (!InlineArrayCopy)  return NULL;
 418     break;
 419   case vmIntrinsics::_hashCode:
 420     if (!InlineObjectHash)  return NULL;
 421     does_virtual_dispatch = true;
 422     break;
 423   case vmIntrinsics::_clone:
 424     does_virtual_dispatch = true;
 425   case vmIntrinsics::_copyOf:
 426   case vmIntrinsics::_copyOfRange:
 427     if (!InlineObjectCopy)  return NULL;
 428     // These also use the arraycopy intrinsic mechanism:
 429     if (!InlineArrayCopy)  return NULL;
 430     break;
 431   case vmIntrinsics::_encodeISOArray:
 432     if (!SpecialEncodeISOArray)  return NULL;
 433     if (!Matcher::match_rule_supported(Op_EncodeISOArray))  return NULL;
 434     break;
 435   case vmIntrinsics::_checkIndex:
 436     // We do not intrinsify this.  The optimizer does fine with it.
 437     return NULL;
 438 
 439   case vmIntrinsics::_getCallerClass:
 440     if (!UseNewReflection)  return NULL;
 441     if (!InlineReflectionGetCallerClass)  return NULL;
 442     if (SystemDictionary::reflect_CallerSensitive_klass() == NULL)  return NULL;
 443     break;
 444 
 445   case vmIntrinsics::_bitCount_i:
 446     if (!Matcher::match_rule_supported(Op_PopCountI)) return NULL;
 447     break;
 448 
 449   case vmIntrinsics::_bitCount_l:
 450     if (!Matcher::match_rule_supported(Op_PopCountL)) return NULL;
 451     break;
 452 
 453   case vmIntrinsics::_numberOfLeadingZeros_i:
 454     if (!Matcher::match_rule_supported(Op_CountLeadingZerosI)) return NULL;
 455     break;
 456 
 457   case vmIntrinsics::_numberOfLeadingZeros_l:
 458     if (!Matcher::match_rule_supported(Op_CountLeadingZerosL)) return NULL;
 459     break;
 460 
 461   case vmIntrinsics::_numberOfTrailingZeros_i:
 462     if (!Matcher::match_rule_supported(Op_CountTrailingZerosI)) return NULL;
 463     break;
 464 
 465   case vmIntrinsics::_numberOfTrailingZeros_l:
 466     if (!Matcher::match_rule_supported(Op_CountTrailingZerosL)) return NULL;
 467     break;
 468 
 469   case vmIntrinsics::_reverseBytes_c:
 470     if (!Matcher::match_rule_supported(Op_ReverseBytesUS)) return NULL;
 471     break;
 472   case vmIntrinsics::_reverseBytes_s:
 473     if (!Matcher::match_rule_supported(Op_ReverseBytesS))  return NULL;
 474     break;
 475   case vmIntrinsics::_reverseBytes_i:
 476     if (!Matcher::match_rule_supported(Op_ReverseBytesI))  return NULL;
 477     break;
 478   case vmIntrinsics::_reverseBytes_l:
 479     if (!Matcher::match_rule_supported(Op_ReverseBytesL))  return NULL;
 480     break;
 481 
 482   case vmIntrinsics::_Reference_get:
 483     // Use the intrinsic version of Reference.get() so that the value in
 484     // the referent field can be registered by the G1 pre-barrier code.
 485     // Also add memory barrier to prevent commoning reads from this field
 486     // across safepoint since GC can change it value.
 487     break;
 488 
 489   case vmIntrinsics::_compareAndSwapObject:
 490 #ifdef _LP64
 491     if (!UseCompressedOops &amp;&amp; !Matcher::match_rule_supported(Op_CompareAndSwapP)) return NULL;
 492 #endif
 493     break;
 494 
 495   case vmIntrinsics::_compareAndSwapLong:
 496     if (!Matcher::match_rule_supported(Op_CompareAndSwapL)) return NULL;
 497     break;
 498 
 499   case vmIntrinsics::_getAndAddInt:
 500     if (!Matcher::match_rule_supported(Op_GetAndAddI)) return NULL;
 501     break;
 502 
 503   case vmIntrinsics::_getAndAddLong:
 504     if (!Matcher::match_rule_supported(Op_GetAndAddL)) return NULL;
 505     break;
 506 
 507   case vmIntrinsics::_getAndSetInt:
 508     if (!Matcher::match_rule_supported(Op_GetAndSetI)) return NULL;
 509     break;
 510 
 511   case vmIntrinsics::_getAndSetLong:
 512     if (!Matcher::match_rule_supported(Op_GetAndSetL)) return NULL;
 513     break;
 514 
 515   case vmIntrinsics::_getAndSetObject:
 516 #ifdef _LP64
 517     if (!UseCompressedOops &amp;&amp; !Matcher::match_rule_supported(Op_GetAndSetP)) return NULL;
 518     if (UseCompressedOops &amp;&amp; !Matcher::match_rule_supported(Op_GetAndSetN)) return NULL;
 519     break;
 520 #else
 521     if (!Matcher::match_rule_supported(Op_GetAndSetP)) return NULL;
 522     break;
 523 #endif
 524 
 525   case vmIntrinsics::_aescrypt_encryptBlock:
 526   case vmIntrinsics::_aescrypt_decryptBlock:
 527     if (!UseAESIntrinsics) return NULL;
 528     break;
 529 
 530   case vmIntrinsics::_multiplyToLen:
 531     if (!UseMultiplyToLenIntrinsic) return NULL;
 532     break;
 533 
 534   case vmIntrinsics::_squareToLen:
 535     if (!UseSquareToLenIntrinsic) return NULL;
 536     break;
 537 
 538   case vmIntrinsics::_mulAdd:
 539     if (!UseMulAddIntrinsic) return NULL;
 540     break;
 541 
 542   case vmIntrinsics::_montgomeryMultiply:
 543      if (!UseMontgomeryMultiplyIntrinsic) return NULL;
 544     break;
 545   case vmIntrinsics::_montgomerySquare:
 546      if (!UseMontgomerySquareIntrinsic) return NULL;
 547     break;
 548 
 549   case vmIntrinsics::_cipherBlockChaining_encryptAESCrypt:
 550   case vmIntrinsics::_cipherBlockChaining_decryptAESCrypt:
 551     if (!UseAESIntrinsics) return NULL;
 552     // these two require the predicated logic
 553     predicates = 1;
 554     break;
 555 
 556   case vmIntrinsics::_sha_implCompress:
 557     if (!UseSHA1Intrinsics) return NULL;
 558     break;
 559 
 560   case vmIntrinsics::_sha2_implCompress:
 561     if (!UseSHA256Intrinsics) return NULL;
 562     break;
 563 
 564   case vmIntrinsics::_sha5_implCompress:
 565     if (!UseSHA512Intrinsics) return NULL;
 566     break;
 567 
 568   case vmIntrinsics::_digestBase_implCompressMB:
 569     if (!(UseSHA1Intrinsics || UseSHA256Intrinsics || UseSHA512Intrinsics)) return NULL;
 570     predicates = 3;
 571     break;
 572 
 573   case vmIntrinsics::_updateCRC32:
 574   case vmIntrinsics::_updateBytesCRC32:
 575   case vmIntrinsics::_updateByteBufferCRC32:
 576     if (!UseCRC32Intrinsics) return NULL;
 577     break;
 578 
 579   case vmIntrinsics::_incrementExactI:
 580   case vmIntrinsics::_addExactI:
 581     if (!Matcher::match_rule_supported(Op_OverflowAddI) || !UseMathExactIntrinsics) return NULL;
 582     break;
 583   case vmIntrinsics::_incrementExactL:
 584   case vmIntrinsics::_addExactL:
 585     if (!Matcher::match_rule_supported(Op_OverflowAddL) || !UseMathExactIntrinsics) return NULL;
 586     break;
 587   case vmIntrinsics::_decrementExactI:
 588   case vmIntrinsics::_subtractExactI:
 589     if (!Matcher::match_rule_supported(Op_OverflowSubI) || !UseMathExactIntrinsics) return NULL;
 590     break;
 591   case vmIntrinsics::_decrementExactL:
 592   case vmIntrinsics::_subtractExactL:
 593     if (!Matcher::match_rule_supported(Op_OverflowSubL) || !UseMathExactIntrinsics) return NULL;
 594     break;
 595   case vmIntrinsics::_negateExactI:
 596     if (!Matcher::match_rule_supported(Op_OverflowSubI) || !UseMathExactIntrinsics) return NULL;
 597     break;
 598   case vmIntrinsics::_negateExactL:
 599     if (!Matcher::match_rule_supported(Op_OverflowSubL) || !UseMathExactIntrinsics) return NULL;
 600     break;
 601   case vmIntrinsics::_multiplyExactI:
 602     if (!Matcher::match_rule_supported(Op_OverflowMulI) || !UseMathExactIntrinsics) return NULL;
 603     break;
 604   case vmIntrinsics::_multiplyExactL:
 605     if (!Matcher::match_rule_supported(Op_OverflowMulL) || !UseMathExactIntrinsics) return NULL;
 606     break;
 607 
 608  default:
 609     assert(id &lt;= vmIntrinsics::LAST_COMPILER_INLINE, "caller responsibility");
 610     assert(id != vmIntrinsics::_Object_init &amp;&amp; id != vmIntrinsics::_invoke, "enum out of order?");
 611     break;
 612   }
 613 
 614   // -XX:-InlineClassNatives disables natives from the Class class.
 615   // The flag applies to all reflective calls, notably Array.newArray
 616   // (visible to Java programmers as Array.newInstance).
 617   if (m-&gt;holder()-&gt;name() == ciSymbol::java_lang_Class() ||
 618       m-&gt;holder()-&gt;name() == ciSymbol::java_lang_reflect_Array()) {
 619     if (!InlineClassNatives)  return NULL;
 620   }
 621 
 622   // -XX:-InlineThreadNatives disables natives from the Thread class.
 623   if (m-&gt;holder()-&gt;name() == ciSymbol::java_lang_Thread()) {
 624     if (!InlineThreadNatives)  return NULL;
 625   }
 626 
 627   // -XX:-InlineMathNatives disables natives from the Math,Float and Double classes.
 628   if (m-&gt;holder()-&gt;name() == ciSymbol::java_lang_Math() ||
 629       m-&gt;holder()-&gt;name() == ciSymbol::java_lang_Float() ||
 630       m-&gt;holder()-&gt;name() == ciSymbol::java_lang_Double()) {
 631     if (!InlineMathNatives)  return NULL;
 632   }
 633 
 634   // -XX:-InlineUnsafeOps disables natives from the Unsafe class.
 635   if (m-&gt;holder()-&gt;name() == ciSymbol::sun_misc_Unsafe()) {
 636     if (!InlineUnsafeOps)  return NULL;
 637   }
 638 
 639   return new LibraryIntrinsic(m, is_virtual, predicates, does_virtual_dispatch, (vmIntrinsics::ID) id);
 640 }
 641 
 642 //----------------------register_library_intrinsics-----------------------
 643 // Initialize this file's data structures, for each Compile instance.
 644 void Compile::register_library_intrinsics() {
 645   // Nothing to do here.
 646 }
 647 
 648 JVMState* LibraryIntrinsic::generate(JVMState* jvms) {
 649   LibraryCallKit kit(jvms, this);
 650   Compile* C = kit.C;
 651   int nodes = C-&gt;unique();
 652 #ifndef PRODUCT
 653   if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
 654     char buf[1000];
 655     const char* str = vmIntrinsics::short_name_as_C_string(intrinsic_id(), buf, sizeof(buf));
 656     tty-&gt;print_cr("Intrinsic %s", str);
 657   }
 658 #endif
 659   ciMethod* callee = kit.callee();
 660   const int bci    = kit.bci();
 661 
 662   // Try to inline the intrinsic.
 663   if (kit.try_to_inline(_last_predicate)) {
 664     if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
 665       C-&gt;print_inlining(callee, jvms-&gt;depth() - 1, bci, is_virtual() ? "(intrinsic, virtual)" : "(intrinsic)");
 666     }
 667     C-&gt;gather_intrinsic_statistics(intrinsic_id(), is_virtual(), Compile::_intrinsic_worked);
 668     if (C-&gt;log()) {
 669       C-&gt;log()-&gt;elem("intrinsic id='%s'%s nodes='%d'",
 670                      vmIntrinsics::name_at(intrinsic_id()),
 671                      (is_virtual() ? " virtual='1'" : ""),
 672                      C-&gt;unique() - nodes);
 673     }
 674     // Push the result from the inlined method onto the stack.
 675     kit.push_result();
 676     return kit.transfer_exceptions_into_jvms();
 677   }
 678 
 679   // The intrinsic bailed out
 680   if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
 681     if (jvms-&gt;has_method()) {
 682       // Not a root compile.
 683       const char* msg = is_virtual() ? "failed to inline (intrinsic, virtual)" : "failed to inline (intrinsic)";
 684       C-&gt;print_inlining(callee, jvms-&gt;depth() - 1, bci, msg);
 685     } else {
 686       // Root compile
 687       tty-&gt;print("Did not generate intrinsic %s%s at bci:%d in",
 688                vmIntrinsics::name_at(intrinsic_id()),
 689                (is_virtual() ? " (virtual)" : ""), bci);
 690     }
 691   }
 692   C-&gt;gather_intrinsic_statistics(intrinsic_id(), is_virtual(), Compile::_intrinsic_failed);
 693   return NULL;
 694 }
 695 
 696 Node* LibraryIntrinsic::generate_predicate(JVMState* jvms, int predicate) {
 697   LibraryCallKit kit(jvms, this);
 698   Compile* C = kit.C;
 699   int nodes = C-&gt;unique();
 700   _last_predicate = predicate;
 701 #ifndef PRODUCT
 702   assert(is_predicated() &amp;&amp; predicate &lt; predicates_count(), "sanity");
 703   if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
 704     char buf[1000];
 705     const char* str = vmIntrinsics::short_name_as_C_string(intrinsic_id(), buf, sizeof(buf));
 706     tty-&gt;print_cr("Predicate for intrinsic %s", str);
 707   }
 708 #endif
 709   ciMethod* callee = kit.callee();
 710   const int bci    = kit.bci();
 711 
 712   Node* slow_ctl = kit.try_to_predicate(predicate);
 713   if (!kit.failing()) {
 714     if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
 715       C-&gt;print_inlining(callee, jvms-&gt;depth() - 1, bci, is_virtual() ? "(intrinsic, virtual, predicate)" : "(intrinsic, predicate)");
 716     }
 717     C-&gt;gather_intrinsic_statistics(intrinsic_id(), is_virtual(), Compile::_intrinsic_worked);
 718     if (C-&gt;log()) {
 719       C-&gt;log()-&gt;elem("predicate_intrinsic id='%s'%s nodes='%d'",
 720                      vmIntrinsics::name_at(intrinsic_id()),
 721                      (is_virtual() ? " virtual='1'" : ""),
 722                      C-&gt;unique() - nodes);
 723     }
 724     return slow_ctl; // Could be NULL if the check folds.
 725   }
 726 
 727   // The intrinsic bailed out
 728   if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
 729     if (jvms-&gt;has_method()) {
 730       // Not a root compile.
 731       const char* msg = "failed to generate predicate for intrinsic";
 732       C-&gt;print_inlining(kit.callee(), jvms-&gt;depth() - 1, bci, msg);
 733     } else {
 734       // Root compile
 735       C-&gt;print_inlining_stream()-&gt;print("Did not generate predicate for intrinsic %s%s at bci:%d in",
 736                                         vmIntrinsics::name_at(intrinsic_id()),
 737                                         (is_virtual() ? " (virtual)" : ""), bci);
 738     }
 739   }
 740   C-&gt;gather_intrinsic_statistics(intrinsic_id(), is_virtual(), Compile::_intrinsic_failed);
 741   return NULL;
 742 }
 743 
 744 bool LibraryCallKit::try_to_inline(int predicate) {
 745   // Handle symbolic names for otherwise undistinguished boolean switches:
 746   const bool is_store       = true;
 747   const bool is_native_ptr  = true;
 748   const bool is_static      = true;
 749   const bool is_volatile    = true;
 750 
 751   if (!jvms()-&gt;has_method()) {
 752     // Root JVMState has a null method.
 753     assert(map()-&gt;memory()-&gt;Opcode() == Op_Parm, "");
 754     // Insert the memory aliasing node
 755     set_all_memory(reset_memory());
 756   }
 757   assert(merged_memory(), "");
 758 
 759 
 760   switch (intrinsic_id()) {
 761   case vmIntrinsics::_hashCode:                 return inline_native_hashcode(intrinsic()-&gt;is_virtual(), !is_static);
 762   case vmIntrinsics::_identityHashCode:         return inline_native_hashcode(/*!virtual*/ false,         is_static);
 763   case vmIntrinsics::_getClass:                 return inline_native_getClass();
 764 
 765   case vmIntrinsics::_dsin:
 766   case vmIntrinsics::_dcos:
 767   case vmIntrinsics::_dtan:
 768   case vmIntrinsics::_dabs:
 769   case vmIntrinsics::_datan2:
 770   case vmIntrinsics::_dsqrt:
 771   case vmIntrinsics::_dexp:
 772   case vmIntrinsics::_dlog:
 773   case vmIntrinsics::_dlog10:
 774   case vmIntrinsics::_dpow:                     return inline_math_native(intrinsic_id());
 775 
 776   case vmIntrinsics::_min:
 777   case vmIntrinsics::_max:                      return inline_min_max(intrinsic_id());
 778 
 779   case vmIntrinsics::_addExactI:                return inline_math_addExactI(false /* add */);
 780   case vmIntrinsics::_addExactL:                return inline_math_addExactL(false /* add */);
 781   case vmIntrinsics::_decrementExactI:          return inline_math_subtractExactI(true /* decrement */);
 782   case vmIntrinsics::_decrementExactL:          return inline_math_subtractExactL(true /* decrement */);
 783   case vmIntrinsics::_incrementExactI:          return inline_math_addExactI(true /* increment */);
 784   case vmIntrinsics::_incrementExactL:          return inline_math_addExactL(true /* increment */);
 785   case vmIntrinsics::_multiplyExactI:           return inline_math_multiplyExactI();
 786   case vmIntrinsics::_multiplyExactL:           return inline_math_multiplyExactL();
 787   case vmIntrinsics::_negateExactI:             return inline_math_negateExactI();
 788   case vmIntrinsics::_negateExactL:             return inline_math_negateExactL();
 789   case vmIntrinsics::_subtractExactI:           return inline_math_subtractExactI(false /* subtract */);
 790   case vmIntrinsics::_subtractExactL:           return inline_math_subtractExactL(false /* subtract */);
 791 
 792   case vmIntrinsics::_arraycopy:                return inline_arraycopy();
 793 
 794   case vmIntrinsics::_compareTo:                return inline_string_compareTo();
 795   case vmIntrinsics::_indexOf:                  return inline_string_indexOf();
 796   case vmIntrinsics::_equals:                   return inline_string_equals();
 797 
 798   case vmIntrinsics::_getObject:                return inline_unsafe_access(!is_native_ptr, !is_store, T_OBJECT,  !is_volatile, false);
 799   case vmIntrinsics::_getBoolean:               return inline_unsafe_access(!is_native_ptr, !is_store, T_BOOLEAN, !is_volatile, false);
 800   case vmIntrinsics::_getByte:                  return inline_unsafe_access(!is_native_ptr, !is_store, T_BYTE,    !is_volatile, false);
 801   case vmIntrinsics::_getShort:                 return inline_unsafe_access(!is_native_ptr, !is_store, T_SHORT,   !is_volatile, false);
 802   case vmIntrinsics::_getChar:                  return inline_unsafe_access(!is_native_ptr, !is_store, T_CHAR,    !is_volatile, false);
 803   case vmIntrinsics::_getInt:                   return inline_unsafe_access(!is_native_ptr, !is_store, T_INT,     !is_volatile, false);
 804   case vmIntrinsics::_getLong:                  return inline_unsafe_access(!is_native_ptr, !is_store, T_LONG,    !is_volatile, false);
 805   case vmIntrinsics::_getFloat:                 return inline_unsafe_access(!is_native_ptr, !is_store, T_FLOAT,   !is_volatile, false);
 806   case vmIntrinsics::_getDouble:                return inline_unsafe_access(!is_native_ptr, !is_store, T_DOUBLE,  !is_volatile, false);
 807 
 808   case vmIntrinsics::_putObject:                return inline_unsafe_access(!is_native_ptr,  is_store, T_OBJECT,  !is_volatile, false);
 809   case vmIntrinsics::_putBoolean:               return inline_unsafe_access(!is_native_ptr,  is_store, T_BOOLEAN, !is_volatile, false);
 810   case vmIntrinsics::_putByte:                  return inline_unsafe_access(!is_native_ptr,  is_store, T_BYTE,    !is_volatile, false);
 811   case vmIntrinsics::_putShort:                 return inline_unsafe_access(!is_native_ptr,  is_store, T_SHORT,   !is_volatile, false);
 812   case vmIntrinsics::_putChar:                  return inline_unsafe_access(!is_native_ptr,  is_store, T_CHAR,    !is_volatile, false);
 813   case vmIntrinsics::_putInt:                   return inline_unsafe_access(!is_native_ptr,  is_store, T_INT,     !is_volatile, false);
 814   case vmIntrinsics::_putLong:                  return inline_unsafe_access(!is_native_ptr,  is_store, T_LONG,    !is_volatile, false);
 815   case vmIntrinsics::_putFloat:                 return inline_unsafe_access(!is_native_ptr,  is_store, T_FLOAT,   !is_volatile, false);
 816   case vmIntrinsics::_putDouble:                return inline_unsafe_access(!is_native_ptr,  is_store, T_DOUBLE,  !is_volatile, false);
 817 
 818   case vmIntrinsics::_getByte_raw:              return inline_unsafe_access( is_native_ptr, !is_store, T_BYTE,    !is_volatile, false);
 819   case vmIntrinsics::_getShort_raw:             return inline_unsafe_access( is_native_ptr, !is_store, T_SHORT,   !is_volatile, false);
 820   case vmIntrinsics::_getChar_raw:              return inline_unsafe_access( is_native_ptr, !is_store, T_CHAR,    !is_volatile, false);
 821   case vmIntrinsics::_getInt_raw:               return inline_unsafe_access( is_native_ptr, !is_store, T_INT,     !is_volatile, false);
 822   case vmIntrinsics::_getLong_raw:              return inline_unsafe_access( is_native_ptr, !is_store, T_LONG,    !is_volatile, false);
 823   case vmIntrinsics::_getFloat_raw:             return inline_unsafe_access( is_native_ptr, !is_store, T_FLOAT,   !is_volatile, false);
 824   case vmIntrinsics::_getDouble_raw:            return inline_unsafe_access( is_native_ptr, !is_store, T_DOUBLE,  !is_volatile, false);
 825   case vmIntrinsics::_getAddress_raw:           return inline_unsafe_access( is_native_ptr, !is_store, T_ADDRESS, !is_volatile, false);
 826 
 827   case vmIntrinsics::_putByte_raw:              return inline_unsafe_access( is_native_ptr,  is_store, T_BYTE,    !is_volatile, false);
 828   case vmIntrinsics::_putShort_raw:             return inline_unsafe_access( is_native_ptr,  is_store, T_SHORT,   !is_volatile, false);
 829   case vmIntrinsics::_putChar_raw:              return inline_unsafe_access( is_native_ptr,  is_store, T_CHAR,    !is_volatile, false);
 830   case vmIntrinsics::_putInt_raw:               return inline_unsafe_access( is_native_ptr,  is_store, T_INT,     !is_volatile, false);
 831   case vmIntrinsics::_putLong_raw:              return inline_unsafe_access( is_native_ptr,  is_store, T_LONG,    !is_volatile, false);
 832   case vmIntrinsics::_putFloat_raw:             return inline_unsafe_access( is_native_ptr,  is_store, T_FLOAT,   !is_volatile, false);
 833   case vmIntrinsics::_putDouble_raw:            return inline_unsafe_access( is_native_ptr,  is_store, T_DOUBLE,  !is_volatile, false);
 834   case vmIntrinsics::_putAddress_raw:           return inline_unsafe_access( is_native_ptr,  is_store, T_ADDRESS, !is_volatile, false);
 835 
 836   case vmIntrinsics::_getObjectVolatile:        return inline_unsafe_access(!is_native_ptr, !is_store, T_OBJECT,   is_volatile, false);
 837   case vmIntrinsics::_getBooleanVolatile:       return inline_unsafe_access(!is_native_ptr, !is_store, T_BOOLEAN,  is_volatile, false);
 838   case vmIntrinsics::_getByteVolatile:          return inline_unsafe_access(!is_native_ptr, !is_store, T_BYTE,     is_volatile, false);
 839   case vmIntrinsics::_getShortVolatile:         return inline_unsafe_access(!is_native_ptr, !is_store, T_SHORT,    is_volatile, false);
 840   case vmIntrinsics::_getCharVolatile:          return inline_unsafe_access(!is_native_ptr, !is_store, T_CHAR,     is_volatile, false);
 841   case vmIntrinsics::_getIntVolatile:           return inline_unsafe_access(!is_native_ptr, !is_store, T_INT,      is_volatile, false);
 842   case vmIntrinsics::_getLongVolatile:          return inline_unsafe_access(!is_native_ptr, !is_store, T_LONG,     is_volatile, false);
 843   case vmIntrinsics::_getFloatVolatile:         return inline_unsafe_access(!is_native_ptr, !is_store, T_FLOAT,    is_volatile, false);
 844   case vmIntrinsics::_getDoubleVolatile:        return inline_unsafe_access(!is_native_ptr, !is_store, T_DOUBLE,   is_volatile, false);
 845 
 846   case vmIntrinsics::_putObjectVolatile:        return inline_unsafe_access(!is_native_ptr,  is_store, T_OBJECT,   is_volatile, false);
 847   case vmIntrinsics::_putBooleanVolatile:       return inline_unsafe_access(!is_native_ptr,  is_store, T_BOOLEAN,  is_volatile, false);
 848   case vmIntrinsics::_putByteVolatile:          return inline_unsafe_access(!is_native_ptr,  is_store, T_BYTE,     is_volatile, false);
 849   case vmIntrinsics::_putShortVolatile:         return inline_unsafe_access(!is_native_ptr,  is_store, T_SHORT,    is_volatile, false);
 850   case vmIntrinsics::_putCharVolatile:          return inline_unsafe_access(!is_native_ptr,  is_store, T_CHAR,     is_volatile, false);
 851   case vmIntrinsics::_putIntVolatile:           return inline_unsafe_access(!is_native_ptr,  is_store, T_INT,      is_volatile, false);
 852   case vmIntrinsics::_putLongVolatile:          return inline_unsafe_access(!is_native_ptr,  is_store, T_LONG,     is_volatile, false);
 853   case vmIntrinsics::_putFloatVolatile:         return inline_unsafe_access(!is_native_ptr,  is_store, T_FLOAT,    is_volatile, false);
 854   case vmIntrinsics::_putDoubleVolatile:        return inline_unsafe_access(!is_native_ptr,  is_store, T_DOUBLE,   is_volatile, false);
 855 
 856   case vmIntrinsics::_prefetchRead:             return inline_unsafe_prefetch(!is_native_ptr, !is_store, !is_static);
 857   case vmIntrinsics::_prefetchWrite:            return inline_unsafe_prefetch(!is_native_ptr,  is_store, !is_static);
 858   case vmIntrinsics::_prefetchReadStatic:       return inline_unsafe_prefetch(!is_native_ptr, !is_store,  is_static);
 859   case vmIntrinsics::_prefetchWriteStatic:      return inline_unsafe_prefetch(!is_native_ptr,  is_store,  is_static);
 860 
 861   case vmIntrinsics::_compareAndSwapObject:     return inline_unsafe_load_store(T_OBJECT, LS_cmpxchg);
 862   case vmIntrinsics::_compareAndSwapInt:        return inline_unsafe_load_store(T_INT,    LS_cmpxchg);
 863   case vmIntrinsics::_compareAndSwapLong:       return inline_unsafe_load_store(T_LONG,   LS_cmpxchg);
 864 
 865   case vmIntrinsics::_putOrderedObject:         return inline_unsafe_ordered_store(T_OBJECT);
 866   case vmIntrinsics::_putOrderedInt:            return inline_unsafe_ordered_store(T_INT);
 867   case vmIntrinsics::_putOrderedLong:           return inline_unsafe_ordered_store(T_LONG);
 868 
 869   case vmIntrinsics::_getAndAddInt:             return inline_unsafe_load_store(T_INT,    LS_xadd);
 870   case vmIntrinsics::_getAndAddLong:            return inline_unsafe_load_store(T_LONG,   LS_xadd);
 871   case vmIntrinsics::_getAndSetInt:             return inline_unsafe_load_store(T_INT,    LS_xchg);
 872   case vmIntrinsics::_getAndSetLong:            return inline_unsafe_load_store(T_LONG,   LS_xchg);
 873   case vmIntrinsics::_getAndSetObject:          return inline_unsafe_load_store(T_OBJECT, LS_xchg);
 874 
 875   case vmIntrinsics::_loadFence:
 876   case vmIntrinsics::_storeFence:
 877   case vmIntrinsics::_fullFence:                return inline_unsafe_fence(intrinsic_id());
 878 
 879   case vmIntrinsics::_currentThread:            return inline_native_currentThread();
 880   case vmIntrinsics::_isInterrupted:            return inline_native_isInterrupted();
 881 
 882 #ifdef TRACE_HAVE_INTRINSICS
 883   case vmIntrinsics::_classID:                  return inline_native_classID();
 884   case vmIntrinsics::_threadID:                 return inline_native_threadID();
 885   case vmIntrinsics::_counterTime:              return inline_native_time_funcs(CAST_FROM_FN_PTR(address, TRACE_TIME_METHOD), "counterTime");
 886 #endif
 887   case vmIntrinsics::_currentTimeMillis:        return inline_native_time_funcs(CAST_FROM_FN_PTR(address, os::javaTimeMillis), "currentTimeMillis");
 888   case vmIntrinsics::_nanoTime:                 return inline_native_time_funcs(CAST_FROM_FN_PTR(address, os::javaTimeNanos), "nanoTime");
 889   case vmIntrinsics::_allocateInstance:         return inline_unsafe_allocate();
 890   case vmIntrinsics::_copyMemory:               return inline_unsafe_copyMemory();
 891   case vmIntrinsics::_newArray:                 return inline_native_newArray();
 892   case vmIntrinsics::_getLength:                return inline_native_getLength();
 893   case vmIntrinsics::_copyOf:                   return inline_array_copyOf(false);
 894   case vmIntrinsics::_copyOfRange:              return inline_array_copyOf(true);
 895   case vmIntrinsics::_equalsC:                  return inline_array_equals();
 896   case vmIntrinsics::_clone:                    return inline_native_clone(intrinsic()-&gt;is_virtual());
 897 
 898   case vmIntrinsics::_isAssignableFrom:         return inline_native_subtype_check();
 899 
 900   case vmIntrinsics::_isInstance:
 901   case vmIntrinsics::_getModifiers:
 902   case vmIntrinsics::_isInterface:
 903   case vmIntrinsics::_isArray:
 904   case vmIntrinsics::_isPrimitive:
 905   case vmIntrinsics::_getSuperclass:
 906   case vmIntrinsics::_getComponentType:
 907   case vmIntrinsics::_getClassAccessFlags:      return inline_native_Class_query(intrinsic_id());
 908 
 909   case vmIntrinsics::_floatToRawIntBits:
 910   case vmIntrinsics::_floatToIntBits:
 911   case vmIntrinsics::_intBitsToFloat:
 912   case vmIntrinsics::_doubleToRawLongBits:
 913   case vmIntrinsics::_doubleToLongBits:
 914   case vmIntrinsics::_longBitsToDouble:         return inline_fp_conversions(intrinsic_id());
 915 
 916   case vmIntrinsics::_numberOfLeadingZeros_i:
 917   case vmIntrinsics::_numberOfLeadingZeros_l:
 918   case vmIntrinsics::_numberOfTrailingZeros_i:
 919   case vmIntrinsics::_numberOfTrailingZeros_l:
 920   case vmIntrinsics::_bitCount_i:
 921   case vmIntrinsics::_bitCount_l:
 922   case vmIntrinsics::_reverseBytes_i:
 923   case vmIntrinsics::_reverseBytes_l:
 924   case vmIntrinsics::_reverseBytes_s:
 925   case vmIntrinsics::_reverseBytes_c:           return inline_number_methods(intrinsic_id());
 926 
 927   case vmIntrinsics::_getCallerClass:           return inline_native_Reflection_getCallerClass();
 928 
 929   case vmIntrinsics::_Reference_get:            return inline_reference_get();
 930 
 931   case vmIntrinsics::_aescrypt_encryptBlock:
 932   case vmIntrinsics::_aescrypt_decryptBlock:    return inline_aescrypt_Block(intrinsic_id());
 933 
 934   case vmIntrinsics::_cipherBlockChaining_encryptAESCrypt:
 935   case vmIntrinsics::_cipherBlockChaining_decryptAESCrypt:
 936     return inline_cipherBlockChaining_AESCrypt(intrinsic_id());
 937 
 938   case vmIntrinsics::_sha_implCompress:
 939   case vmIntrinsics::_sha2_implCompress:
 940   case vmIntrinsics::_sha5_implCompress:
 941     return inline_sha_implCompress(intrinsic_id());
 942 
 943   case vmIntrinsics::_digestBase_implCompressMB:
 944     return inline_digestBase_implCompressMB(predicate);
 945 
 946   case vmIntrinsics::_multiplyToLen:
 947     return inline_multiplyToLen();
 948 
 949   case vmIntrinsics::_squareToLen:
 950     return inline_squareToLen();
 951 
 952   case vmIntrinsics::_mulAdd:
 953     return inline_mulAdd();
 954 
 955   case vmIntrinsics::_montgomeryMultiply:
 956     return inline_montgomeryMultiply();
 957   case vmIntrinsics::_montgomerySquare:
 958     return inline_montgomerySquare();
 959 
 960   case vmIntrinsics::_encodeISOArray:
 961     return inline_encodeISOArray();
 962 
 963   case vmIntrinsics::_updateCRC32:
 964     return inline_updateCRC32();
 965   case vmIntrinsics::_updateBytesCRC32:
 966     return inline_updateBytesCRC32();
 967   case vmIntrinsics::_updateByteBufferCRC32:
 968     return inline_updateByteBufferCRC32();
 969 
 970   case vmIntrinsics::_profileBoolean:
 971     return inline_profileBoolean();
 972 
 973   default:
 974     // If you get here, it may be that someone has added a new intrinsic
 975     // to the list in vmSymbols.hpp without implementing it here.
 976 #ifndef PRODUCT
 977     if ((PrintMiscellaneous &amp;&amp; (Verbose || WizardMode)) || PrintOpto) {
 978       tty-&gt;print_cr("*** Warning: Unimplemented intrinsic %s(%d)",
 979                     vmIntrinsics::name_at(intrinsic_id()), intrinsic_id());
 980     }
 981 #endif
 982     return false;
 983   }
 984 }
 985 
 986 Node* LibraryCallKit::try_to_predicate(int predicate) {
 987   if (!jvms()-&gt;has_method()) {
 988     // Root JVMState has a null method.
 989     assert(map()-&gt;memory()-&gt;Opcode() == Op_Parm, "");
 990     // Insert the memory aliasing node
 991     set_all_memory(reset_memory());
 992   }
 993   assert(merged_memory(), "");
 994 
 995   switch (intrinsic_id()) {
 996   case vmIntrinsics::_cipherBlockChaining_encryptAESCrypt:
 997     return inline_cipherBlockChaining_AESCrypt_predicate(false);
 998   case vmIntrinsics::_cipherBlockChaining_decryptAESCrypt:
 999     return inline_cipherBlockChaining_AESCrypt_predicate(true);
1000   case vmIntrinsics::_digestBase_implCompressMB:
1001     return inline_digestBase_implCompressMB_predicate(predicate);
1002 
1003   default:
1004     // If you get here, it may be that someone has added a new intrinsic
1005     // to the list in vmSymbols.hpp without implementing it here.
1006 #ifndef PRODUCT
1007     if ((PrintMiscellaneous &amp;&amp; (Verbose || WizardMode)) || PrintOpto) {
1008       tty-&gt;print_cr("*** Warning: Unimplemented predicate for intrinsic %s(%d)",
1009                     vmIntrinsics::name_at(intrinsic_id()), intrinsic_id());
1010     }
1011 #endif
1012     Node* slow_ctl = control();
1013     set_control(top()); // No fast path instrinsic
1014     return slow_ctl;
1015   }
1016 }
1017 
1018 //------------------------------set_result-------------------------------
1019 // Helper function for finishing intrinsics.
1020 void LibraryCallKit::set_result(RegionNode* region, PhiNode* value) {
1021   record_for_igvn(region);
1022   set_control(_gvn.transform(region));
1023   set_result( _gvn.transform(value));
1024   assert(value-&gt;type()-&gt;basic_type() == result()-&gt;bottom_type()-&gt;basic_type(), "sanity");
1025 }
1026 
1027 //------------------------------generate_guard---------------------------
1028 // Helper function for generating guarded fast-slow graph structures.
1029 // The given 'test', if true, guards a slow path.  If the test fails
1030 // then a fast path can be taken.  (We generally hope it fails.)
1031 // In all cases, GraphKit::control() is updated to the fast path.
1032 // The returned value represents the control for the slow path.
1033 // The return value is never 'top'; it is either a valid control
1034 // or NULL if it is obvious that the slow path can never be taken.
1035 // Also, if region and the slow control are not NULL, the slow edge
1036 // is appended to the region.
1037 Node* LibraryCallKit::generate_guard(Node* test, RegionNode* region, float true_prob) {
1038   if (stopped()) {
1039     // Already short circuited.
1040     return NULL;
1041   }
1042 
1043   // Build an if node and its projections.
1044   // If test is true we take the slow path, which we assume is uncommon.
1045   if (_gvn.type(test) == TypeInt::ZERO) {
1046     // The slow branch is never taken.  No need to build this guard.
1047     return NULL;
1048   }
1049 
1050   IfNode* iff = create_and_map_if(control(), test, true_prob, COUNT_UNKNOWN);
1051 
1052   Node* if_slow = _gvn.transform(new (C) IfTrueNode(iff));
1053   if (if_slow == top()) {
1054     // The slow branch is never taken.  No need to build this guard.
1055     return NULL;
1056   }
1057 
1058   if (region != NULL)
1059     region-&gt;add_req(if_slow);
1060 
1061   Node* if_fast = _gvn.transform(new (C) IfFalseNode(iff));
1062   set_control(if_fast);
1063 
1064   return if_slow;
1065 }
1066 
1067 inline Node* LibraryCallKit::generate_slow_guard(Node* test, RegionNode* region) {
1068   return generate_guard(test, region, PROB_UNLIKELY_MAG(3));
1069 }
1070 inline Node* LibraryCallKit::generate_fair_guard(Node* test, RegionNode* region) {
1071   return generate_guard(test, region, PROB_FAIR);
1072 }
1073 
1074 inline Node* LibraryCallKit::generate_negative_guard(Node* index, RegionNode* region,
1075                                                      Node* *pos_index) {
1076   if (stopped())
1077     return NULL;                // already stopped
1078   if (_gvn.type(index)-&gt;higher_equal(TypeInt::POS)) // [0,maxint]
1079     return NULL;                // index is already adequately typed
1080   Node* cmp_lt = _gvn.transform(new (C) CmpINode(index, intcon(0)));
1081   Node* bol_lt = _gvn.transform(new (C) BoolNode(cmp_lt, BoolTest::lt));
1082   Node* is_neg = generate_guard(bol_lt, region, PROB_MIN);
1083   if (is_neg != NULL &amp;&amp; pos_index != NULL) {
1084     // Emulate effect of Parse::adjust_map_after_if.
1085     Node* ccast = new (C) CastIINode(index, TypeInt::POS);
1086     ccast-&gt;set_req(0, control());
1087     (*pos_index) = _gvn.transform(ccast);
1088   }
1089   return is_neg;
1090 }
1091 
1092 inline Node* LibraryCallKit::generate_nonpositive_guard(Node* index, bool never_negative,
1093                                                         Node* *pos_index) {
1094   if (stopped())
1095     return NULL;                // already stopped
1096   if (_gvn.type(index)-&gt;higher_equal(TypeInt::POS1)) // [1,maxint]
1097     return NULL;                // index is already adequately typed
1098   Node* cmp_le = _gvn.transform(new (C) CmpINode(index, intcon(0)));
1099   BoolTest::mask le_or_eq = (never_negative ? BoolTest::eq : BoolTest::le);
1100   Node* bol_le = _gvn.transform(new (C) BoolNode(cmp_le, le_or_eq));
1101   Node* is_notp = generate_guard(bol_le, NULL, PROB_MIN);
1102   if (is_notp != NULL &amp;&amp; pos_index != NULL) {
1103     // Emulate effect of Parse::adjust_map_after_if.
1104     Node* ccast = new (C) CastIINode(index, TypeInt::POS1);
1105     ccast-&gt;set_req(0, control());
1106     (*pos_index) = _gvn.transform(ccast);
1107   }
1108   return is_notp;
1109 }
1110 
1111 // Make sure that 'position' is a valid limit index, in [0..length].
1112 // There are two equivalent plans for checking this:
1113 //   A. (offset + copyLength)  unsigned&lt;=  arrayLength
1114 //   B. offset  &lt;=  (arrayLength - copyLength)
1115 // We require that all of the values above, except for the sum and
1116 // difference, are already known to be non-negative.
1117 // Plan A is robust in the face of overflow, if offset and copyLength
1118 // are both hugely positive.
1119 //
1120 // Plan B is less direct and intuitive, but it does not overflow at
1121 // all, since the difference of two non-negatives is always
1122 // representable.  Whenever Java methods must perform the equivalent
1123 // check they generally use Plan B instead of Plan A.
1124 // For the moment we use Plan A.
1125 inline Node* LibraryCallKit::generate_limit_guard(Node* offset,
1126                                                   Node* subseq_length,
1127                                                   Node* array_length,
1128                                                   RegionNode* region) {
1129   if (stopped())
1130     return NULL;                // already stopped
1131   bool zero_offset = _gvn.type(offset) == TypeInt::ZERO;
1132   if (zero_offset &amp;&amp; subseq_length-&gt;eqv_uncast(array_length))
1133     return NULL;                // common case of whole-array copy
1134   Node* last = subseq_length;
1135   if (!zero_offset)             // last += offset
1136     last = _gvn.transform(new (C) AddINode(last, offset));
1137   Node* cmp_lt = _gvn.transform(new (C) CmpUNode(array_length, last));
1138   Node* bol_lt = _gvn.transform(new (C) BoolNode(cmp_lt, BoolTest::lt));
1139   Node* is_over = generate_guard(bol_lt, region, PROB_MIN);
1140   return is_over;
1141 }
1142 
1143 
1144 //--------------------------generate_current_thread--------------------
1145 Node* LibraryCallKit::generate_current_thread(Node* &amp;tls_output) {
1146   ciKlass*    thread_klass = env()-&gt;Thread_klass();
1147   const Type* thread_type  = TypeOopPtr::make_from_klass(thread_klass)-&gt;cast_to_ptr_type(TypePtr::NotNull);
1148   Node* thread = _gvn.transform(new (C) ThreadLocalNode());
1149   Node* p = basic_plus_adr(top()/*!oop*/, thread, in_bytes(JavaThread::threadObj_offset()));
1150   Node* threadObj = make_load(NULL, p, thread_type, T_OBJECT, MemNode::unordered);
1151   tls_output = thread;
1152   return threadObj;
1153 }
1154 
1155 
1156 //------------------------------make_string_method_node------------------------
1157 // Helper method for String intrinsic functions. This version is called
1158 // with str1 and str2 pointing to String object nodes.
1159 //
1160 Node* LibraryCallKit::make_string_method_node(int opcode, Node* str1, Node* str2) {
1161   Node* no_ctrl = NULL;
1162 
1163   // Get start addr of string
1164   Node* str1_value   = load_String_value(no_ctrl, str1);
1165   Node* str1_offset  = load_String_offset(no_ctrl, str1);
1166   Node* str1_start   = array_element_address(str1_value, str1_offset, T_CHAR);
1167 
1168   // Get length of string 1
1169   Node* str1_len  = load_String_length(no_ctrl, str1);
1170 
1171   Node* str2_value   = load_String_value(no_ctrl, str2);
1172   Node* str2_offset  = load_String_offset(no_ctrl, str2);
1173   Node* str2_start   = array_element_address(str2_value, str2_offset, T_CHAR);
1174 
1175   Node* str2_len = NULL;
1176   Node* result = NULL;
1177 
1178   switch (opcode) {
1179   case Op_StrIndexOf:
1180     // Get length of string 2
1181     str2_len = load_String_length(no_ctrl, str2);
1182 
1183     result = new (C) StrIndexOfNode(control(), memory(TypeAryPtr::CHARS),
1184                                  str1_start, str1_len, str2_start, str2_len);
1185     break;
1186   case Op_StrComp:
1187     // Get length of string 2
1188     str2_len = load_String_length(no_ctrl, str2);
1189 
1190     result = new (C) StrCompNode(control(), memory(TypeAryPtr::CHARS),
1191                                  str1_start, str1_len, str2_start, str2_len);
1192     break;
1193   case Op_StrEquals:
1194     result = new (C) StrEqualsNode(control(), memory(TypeAryPtr::CHARS),
1195                                str1_start, str2_start, str1_len);
1196     break;
1197   default:
1198     ShouldNotReachHere();
1199     return NULL;
1200   }
1201 
1202   // All these intrinsics have checks.
1203   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
1204 
1205   return _gvn.transform(result);
1206 }
1207 
1208 // Helper method for String intrinsic functions. This version is called
1209 // with str1 and str2 pointing to char[] nodes, with cnt1 and cnt2 pointing
1210 // to Int nodes containing the lenghts of str1 and str2.
1211 //
1212 Node* LibraryCallKit::make_string_method_node(int opcode, Node* str1_start, Node* cnt1, Node* str2_start, Node* cnt2) {
1213   Node* result = NULL;
1214   switch (opcode) {
1215   case Op_StrIndexOf:
1216     result = new (C) StrIndexOfNode(control(), memory(TypeAryPtr::CHARS),
1217                                  str1_start, cnt1, str2_start, cnt2);
1218     break;
1219   case Op_StrComp:
1220     result = new (C) StrCompNode(control(), memory(TypeAryPtr::CHARS),
1221                                  str1_start, cnt1, str2_start, cnt2);
1222     break;
1223   case Op_StrEquals:
1224     result = new (C) StrEqualsNode(control(), memory(TypeAryPtr::CHARS),
1225                                  str1_start, str2_start, cnt1);
1226     break;
1227   default:
1228     ShouldNotReachHere();
1229     return NULL;
1230   }
1231 
1232   // All these intrinsics have checks.
1233   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
1234 
1235   return _gvn.transform(result);
1236 }
1237 
1238 //------------------------------inline_string_compareTo------------------------
1239 // public int java.lang.String.compareTo(String anotherString);
1240 bool LibraryCallKit::inline_string_compareTo() {
1241   Node* receiver = null_check(argument(0));
1242   Node* arg      = null_check(argument(1));
1243   if (stopped()) {
1244     return true;
1245   }
1246   set_result(make_string_method_node(Op_StrComp, receiver, arg));
1247   return true;
1248 }
1249 
1250 //------------------------------inline_string_equals------------------------
1251 bool LibraryCallKit::inline_string_equals() {
1252   Node* receiver = null_check_receiver();
1253   // NOTE: Do not null check argument for String.equals() because spec
1254   // allows to specify NULL as argument.
1255   Node* argument = this-&gt;argument(1);
1256   if (stopped()) {
1257     return true;
1258   }
1259 
1260   // paths (plus control) merge
1261   RegionNode* region = new (C) RegionNode(5);
1262   Node* phi = new (C) PhiNode(region, TypeInt::BOOL);
1263 
1264   // does source == target string?
1265   Node* cmp = _gvn.transform(new (C) CmpPNode(receiver, argument));
1266   Node* bol = _gvn.transform(new (C) BoolNode(cmp, BoolTest::eq));
1267 
1268   Node* if_eq = generate_slow_guard(bol, NULL);
1269   if (if_eq != NULL) {
1270     // receiver == argument
1271     phi-&gt;init_req(2, intcon(1));
1272     region-&gt;init_req(2, if_eq);
1273   }
1274 
1275   // get String klass for instanceOf
1276   ciInstanceKlass* klass = env()-&gt;String_klass();
1277 
1278   if (!stopped()) {
1279     Node* inst = gen_instanceof(argument, makecon(TypeKlassPtr::make(klass)));
1280     Node* cmp  = _gvn.transform(new (C) CmpINode(inst, intcon(1)));
1281     Node* bol  = _gvn.transform(new (C) BoolNode(cmp, BoolTest::ne));
1282 
1283     Node* inst_false = generate_guard(bol, NULL, PROB_MIN);
1284     //instanceOf == true, fallthrough
1285 
1286     if (inst_false != NULL) {
1287       phi-&gt;init_req(3, intcon(0));
1288       region-&gt;init_req(3, inst_false);
1289     }
1290   }
1291 
1292   if (!stopped()) {
1293     const TypeOopPtr* string_type = TypeOopPtr::make_from_klass(klass);
1294 
1295     // Properly cast the argument to String
1296     argument = _gvn.transform(new (C) CheckCastPPNode(control(), argument, string_type));
1297     // This path is taken only when argument's type is String:NotNull.
1298     argument = cast_not_null(argument, false);
1299 
1300     Node* no_ctrl = NULL;
1301 
1302     // Get start addr of receiver
1303     Node* receiver_val    = load_String_value(no_ctrl, receiver);
1304     Node* receiver_offset = load_String_offset(no_ctrl, receiver);
1305     Node* receiver_start = array_element_address(receiver_val, receiver_offset, T_CHAR);
1306 
1307     // Get length of receiver
1308     Node* receiver_cnt  = load_String_length(no_ctrl, receiver);
1309 
1310     // Get start addr of argument
1311     Node* argument_val    = load_String_value(no_ctrl, argument);
1312     Node* argument_offset = load_String_offset(no_ctrl, argument);
1313     Node* argument_start = array_element_address(argument_val, argument_offset, T_CHAR);
1314 
1315     // Get length of argument
1316     Node* argument_cnt  = load_String_length(no_ctrl, argument);
1317 
1318     // Check for receiver count != argument count
1319     Node* cmp = _gvn.transform(new(C) CmpINode(receiver_cnt, argument_cnt));
1320     Node* bol = _gvn.transform(new(C) BoolNode(cmp, BoolTest::ne));
1321     Node* if_ne = generate_slow_guard(bol, NULL);
1322     if (if_ne != NULL) {
1323       phi-&gt;init_req(4, intcon(0));
1324       region-&gt;init_req(4, if_ne);
1325     }
1326 
1327     // Check for count == 0 is done by assembler code for StrEquals.
1328 
1329     if (!stopped()) {
1330       Node* equals = make_string_method_node(Op_StrEquals, receiver_start, receiver_cnt, argument_start, argument_cnt);
1331       phi-&gt;init_req(1, equals);
1332       region-&gt;init_req(1, control());
1333     }
1334   }
1335 
1336   // post merge
1337   set_control(_gvn.transform(region));
1338   record_for_igvn(region);
1339 
1340   set_result(_gvn.transform(phi));
1341   return true;
1342 }
1343 
1344 //------------------------------inline_array_equals----------------------------
1345 bool LibraryCallKit::inline_array_equals() {
1346   Node* arg1 = argument(0);
1347   Node* arg2 = argument(1);
1348   set_result(_gvn.transform(new (C) AryEqNode(control(), memory(TypeAryPtr::CHARS), arg1, arg2)));
1349   return true;
1350 }
1351 
1352 // Java version of String.indexOf(constant string)
1353 // class StringDecl {
1354 //   StringDecl(char[] ca) {
1355 //     offset = 0;
1356 //     count = ca.length;
1357 //     value = ca;
1358 //   }
1359 //   int offset;
1360 //   int count;
1361 //   char[] value;
1362 // }
1363 //
1364 // static int string_indexOf_J(StringDecl string_object, char[] target_object,
1365 //                             int targetOffset, int cache_i, int md2) {
1366 //   int cache = cache_i;
1367 //   int sourceOffset = string_object.offset;
1368 //   int sourceCount = string_object.count;
1369 //   int targetCount = target_object.length;
1370 //
1371 //   int targetCountLess1 = targetCount - 1;
1372 //   int sourceEnd = sourceOffset + sourceCount - targetCountLess1;
1373 //
1374 //   char[] source = string_object.value;
1375 //   char[] target = target_object;
1376 //   int lastChar = target[targetCountLess1];
1377 //
1378 //  outer_loop:
1379 //   for (int i = sourceOffset; i &lt; sourceEnd; ) {
1380 //     int src = source[i + targetCountLess1];
1381 //     if (src == lastChar) {
1382 //       // With random strings and a 4-character alphabet,
1383 //       // reverse matching at this point sets up 0.8% fewer
1384 //       // frames, but (paradoxically) makes 0.3% more probes.
1385 //       // Since those probes are nearer the lastChar probe,
1386 //       // there is may be a net D$ win with reverse matching.
1387 //       // But, reversing loop inhibits unroll of inner loop
1388 //       // for unknown reason.  So, does running outer loop from
1389 //       // (sourceOffset - targetCountLess1) to (sourceOffset + sourceCount)
1390 //       for (int j = 0; j &lt; targetCountLess1; j++) {
1391 //         if (target[targetOffset + j] != source[i+j]) {
1392 //           if ((cache &amp; (1 &lt;&lt; source[i+j])) == 0) {
1393 //             if (md2 &lt; j+1) {
1394 //               i += j+1;
1395 //               continue outer_loop;
1396 //             }
1397 //           }
1398 //           i += md2;
1399 //           continue outer_loop;
1400 //         }
1401 //       }
1402 //       return i - sourceOffset;
1403 //     }
1404 //     if ((cache &amp; (1 &lt;&lt; src)) == 0) {
1405 //       i += targetCountLess1;
1406 //     } // using "i += targetCount;" and an "else i++;" causes a jump to jump.
1407 //     i++;
1408 //   }
1409 //   return -1;
1410 // }
1411 
1412 //------------------------------string_indexOf------------------------
1413 Node* LibraryCallKit::string_indexOf(Node* string_object, ciTypeArray* target_array, jint targetOffset_i,
1414                                      jint cache_i, jint md2_i) {
1415 
1416   Node* no_ctrl  = NULL;
1417   float likely   = PROB_LIKELY(0.9);
1418   float unlikely = PROB_UNLIKELY(0.9);
1419 
1420   const int nargs = 0; // no arguments to push back for uncommon trap in predicate
1421 
1422   Node* source        = load_String_value(no_ctrl, string_object);
1423   Node* sourceOffset  = load_String_offset(no_ctrl, string_object);
1424   Node* sourceCount   = load_String_length(no_ctrl, string_object);
1425 
1426   Node* target = _gvn.transform( makecon(TypeOopPtr::make_from_constant(target_array, true)));
1427   jint target_length = target_array-&gt;length();
1428   const TypeAry* target_array_type = TypeAry::make(TypeInt::CHAR, TypeInt::make(0, target_length, Type::WidenMin));
1429   const TypeAryPtr* target_type = TypeAryPtr::make(TypePtr::BotPTR, target_array_type, target_array-&gt;klass(), true, Type::OffsetBot);
1430 
1431   // String.value field is known to be @Stable.
1432   if (UseImplicitStableValues) {
1433     target = cast_array_to_stable(target, target_type);
1434   }
1435 
1436   IdealKit kit(this, false, true);
1437 #define __ kit.
1438   Node* zero             = __ ConI(0);
1439   Node* one              = __ ConI(1);
1440   Node* cache            = __ ConI(cache_i);
1441   Node* md2              = __ ConI(md2_i);
1442   Node* lastChar         = __ ConI(target_array-&gt;char_at(target_length - 1));
1443   Node* targetCount      = __ ConI(target_length);
1444   Node* targetCountLess1 = __ ConI(target_length - 1);
1445   Node* targetOffset     = __ ConI(targetOffset_i);
1446   Node* sourceEnd        = __ SubI(__ AddI(sourceOffset, sourceCount), targetCountLess1);
1447 
1448   IdealVariable rtn(kit), i(kit), j(kit); __ declarations_done();
1449   Node* outer_loop = __ make_label(2 /* goto */);
1450   Node* return_    = __ make_label(1);
1451 
1452   __ set(rtn,__ ConI(-1));
1453   __ loop(this, nargs, i, sourceOffset, BoolTest::lt, sourceEnd); {
1454        Node* i2  = __ AddI(__ value(i), targetCountLess1);
1455        // pin to prohibit loading of "next iteration" value which may SEGV (rare)
1456        Node* src = load_array_element(__ ctrl(), source, i2, TypeAryPtr::CHARS);
1457        __ if_then(src, BoolTest::eq, lastChar, unlikely); {
1458          __ loop(this, nargs, j, zero, BoolTest::lt, targetCountLess1); {
1459               Node* tpj = __ AddI(targetOffset, __ value(j));
1460               Node* targ = load_array_element(no_ctrl, target, tpj, target_type);
1461               Node* ipj  = __ AddI(__ value(i), __ value(j));
1462               Node* src2 = load_array_element(no_ctrl, source, ipj, TypeAryPtr::CHARS);
1463               __ if_then(targ, BoolTest::ne, src2); {
1464                 __ if_then(__ AndI(cache, __ LShiftI(one, src2)), BoolTest::eq, zero); {
1465                   __ if_then(md2, BoolTest::lt, __ AddI(__ value(j), one)); {
1466                     __ increment(i, __ AddI(__ value(j), one));
1467                     __ goto_(outer_loop);
1468                   } __ end_if(); __ dead(j);
1469                 }__ end_if(); __ dead(j);
1470                 __ increment(i, md2);
1471                 __ goto_(outer_loop);
1472               }__ end_if();
1473               __ increment(j, one);
1474          }__ end_loop(); __ dead(j);
1475          __ set(rtn, __ SubI(__ value(i), sourceOffset)); __ dead(i);
1476          __ goto_(return_);
1477        }__ end_if();
1478        __ if_then(__ AndI(cache, __ LShiftI(one, src)), BoolTest::eq, zero, likely); {
1479          __ increment(i, targetCountLess1);
1480        }__ end_if();
1481        __ increment(i, one);
1482        __ bind(outer_loop);
1483   }__ end_loop(); __ dead(i);
1484   __ bind(return_);
1485 
1486   // Final sync IdealKit and GraphKit.
1487   final_sync(kit);
1488   Node* result = __ value(rtn);
1489 #undef __
1490   C-&gt;set_has_loops(true);
1491   return result;
1492 }
1493 
1494 //------------------------------inline_string_indexOf------------------------
1495 bool LibraryCallKit::inline_string_indexOf() {
1496   Node* receiver = argument(0);
1497   Node* arg      = argument(1);
1498 
1499   Node* result;
1500   // Disable the use of pcmpestri until it can be guaranteed that
1501   // the load doesn't cross into the uncommited space.
1502   if (Matcher::has_match_rule(Op_StrIndexOf) &amp;&amp;
1503       UseSSE42Intrinsics) {
1504     // Generate SSE4.2 version of indexOf
1505     // We currently only have match rules that use SSE4.2
1506 
1507     receiver = null_check(receiver);
1508     arg      = null_check(arg);
1509     if (stopped()) {
1510       return true;
1511     }
1512 
1513     ciInstanceKlass* str_klass = env()-&gt;String_klass();
1514     const TypeOopPtr* string_type = TypeOopPtr::make_from_klass(str_klass);
1515 
1516     // Make the merge point
1517     RegionNode* result_rgn = new (C) RegionNode(4);
1518     Node*       result_phi = new (C) PhiNode(result_rgn, TypeInt::INT);
1519     Node* no_ctrl  = NULL;
1520 
1521     // Get start addr of source string
1522     Node* source = load_String_value(no_ctrl, receiver);
1523     Node* source_offset = load_String_offset(no_ctrl, receiver);
1524     Node* source_start = array_element_address(source, source_offset, T_CHAR);
1525 
1526     // Get length of source string
1527     Node* source_cnt  = load_String_length(no_ctrl, receiver);
1528 
1529     // Get start addr of substring
1530     Node* substr = load_String_value(no_ctrl, arg);
1531     Node* substr_offset = load_String_offset(no_ctrl, arg);
1532     Node* substr_start = array_element_address(substr, substr_offset, T_CHAR);
1533 
1534     // Get length of source string
1535     Node* substr_cnt  = load_String_length(no_ctrl, arg);
1536 
1537     // Check for substr count &gt; string count
1538     Node* cmp = _gvn.transform(new(C) CmpINode(substr_cnt, source_cnt));
1539     Node* bol = _gvn.transform(new(C) BoolNode(cmp, BoolTest::gt));
1540     Node* if_gt = generate_slow_guard(bol, NULL);
1541     if (if_gt != NULL) {
1542       result_phi-&gt;init_req(2, intcon(-1));
1543       result_rgn-&gt;init_req(2, if_gt);
1544     }
1545 
1546     if (!stopped()) {
1547       // Check for substr count == 0
1548       cmp = _gvn.transform(new(C) CmpINode(substr_cnt, intcon(0)));
1549       bol = _gvn.transform(new(C) BoolNode(cmp, BoolTest::eq));
1550       Node* if_zero = generate_slow_guard(bol, NULL);
1551       if (if_zero != NULL) {
1552         result_phi-&gt;init_req(3, intcon(0));
1553         result_rgn-&gt;init_req(3, if_zero);
1554       }
1555     }
1556 
1557     if (!stopped()) {
1558       result = make_string_method_node(Op_StrIndexOf, source_start, source_cnt, substr_start, substr_cnt);
1559       result_phi-&gt;init_req(1, result);
1560       result_rgn-&gt;init_req(1, control());
1561     }
1562     set_control(_gvn.transform(result_rgn));
1563     record_for_igvn(result_rgn);
1564     result = _gvn.transform(result_phi);
1565 
1566   } else { // Use LibraryCallKit::string_indexOf
1567     // don't intrinsify if argument isn't a constant string.
1568     if (!arg-&gt;is_Con()) {
1569      return false;
1570     }
1571     const TypeOopPtr* str_type = _gvn.type(arg)-&gt;isa_oopptr();
1572     if (str_type == NULL) {
1573       return false;
1574     }
1575     ciInstanceKlass* klass = env()-&gt;String_klass();
1576     ciObject* str_const = str_type-&gt;const_oop();
1577     if (str_const == NULL || str_const-&gt;klass() != klass) {
1578       return false;
1579     }
1580     ciInstance* str = str_const-&gt;as_instance();
1581     assert(str != NULL, "must be instance");
1582 
1583     ciObject* v = str-&gt;field_value_by_offset(java_lang_String::value_offset_in_bytes()).as_object();
1584     ciTypeArray* pat = v-&gt;as_type_array(); // pattern (argument) character array
1585 
1586     int o;
1587     int c;
1588     if (java_lang_String::has_offset_field()) {
1589       o = str-&gt;field_value_by_offset(java_lang_String::offset_offset_in_bytes()).as_int();
1590       c = str-&gt;field_value_by_offset(java_lang_String::count_offset_in_bytes()).as_int();
1591     } else {
1592       o = 0;
1593       c = pat-&gt;length();
1594     }
1595 
1596     // constant strings have no offset and count == length which
1597     // simplifies the resulting code somewhat so lets optimize for that.
1598     if (o != 0 || c != pat-&gt;length()) {
1599      return false;
1600     }
1601 
1602     receiver = null_check(receiver, T_OBJECT);
1603     // NOTE: No null check on the argument is needed since it's a constant String oop.
1604     if (stopped()) {
1605       return true;
1606     }
1607 
1608     // The null string as a pattern always returns 0 (match at beginning of string)
1609     if (c == 0) {
1610       set_result(intcon(0));
1611       return true;
1612     }
1613 
1614     // Generate default indexOf
1615     jchar lastChar = pat-&gt;char_at(o + (c - 1));
1616     int cache = 0;
1617     int i;
1618     for (i = 0; i &lt; c - 1; i++) {
1619       assert(i &lt; pat-&gt;length(), "out of range");
1620       cache |= (1 &lt;&lt; (pat-&gt;char_at(o + i) &amp; (sizeof(cache) * BitsPerByte - 1)));
1621     }
1622 
1623     int md2 = c;
1624     for (i = 0; i &lt; c - 1; i++) {
1625       assert(i &lt; pat-&gt;length(), "out of range");
1626       if (pat-&gt;char_at(o + i) == lastChar) {
1627         md2 = (c - 1) - i;
1628       }
1629     }
1630 
1631     result = string_indexOf(receiver, pat, o, cache, md2);
1632   }
1633   set_result(result);
1634   return true;
1635 }
1636 
1637 //--------------------------round_double_node--------------------------------
1638 // Round a double node if necessary.
1639 Node* LibraryCallKit::round_double_node(Node* n) {
1640   if (Matcher::strict_fp_requires_explicit_rounding &amp;&amp; UseSSE &lt;= 1)
1641     n = _gvn.transform(new (C) RoundDoubleNode(0, n));
1642   return n;
1643 }
1644 
1645 //------------------------------inline_math-----------------------------------
1646 // public static double Math.abs(double)
1647 // public static double Math.sqrt(double)
1648 // public static double Math.log(double)
1649 // public static double Math.log10(double)
1650 bool LibraryCallKit::inline_math(vmIntrinsics::ID id) {
1651   Node* arg = round_double_node(argument(0));
1652   Node* n = NULL;
1653   switch (id) {
1654   case vmIntrinsics::_dabs:   n = new (C) AbsDNode(                arg);  break;
1655   case vmIntrinsics::_dsqrt:  n = new (C) SqrtDNode(C, control(),  arg);  break;
1656   case vmIntrinsics::_dlog:   n = new (C) LogDNode(C, control(),   arg);  break;
1657   case vmIntrinsics::_dlog10: n = new (C) Log10DNode(C, control(), arg);  break;
1658   default:  fatal_unexpected_iid(id);  break;
1659   }
1660   set_result(_gvn.transform(n));
1661   return true;
1662 }
1663 
1664 //------------------------------inline_trig----------------------------------
1665 // Inline sin/cos/tan instructions, if possible.  If rounding is required, do
1666 // argument reduction which will turn into a fast/slow diamond.
1667 bool LibraryCallKit::inline_trig(vmIntrinsics::ID id) {
1668   Node* arg = round_double_node(argument(0));
1669   Node* n = NULL;
1670 
1671   switch (id) {
1672   case vmIntrinsics::_dsin:  n = new (C) SinDNode(C, control(), arg);  break;
1673   case vmIntrinsics::_dcos:  n = new (C) CosDNode(C, control(), arg);  break;
1674   case vmIntrinsics::_dtan:  n = new (C) TanDNode(C, control(), arg);  break;
1675   default:  fatal_unexpected_iid(id);  break;
1676   }
1677   n = _gvn.transform(n);
1678 
1679   // Rounding required?  Check for argument reduction!
1680   if (Matcher::strict_fp_requires_explicit_rounding) {
1681     static const double     pi_4 =  0.7853981633974483;
1682     static const double neg_pi_4 = -0.7853981633974483;
1683     // pi/2 in 80-bit extended precision
1684     // static const unsigned char pi_2_bits_x[] = {0x35,0xc2,0x68,0x21,0xa2,0xda,0x0f,0xc9,0xff,0x3f,0x00,0x00,0x00,0x00,0x00,0x00};
1685     // -pi/2 in 80-bit extended precision
1686     // static const unsigned char neg_pi_2_bits_x[] = {0x35,0xc2,0x68,0x21,0xa2,0xda,0x0f,0xc9,0xff,0xbf,0x00,0x00,0x00,0x00,0x00,0x00};
1687     // Cutoff value for using this argument reduction technique
1688     //static const double    pi_2_minus_epsilon =  1.564660403643354;
1689     //static const double neg_pi_2_plus_epsilon = -1.564660403643354;
1690 
1691     // Pseudocode for sin:
1692     // if (x &lt;= Math.PI / 4.0) {
1693     //   if (x &gt;= -Math.PI / 4.0) return  fsin(x);
1694     //   if (x &gt;= -Math.PI / 2.0) return -fcos(x + Math.PI / 2.0);
1695     // } else {
1696     //   if (x &lt;=  Math.PI / 2.0) return  fcos(x - Math.PI / 2.0);
1697     // }
1698     // return StrictMath.sin(x);
1699 
1700     // Pseudocode for cos:
1701     // if (x &lt;= Math.PI / 4.0) {
1702     //   if (x &gt;= -Math.PI / 4.0) return  fcos(x);
1703     //   if (x &gt;= -Math.PI / 2.0) return  fsin(x + Math.PI / 2.0);
1704     // } else {
1705     //   if (x &lt;=  Math.PI / 2.0) return -fsin(x - Math.PI / 2.0);
1706     // }
1707     // return StrictMath.cos(x);
1708 
1709     // Actually, sticking in an 80-bit Intel value into C2 will be tough; it
1710     // requires a special machine instruction to load it.  Instead we'll try
1711     // the 'easy' case.  If we really need the extra range +/- PI/2 we'll
1712     // probably do the math inside the SIN encoding.
1713 
1714     // Make the merge point
1715     RegionNode* r = new (C) RegionNode(3);
1716     Node* phi = new (C) PhiNode(r, Type::DOUBLE);
1717 
1718     // Flatten arg so we need only 1 test
1719     Node *abs = _gvn.transform(new (C) AbsDNode(arg));
1720     // Node for PI/4 constant
1721     Node *pi4 = makecon(TypeD::make(pi_4));
1722     // Check PI/4 : abs(arg)
1723     Node *cmp = _gvn.transform(new (C) CmpDNode(pi4,abs));
1724     // Check: If PI/4 &lt; abs(arg) then go slow
1725     Node *bol = _gvn.transform(new (C) BoolNode( cmp, BoolTest::lt ));
1726     // Branch either way
1727     IfNode *iff = create_and_xform_if(control(),bol, PROB_STATIC_FREQUENT, COUNT_UNKNOWN);
1728     set_control(opt_iff(r,iff));
1729 
1730     // Set fast path result
1731     phi-&gt;init_req(2, n);
1732 
1733     // Slow path - non-blocking leaf call
1734     Node* call = NULL;
1735     switch (id) {
1736     case vmIntrinsics::_dsin:
1737       call = make_runtime_call(RC_LEAF, OptoRuntime::Math_D_D_Type(),
1738                                CAST_FROM_FN_PTR(address, SharedRuntime::dsin),
1739                                "Sin", NULL, arg, top());
1740       break;
1741     case vmIntrinsics::_dcos:
1742       call = make_runtime_call(RC_LEAF, OptoRuntime::Math_D_D_Type(),
1743                                CAST_FROM_FN_PTR(address, SharedRuntime::dcos),
1744                                "Cos", NULL, arg, top());
1745       break;
1746     case vmIntrinsics::_dtan:
1747       call = make_runtime_call(RC_LEAF, OptoRuntime::Math_D_D_Type(),
1748                                CAST_FROM_FN_PTR(address, SharedRuntime::dtan),
1749                                "Tan", NULL, arg, top());
1750       break;
1751     }
1752     assert(control()-&gt;in(0) == call, "");
1753     Node* slow_result = _gvn.transform(new (C) ProjNode(call, TypeFunc::Parms));
1754     r-&gt;init_req(1, control());
1755     phi-&gt;init_req(1, slow_result);
1756 
1757     // Post-merge
1758     set_control(_gvn.transform(r));
1759     record_for_igvn(r);
1760     n = _gvn.transform(phi);
1761 
1762     C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
1763   }
1764   set_result(n);
1765   return true;
1766 }
1767 
1768 Node* LibraryCallKit::finish_pow_exp(Node* result, Node* x, Node* y, const TypeFunc* call_type, address funcAddr, const char* funcName) {
1769   //-------------------
1770   //result=(result.isNaN())? funcAddr():result;
1771   // Check: If isNaN() by checking result!=result? then either trap
1772   // or go to runtime
1773   Node* cmpisnan = _gvn.transform(new (C) CmpDNode(result, result));
1774   // Build the boolean node
1775   Node* bolisnum = _gvn.transform(new (C) BoolNode(cmpisnan, BoolTest::eq));
1776 
1777   if (!too_many_traps(Deoptimization::Reason_intrinsic)) {
1778     { BuildCutout unless(this, bolisnum, PROB_STATIC_FREQUENT);
1779       // The pow or exp intrinsic returned a NaN, which requires a call
1780       // to the runtime.  Recompile with the runtime call.
1781       uncommon_trap(Deoptimization::Reason_intrinsic,
1782                     Deoptimization::Action_make_not_entrant);
1783     }
1784     return result;
1785   } else {
1786     // If this inlining ever returned NaN in the past, we compile a call
1787     // to the runtime to properly handle corner cases
1788 
1789     IfNode* iff = create_and_xform_if(control(), bolisnum, PROB_STATIC_FREQUENT, COUNT_UNKNOWN);
1790     Node* if_slow = _gvn.transform(new (C) IfFalseNode(iff));
1791     Node* if_fast = _gvn.transform(new (C) IfTrueNode(iff));
1792 
1793     if (!if_slow-&gt;is_top()) {
1794       RegionNode* result_region = new (C) RegionNode(3);
1795       PhiNode*    result_val = new (C) PhiNode(result_region, Type::DOUBLE);
1796 
1797       result_region-&gt;init_req(1, if_fast);
1798       result_val-&gt;init_req(1, result);
1799 
1800       set_control(if_slow);
1801 
1802       const TypePtr* no_memory_effects = NULL;
1803       Node* rt = make_runtime_call(RC_LEAF, call_type, funcAddr, funcName,
1804                                    no_memory_effects,
1805                                    x, top(), y, y ? top() : NULL);
1806       Node* value = _gvn.transform(new (C) ProjNode(rt, TypeFunc::Parms+0));
1807 #ifdef ASSERT
1808       Node* value_top = _gvn.transform(new (C) ProjNode(rt, TypeFunc::Parms+1));
1809       assert(value_top == top(), "second value must be top");
1810 #endif
1811 
1812       result_region-&gt;init_req(2, control());
1813       result_val-&gt;init_req(2, value);
1814       set_control(_gvn.transform(result_region));
1815       return _gvn.transform(result_val);
1816     } else {
1817       return result;
1818     }
1819   }
1820 }
1821 
1822 //------------------------------inline_exp-------------------------------------
1823 // Inline exp instructions, if possible.  The Intel hardware only misses
1824 // really odd corner cases (+/- Infinity).  Just uncommon-trap them.
1825 bool LibraryCallKit::inline_exp() {
1826   Node* arg = round_double_node(argument(0));
1827   Node* n   = _gvn.transform(new (C) ExpDNode(C, control(), arg));
1828 
1829   n = finish_pow_exp(n, arg, NULL, OptoRuntime::Math_D_D_Type(), CAST_FROM_FN_PTR(address, SharedRuntime::dexp), "EXP");
1830   set_result(n);
1831 
1832   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
1833   return true;
1834 }
1835 
1836 //------------------------------inline_pow-------------------------------------
1837 // Inline power instructions, if possible.
1838 bool LibraryCallKit::inline_pow() {
1839   // Pseudocode for pow
1840   // if (y == 2) {
1841   //   return x * x;
1842   // } else {
1843   //   if (x &lt;= 0.0) {
1844   //     long longy = (long)y;
1845   //     if ((double)longy == y) { // if y is long
1846   //       if (y + 1 == y) longy = 0; // huge number: even
1847   //       result = ((1&amp;longy) == 0)?-DPow(abs(x), y):DPow(abs(x), y);
1848   //     } else {
1849   //       result = NaN;
1850   //     }
1851   //   } else {
1852   //     result = DPow(x,y);
1853   //   }
1854   //   if (result != result)?  {
1855   //     result = uncommon_trap() or runtime_call();
1856   //   }
1857   //   return result;
1858   // }
1859 
1860   Node* x = round_double_node(argument(0));
1861   Node* y = round_double_node(argument(2));
1862 
1863   Node* result = NULL;
1864 
1865   Node*   const_two_node = makecon(TypeD::make(2.0));
1866   Node*   cmp_node       = _gvn.transform(new (C) CmpDNode(y, const_two_node));
1867   Node*   bool_node      = _gvn.transform(new (C) BoolNode(cmp_node, BoolTest::eq));
1868   IfNode* if_node        = create_and_xform_if(control(), bool_node, PROB_STATIC_INFREQUENT, COUNT_UNKNOWN);
1869   Node*   if_true        = _gvn.transform(new (C) IfTrueNode(if_node));
1870   Node*   if_false       = _gvn.transform(new (C) IfFalseNode(if_node));
1871 
1872   RegionNode* region_node = new (C) RegionNode(3);
1873   region_node-&gt;init_req(1, if_true);
1874 
1875   Node* phi_node = new (C) PhiNode(region_node, Type::DOUBLE);
1876   // special case for x^y where y == 2, we can convert it to x * x
1877   phi_node-&gt;init_req(1, _gvn.transform(new (C) MulDNode(x, x)));
1878 
1879   // set control to if_false since we will now process the false branch
1880   set_control(if_false);
1881 
1882   if (!too_many_traps(Deoptimization::Reason_intrinsic)) {
1883     // Short form: skip the fancy tests and just check for NaN result.
1884     result = _gvn.transform(new (C) PowDNode(C, control(), x, y));
1885   } else {
1886     // If this inlining ever returned NaN in the past, include all
1887     // checks + call to the runtime.
1888 
1889     // Set the merge point for If node with condition of (x &lt;= 0.0)
1890     // There are four possible paths to region node and phi node
1891     RegionNode *r = new (C) RegionNode(4);
1892     Node *phi = new (C) PhiNode(r, Type::DOUBLE);
1893 
1894     // Build the first if node: if (x &lt;= 0.0)
1895     // Node for 0 constant
1896     Node *zeronode = makecon(TypeD::ZERO);
1897     // Check x:0
1898     Node *cmp = _gvn.transform(new (C) CmpDNode(x, zeronode));
1899     // Check: If (x&lt;=0) then go complex path
1900     Node *bol1 = _gvn.transform(new (C) BoolNode( cmp, BoolTest::le ));
1901     // Branch either way
1902     IfNode *if1 = create_and_xform_if(control(),bol1, PROB_STATIC_INFREQUENT, COUNT_UNKNOWN);
1903     // Fast path taken; set region slot 3
1904     Node *fast_taken = _gvn.transform(new (C) IfFalseNode(if1));
1905     r-&gt;init_req(3,fast_taken); // Capture fast-control
1906 
1907     // Fast path not-taken, i.e. slow path
1908     Node *complex_path = _gvn.transform(new (C) IfTrueNode(if1));
1909 
1910     // Set fast path result
1911     Node *fast_result = _gvn.transform(new (C) PowDNode(C, control(), x, y));
1912     phi-&gt;init_req(3, fast_result);
1913 
1914     // Complex path
1915     // Build the second if node (if y is long)
1916     // Node for (long)y
1917     Node *longy = _gvn.transform(new (C) ConvD2LNode(y));
1918     // Node for (double)((long) y)
1919     Node *doublelongy= _gvn.transform(new (C) ConvL2DNode(longy));
1920     // Check (double)((long) y) : y
1921     Node *cmplongy= _gvn.transform(new (C) CmpDNode(doublelongy, y));
1922     // Check if (y isn't long) then go to slow path
1923 
1924     Node *bol2 = _gvn.transform(new (C) BoolNode( cmplongy, BoolTest::ne ));
1925     // Branch either way
1926     IfNode *if2 = create_and_xform_if(complex_path,bol2, PROB_STATIC_INFREQUENT, COUNT_UNKNOWN);
1927     Node* ylong_path = _gvn.transform(new (C) IfFalseNode(if2));
1928 
1929     Node *slow_path = _gvn.transform(new (C) IfTrueNode(if2));
1930 
1931     // Calculate DPow(abs(x), y)*(1 &amp; (long)y)
1932     // Node for constant 1
1933     Node *conone = longcon(1);
1934     // 1&amp; (long)y
1935     Node *signnode= _gvn.transform(new (C) AndLNode(conone, longy));
1936 
1937     // A huge number is always even. Detect a huge number by checking
1938     // if y + 1 == y and set integer to be tested for parity to 0.
1939     // Required for corner case:
1940     // (long)9.223372036854776E18 = max_jlong
1941     // (double)(long)9.223372036854776E18 = 9.223372036854776E18
1942     // max_jlong is odd but 9.223372036854776E18 is even
1943     Node* yplus1 = _gvn.transform(new (C) AddDNode(y, makecon(TypeD::make(1))));
1944     Node *cmpyplus1= _gvn.transform(new (C) CmpDNode(yplus1, y));
1945     Node *bolyplus1 = _gvn.transform(new (C) BoolNode( cmpyplus1, BoolTest::eq ));
1946     Node* correctedsign = NULL;
1947     if (ConditionalMoveLimit != 0) {
1948       correctedsign = _gvn.transform( CMoveNode::make(C, NULL, bolyplus1, signnode, longcon(0), TypeLong::LONG));
1949     } else {
1950       IfNode *ifyplus1 = create_and_xform_if(ylong_path,bolyplus1, PROB_FAIR, COUNT_UNKNOWN);
1951       RegionNode *r = new (C) RegionNode(3);
1952       Node *phi = new (C) PhiNode(r, TypeLong::LONG);
1953       r-&gt;init_req(1, _gvn.transform(new (C) IfFalseNode(ifyplus1)));
1954       r-&gt;init_req(2, _gvn.transform(new (C) IfTrueNode(ifyplus1)));
1955       phi-&gt;init_req(1, signnode);
1956       phi-&gt;init_req(2, longcon(0));
1957       correctedsign = _gvn.transform(phi);
1958       ylong_path = _gvn.transform(r);
1959       record_for_igvn(r);
1960     }
1961 
1962     // zero node
1963     Node *conzero = longcon(0);
1964     // Check (1&amp;(long)y)==0?
1965     Node *cmpeq1 = _gvn.transform(new (C) CmpLNode(correctedsign, conzero));
1966     // Check if (1&amp;(long)y)!=0?, if so the result is negative
1967     Node *bol3 = _gvn.transform(new (C) BoolNode( cmpeq1, BoolTest::ne ));
1968     // abs(x)
1969     Node *absx=_gvn.transform(new (C) AbsDNode(x));
1970     // abs(x)^y
1971     Node *absxpowy = _gvn.transform(new (C) PowDNode(C, control(), absx, y));
1972     // -abs(x)^y
1973     Node *negabsxpowy = _gvn.transform(new (C) NegDNode (absxpowy));
1974     // (1&amp;(long)y)==1?-DPow(abs(x), y):DPow(abs(x), y)
1975     Node *signresult = NULL;
1976     if (ConditionalMoveLimit != 0) {
1977       signresult = _gvn.transform( CMoveNode::make(C, NULL, bol3, absxpowy, negabsxpowy, Type::DOUBLE));
1978     } else {
1979       IfNode *ifyeven = create_and_xform_if(ylong_path,bol3, PROB_FAIR, COUNT_UNKNOWN);
1980       RegionNode *r = new (C) RegionNode(3);
1981       Node *phi = new (C) PhiNode(r, Type::DOUBLE);
1982       r-&gt;init_req(1, _gvn.transform(new (C) IfFalseNode(ifyeven)));
1983       r-&gt;init_req(2, _gvn.transform(new (C) IfTrueNode(ifyeven)));
1984       phi-&gt;init_req(1, absxpowy);
1985       phi-&gt;init_req(2, negabsxpowy);
1986       signresult = _gvn.transform(phi);
1987       ylong_path = _gvn.transform(r);
1988       record_for_igvn(r);
1989     }
1990     // Set complex path fast result
1991     r-&gt;init_req(2, ylong_path);
1992     phi-&gt;init_req(2, signresult);
1993 
1994     static const jlong nan_bits = CONST64(0x7ff8000000000000);
1995     Node *slow_result = makecon(TypeD::make(*(double*)&amp;nan_bits)); // return NaN
1996     r-&gt;init_req(1,slow_path);
1997     phi-&gt;init_req(1,slow_result);
1998 
1999     // Post merge
2000     set_control(_gvn.transform(r));
2001     record_for_igvn(r);
2002     result = _gvn.transform(phi);
2003   }
2004 
2005   result = finish_pow_exp(result, x, y, OptoRuntime::Math_DD_D_Type(), CAST_FROM_FN_PTR(address, SharedRuntime::dpow), "POW");
2006 
2007   // control from finish_pow_exp is now input to the region node
2008   region_node-&gt;set_req(2, control());
2009   // the result from finish_pow_exp is now input to the phi node
2010   phi_node-&gt;init_req(2, result);
2011   set_control(_gvn.transform(region_node));
2012   record_for_igvn(region_node);
2013   set_result(_gvn.transform(phi_node));
2014 
2015   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
2016   return true;
2017 }
2018 
2019 //------------------------------runtime_math-----------------------------
2020 bool LibraryCallKit::runtime_math(const TypeFunc* call_type, address funcAddr, const char* funcName) {
2021   assert(call_type == OptoRuntime::Math_DD_D_Type() || call_type == OptoRuntime::Math_D_D_Type(),
2022          "must be (DD)D or (D)D type");
2023 
2024   // Inputs
2025   Node* a = round_double_node(argument(0));
2026   Node* b = (call_type == OptoRuntime::Math_DD_D_Type()) ? round_double_node(argument(2)) : NULL;
2027 
2028   const TypePtr* no_memory_effects = NULL;
2029   Node* trig = make_runtime_call(RC_LEAF, call_type, funcAddr, funcName,
2030                                  no_memory_effects,
2031                                  a, top(), b, b ? top() : NULL);
2032   Node* value = _gvn.transform(new (C) ProjNode(trig, TypeFunc::Parms+0));
2033 #ifdef ASSERT
2034   Node* value_top = _gvn.transform(new (C) ProjNode(trig, TypeFunc::Parms+1));
2035   assert(value_top == top(), "second value must be top");
2036 #endif
2037 
2038   set_result(value);
2039   return true;
2040 }
2041 
2042 //------------------------------inline_math_native-----------------------------
2043 bool LibraryCallKit::inline_math_native(vmIntrinsics::ID id) {
2044 #define FN_PTR(f) CAST_FROM_FN_PTR(address, f)
2045   switch (id) {
2046     // These intrinsics are not properly supported on all hardware
2047   case vmIntrinsics::_dcos:   return Matcher::has_match_rule(Op_CosD)   ? inline_trig(id) :
2048     runtime_math(OptoRuntime::Math_D_D_Type(), FN_PTR(SharedRuntime::dcos),   "COS");
2049   case vmIntrinsics::_dsin:   return Matcher::has_match_rule(Op_SinD)   ? inline_trig(id) :
2050     runtime_math(OptoRuntime::Math_D_D_Type(), FN_PTR(SharedRuntime::dsin),   "SIN");
2051   case vmIntrinsics::_dtan:   return Matcher::has_match_rule(Op_TanD)   ? inline_trig(id) :
2052     runtime_math(OptoRuntime::Math_D_D_Type(), FN_PTR(SharedRuntime::dtan),   "TAN");
2053 
2054   case vmIntrinsics::_dlog:   return Matcher::has_match_rule(Op_LogD)   ? inline_math(id) :
2055     runtime_math(OptoRuntime::Math_D_D_Type(), FN_PTR(SharedRuntime::dlog),   "LOG");
2056   case vmIntrinsics::_dlog10: return Matcher::has_match_rule(Op_Log10D) ? inline_math(id) :
2057     runtime_math(OptoRuntime::Math_D_D_Type(), FN_PTR(SharedRuntime::dlog10), "LOG10");
2058 
2059     // These intrinsics are supported on all hardware
2060   case vmIntrinsics::_dsqrt:  return Matcher::match_rule_supported(Op_SqrtD) ? inline_math(id) : false;
2061   case vmIntrinsics::_dabs:   return Matcher::has_match_rule(Op_AbsD)   ? inline_math(id) : false;
2062 
2063   case vmIntrinsics::_dexp:   return Matcher::has_match_rule(Op_ExpD)   ? inline_exp()    :
2064     runtime_math(OptoRuntime::Math_D_D_Type(),  FN_PTR(SharedRuntime::dexp),  "EXP");
2065   case vmIntrinsics::_dpow:   return Matcher::has_match_rule(Op_PowD)   ? inline_pow()    :
2066     runtime_math(OptoRuntime::Math_DD_D_Type(), FN_PTR(SharedRuntime::dpow),  "POW");
2067 #undef FN_PTR
2068 
2069    // These intrinsics are not yet correctly implemented
2070   case vmIntrinsics::_datan2:
2071     return false;
2072 
2073   default:
2074     fatal_unexpected_iid(id);
2075     return false;
2076   }
2077 }
2078 
2079 static bool is_simple_name(Node* n) {
2080   return (n-&gt;req() == 1         // constant
2081           || (n-&gt;is_Type() &amp;&amp; n-&gt;as_Type()-&gt;type()-&gt;singleton())
2082           || n-&gt;is_Proj()       // parameter or return value
2083           || n-&gt;is_Phi()        // local of some sort
2084           );
2085 }
2086 
2087 //----------------------------inline_min_max-----------------------------------
2088 bool LibraryCallKit::inline_min_max(vmIntrinsics::ID id) {
2089   set_result(generate_min_max(id, argument(0), argument(1)));
2090   return true;
2091 }
2092 
2093 void LibraryCallKit::inline_math_mathExact(Node* math, Node *test) {
2094   Node* bol = _gvn.transform( new (C) BoolNode(test, BoolTest::overflow) );
2095   IfNode* check = create_and_map_if(control(), bol, PROB_UNLIKELY_MAG(3), COUNT_UNKNOWN);
2096   Node* fast_path = _gvn.transform( new (C) IfFalseNode(check));
2097   Node* slow_path = _gvn.transform( new (C) IfTrueNode(check) );
2098 
2099   {
2100     PreserveJVMState pjvms(this);
2101     PreserveReexecuteState preexecs(this);
2102     jvms()-&gt;set_should_reexecute(true);
2103 
2104     set_control(slow_path);
2105     set_i_o(i_o());
2106 
2107     uncommon_trap(Deoptimization::Reason_intrinsic,
2108                   Deoptimization::Action_none);
2109   }
2110 
2111   set_control(fast_path);
2112   set_result(math);
2113 }
2114 
2115 template &lt;typename OverflowOp&gt;
2116 bool LibraryCallKit::inline_math_overflow(Node* arg1, Node* arg2) {
2117   typedef typename OverflowOp::MathOp MathOp;
2118 
2119   MathOp* mathOp = new(C) MathOp(arg1, arg2);
2120   Node* operation = _gvn.transform( mathOp );
2121   Node* ofcheck = _gvn.transform( new(C) OverflowOp(arg1, arg2) );
2122   inline_math_mathExact(operation, ofcheck);
2123   return true;
2124 }
2125 
2126 bool LibraryCallKit::inline_math_addExactI(bool is_increment) {
2127   return inline_math_overflow&lt;OverflowAddINode&gt;(argument(0), is_increment ? intcon(1) : argument(1));
2128 }
2129 
2130 bool LibraryCallKit::inline_math_addExactL(bool is_increment) {
2131   return inline_math_overflow&lt;OverflowAddLNode&gt;(argument(0), is_increment ? longcon(1) : argument(2));
2132 }
2133 
2134 bool LibraryCallKit::inline_math_subtractExactI(bool is_decrement) {
2135   return inline_math_overflow&lt;OverflowSubINode&gt;(argument(0), is_decrement ? intcon(1) : argument(1));
2136 }
2137 
2138 bool LibraryCallKit::inline_math_subtractExactL(bool is_decrement) {
2139   return inline_math_overflow&lt;OverflowSubLNode&gt;(argument(0), is_decrement ? longcon(1) : argument(2));
2140 }
2141 
2142 bool LibraryCallKit::inline_math_negateExactI() {
2143   return inline_math_overflow&lt;OverflowSubINode&gt;(intcon(0), argument(0));
2144 }
2145 
2146 bool LibraryCallKit::inline_math_negateExactL() {
2147   return inline_math_overflow&lt;OverflowSubLNode&gt;(longcon(0), argument(0));
2148 }
2149 
2150 bool LibraryCallKit::inline_math_multiplyExactI() {
2151   return inline_math_overflow&lt;OverflowMulINode&gt;(argument(0), argument(1));
2152 }
2153 
2154 bool LibraryCallKit::inline_math_multiplyExactL() {
2155   return inline_math_overflow&lt;OverflowMulLNode&gt;(argument(0), argument(2));
2156 }
2157 
2158 Node*
2159 LibraryCallKit::generate_min_max(vmIntrinsics::ID id, Node* x0, Node* y0) {
2160   // These are the candidate return value:
2161   Node* xvalue = x0;
2162   Node* yvalue = y0;
2163 
2164   if (xvalue == yvalue) {
2165     return xvalue;
2166   }
2167 
2168   bool want_max = (id == vmIntrinsics::_max);
2169 
2170   const TypeInt* txvalue = _gvn.type(xvalue)-&gt;isa_int();
2171   const TypeInt* tyvalue = _gvn.type(yvalue)-&gt;isa_int();
2172   if (txvalue == NULL || tyvalue == NULL)  return top();
2173   // This is not really necessary, but it is consistent with a
2174   // hypothetical MaxINode::Value method:
2175   int widen = MAX2(txvalue-&gt;_widen, tyvalue-&gt;_widen);
2176 
2177   // %%% This folding logic should (ideally) be in a different place.
2178   // Some should be inside IfNode, and there to be a more reliable
2179   // transformation of ?: style patterns into cmoves.  We also want
2180   // more powerful optimizations around cmove and min/max.
2181 
2182   // Try to find a dominating comparison of these guys.
2183   // It can simplify the index computation for Arrays.copyOf
2184   // and similar uses of System.arraycopy.
2185   // First, compute the normalized version of CmpI(x, y).
2186   int   cmp_op = Op_CmpI;
2187   Node* xkey = xvalue;
2188   Node* ykey = yvalue;
2189   Node* ideal_cmpxy = _gvn.transform(new(C) CmpINode(xkey, ykey));
2190   if (ideal_cmpxy-&gt;is_Cmp()) {
2191     // E.g., if we have CmpI(length - offset, count),
2192     // it might idealize to CmpI(length, count + offset)
2193     cmp_op = ideal_cmpxy-&gt;Opcode();
2194     xkey = ideal_cmpxy-&gt;in(1);
2195     ykey = ideal_cmpxy-&gt;in(2);
2196   }
2197 
2198   // Start by locating any relevant comparisons.
2199   Node* start_from = (xkey-&gt;outcnt() &lt; ykey-&gt;outcnt()) ? xkey : ykey;
2200   Node* cmpxy = NULL;
2201   Node* cmpyx = NULL;
2202   for (DUIterator_Fast kmax, k = start_from-&gt;fast_outs(kmax); k &lt; kmax; k++) {
2203     Node* cmp = start_from-&gt;fast_out(k);
2204     if (cmp-&gt;outcnt() &gt; 0 &amp;&amp;            // must have prior uses
2205         cmp-&gt;in(0) == NULL &amp;&amp;           // must be context-independent
2206         cmp-&gt;Opcode() == cmp_op) {      // right kind of compare
2207       if (cmp-&gt;in(1) == xkey &amp;&amp; cmp-&gt;in(2) == ykey)  cmpxy = cmp;
2208       if (cmp-&gt;in(1) == ykey &amp;&amp; cmp-&gt;in(2) == xkey)  cmpyx = cmp;
2209     }
2210   }
2211 
2212   const int NCMPS = 2;
2213   Node* cmps[NCMPS] = { cmpxy, cmpyx };
2214   int cmpn;
2215   for (cmpn = 0; cmpn &lt; NCMPS; cmpn++) {
2216     if (cmps[cmpn] != NULL)  break;     // find a result
2217   }
2218   if (cmpn &lt; NCMPS) {
2219     // Look for a dominating test that tells us the min and max.
2220     int depth = 0;                // Limit search depth for speed
2221     Node* dom = control();
2222     for (; dom != NULL; dom = IfNode::up_one_dom(dom, true)) {
2223       if (++depth &gt;= 100)  break;
2224       Node* ifproj = dom;
2225       if (!ifproj-&gt;is_Proj())  continue;
2226       Node* iff = ifproj-&gt;in(0);
2227       if (!iff-&gt;is_If())  continue;
2228       Node* bol = iff-&gt;in(1);
2229       if (!bol-&gt;is_Bool())  continue;
2230       Node* cmp = bol-&gt;in(1);
2231       if (cmp == NULL)  continue;
2232       for (cmpn = 0; cmpn &lt; NCMPS; cmpn++)
2233         if (cmps[cmpn] == cmp)  break;
2234       if (cmpn == NCMPS)  continue;
2235       BoolTest::mask btest = bol-&gt;as_Bool()-&gt;_test._test;
2236       if (ifproj-&gt;is_IfFalse())  btest = BoolTest(btest).negate();
2237       if (cmp-&gt;in(1) == ykey)    btest = BoolTest(btest).commute();
2238       // At this point, we know that 'x btest y' is true.
2239       switch (btest) {
2240       case BoolTest::eq:
2241         // They are proven equal, so we can collapse the min/max.
2242         // Either value is the answer.  Choose the simpler.
2243         if (is_simple_name(yvalue) &amp;&amp; !is_simple_name(xvalue))
2244           return yvalue;
2245         return xvalue;
2246       case BoolTest::lt:          // x &lt; y
2247       case BoolTest::le:          // x &lt;= y
2248         return (want_max ? yvalue : xvalue);
2249       case BoolTest::gt:          // x &gt; y
2250       case BoolTest::ge:          // x &gt;= y
2251         return (want_max ? xvalue : yvalue);
2252       }
2253     }
2254   }
2255 
2256   // We failed to find a dominating test.
2257   // Let's pick a test that might GVN with prior tests.
2258   Node*          best_bol   = NULL;
2259   BoolTest::mask best_btest = BoolTest::illegal;
2260   for (cmpn = 0; cmpn &lt; NCMPS; cmpn++) {
2261     Node* cmp = cmps[cmpn];
2262     if (cmp == NULL)  continue;
2263     for (DUIterator_Fast jmax, j = cmp-&gt;fast_outs(jmax); j &lt; jmax; j++) {
2264       Node* bol = cmp-&gt;fast_out(j);
2265       if (!bol-&gt;is_Bool())  continue;
2266       BoolTest::mask btest = bol-&gt;as_Bool()-&gt;_test._test;
2267       if (btest == BoolTest::eq || btest == BoolTest::ne)  continue;
2268       if (cmp-&gt;in(1) == ykey)   btest = BoolTest(btest).commute();
2269       if (bol-&gt;outcnt() &gt; (best_bol == NULL ? 0 : best_bol-&gt;outcnt())) {
2270         best_bol   = bol-&gt;as_Bool();
2271         best_btest = btest;
2272       }
2273     }
2274   }
2275 
2276   Node* answer_if_true  = NULL;
2277   Node* answer_if_false = NULL;
2278   switch (best_btest) {
2279   default:
2280     if (cmpxy == NULL)
2281       cmpxy = ideal_cmpxy;
2282     best_bol = _gvn.transform(new(C) BoolNode(cmpxy, BoolTest::lt));
2283     // and fall through:
2284   case BoolTest::lt:          // x &lt; y
2285   case BoolTest::le:          // x &lt;= y
2286     answer_if_true  = (want_max ? yvalue : xvalue);
2287     answer_if_false = (want_max ? xvalue : yvalue);
2288     break;
2289   case BoolTest::gt:          // x &gt; y
2290   case BoolTest::ge:          // x &gt;= y
2291     answer_if_true  = (want_max ? xvalue : yvalue);
2292     answer_if_false = (want_max ? yvalue : xvalue);
2293     break;
2294   }
2295 
2296   jint hi, lo;
2297   if (want_max) {
2298     // We can sharpen the minimum.
2299     hi = MAX2(txvalue-&gt;_hi, tyvalue-&gt;_hi);
2300     lo = MAX2(txvalue-&gt;_lo, tyvalue-&gt;_lo);
2301   } else {
2302     // We can sharpen the maximum.
2303     hi = MIN2(txvalue-&gt;_hi, tyvalue-&gt;_hi);
2304     lo = MIN2(txvalue-&gt;_lo, tyvalue-&gt;_lo);
2305   }
2306 
2307   // Use a flow-free graph structure, to avoid creating excess control edges
2308   // which could hinder other optimizations.
2309   // Since Math.min/max is often used with arraycopy, we want
2310   // tightly_coupled_allocation to be able to see beyond min/max expressions.
2311   Node* cmov = CMoveNode::make(C, NULL, best_bol,
2312                                answer_if_false, answer_if_true,
2313                                TypeInt::make(lo, hi, widen));
2314 
2315   return _gvn.transform(cmov);
2316 
2317   /*
2318   // This is not as desirable as it may seem, since Min and Max
2319   // nodes do not have a full set of optimizations.
2320   // And they would interfere, anyway, with 'if' optimizations
2321   // and with CMoveI canonical forms.
2322   switch (id) {
2323   case vmIntrinsics::_min:
2324     result_val = _gvn.transform(new (C, 3) MinINode(x,y)); break;
2325   case vmIntrinsics::_max:
2326     result_val = _gvn.transform(new (C, 3) MaxINode(x,y)); break;
2327   default:
2328     ShouldNotReachHere();
2329   }
2330   */
2331 }
2332 
2333 inline int
2334 LibraryCallKit::classify_unsafe_addr(Node* &amp;base, Node* &amp;offset) {
2335   const TypePtr* base_type = TypePtr::NULL_PTR;
2336   if (base != NULL)  base_type = _gvn.type(base)-&gt;isa_ptr();
2337   if (base_type == NULL) {
2338     // Unknown type.
2339     return Type::AnyPtr;
2340   } else if (base_type == TypePtr::NULL_PTR) {
2341     // Since this is a NULL+long form, we have to switch to a rawptr.
2342     base   = _gvn.transform(new (C) CastX2PNode(offset));
2343     offset = MakeConX(0);
2344     return Type::RawPtr;
2345   } else if (base_type-&gt;base() == Type::RawPtr) {
2346     return Type::RawPtr;
2347   } else if (base_type-&gt;isa_oopptr()) {
2348     // Base is never null =&gt; always a heap address.
2349     if (base_type-&gt;ptr() == TypePtr::NotNull) {
2350       return Type::OopPtr;
2351     }
2352     // Offset is small =&gt; always a heap address.
2353     const TypeX* offset_type = _gvn.type(offset)-&gt;isa_intptr_t();
2354     if (offset_type != NULL &amp;&amp;
2355         base_type-&gt;offset() == 0 &amp;&amp;     // (should always be?)
2356         offset_type-&gt;_lo &gt;= 0 &amp;&amp;
2357         !MacroAssembler::needs_explicit_null_check(offset_type-&gt;_hi)) {
2358       return Type::OopPtr;
2359     }
2360     // Otherwise, it might either be oop+off or NULL+addr.
2361     return Type::AnyPtr;
2362   } else {
2363     // No information:
2364     return Type::AnyPtr;
2365   }
2366 }
2367 
2368 inline Node* LibraryCallKit::make_unsafe_address(Node* base, Node* offset) {
2369   int kind = classify_unsafe_addr(base, offset);
2370   if (kind == Type::RawPtr) {
2371     return basic_plus_adr(top(), base, offset);
2372   } else {
2373     return basic_plus_adr(base, offset);
2374   }
2375 }
2376 
2377 //--------------------------inline_number_methods-----------------------------
2378 // inline int     Integer.numberOfLeadingZeros(int)
2379 // inline int        Long.numberOfLeadingZeros(long)
2380 //
2381 // inline int     Integer.numberOfTrailingZeros(int)
2382 // inline int        Long.numberOfTrailingZeros(long)
2383 //
2384 // inline int     Integer.bitCount(int)
2385 // inline int        Long.bitCount(long)
2386 //
2387 // inline char  Character.reverseBytes(char)
2388 // inline short     Short.reverseBytes(short)
2389 // inline int     Integer.reverseBytes(int)
2390 // inline long       Long.reverseBytes(long)
2391 bool LibraryCallKit::inline_number_methods(vmIntrinsics::ID id) {
2392   Node* arg = argument(0);
2393   Node* n = NULL;
2394   switch (id) {
2395   case vmIntrinsics::_numberOfLeadingZeros_i:   n = new (C) CountLeadingZerosINode( arg);  break;
2396   case vmIntrinsics::_numberOfLeadingZeros_l:   n = new (C) CountLeadingZerosLNode( arg);  break;
2397   case vmIntrinsics::_numberOfTrailingZeros_i:  n = new (C) CountTrailingZerosINode(arg);  break;
2398   case vmIntrinsics::_numberOfTrailingZeros_l:  n = new (C) CountTrailingZerosLNode(arg);  break;
2399   case vmIntrinsics::_bitCount_i:               n = new (C) PopCountINode(          arg);  break;
2400   case vmIntrinsics::_bitCount_l:               n = new (C) PopCountLNode(          arg);  break;
2401   case vmIntrinsics::_reverseBytes_c:           n = new (C) ReverseBytesUSNode(0,   arg);  break;
2402   case vmIntrinsics::_reverseBytes_s:           n = new (C) ReverseBytesSNode( 0,   arg);  break;
2403   case vmIntrinsics::_reverseBytes_i:           n = new (C) ReverseBytesINode( 0,   arg);  break;
2404   case vmIntrinsics::_reverseBytes_l:           n = new (C) ReverseBytesLNode( 0,   arg);  break;
2405   default:  fatal_unexpected_iid(id);  break;
2406   }
2407   set_result(_gvn.transform(n));
2408   return true;
2409 }
2410 
2411 //----------------------------inline_unsafe_access----------------------------
2412 
2413 const static BasicType T_ADDRESS_HOLDER = T_LONG;
2414 
2415 // Helper that guards and inserts a pre-barrier.
2416 void LibraryCallKit::insert_pre_barrier(Node* base_oop, Node* offset,
2417                                         Node* pre_val, bool need_mem_bar) {
2418   // We could be accessing the referent field of a reference object. If so, when G1
2419   // is enabled, we need to log the value in the referent field in an SATB buffer.
2420   // This routine performs some compile time filters and generates suitable
2421   // runtime filters that guard the pre-barrier code.
2422   // Also add memory barrier for non volatile load from the referent field
2423   // to prevent commoning of loads across safepoint.
2424   if (!UseG1GC &amp;&amp; !need_mem_bar)
2425     return;
2426 
2427   // Some compile time checks.
2428 
2429   // If offset is a constant, is it java_lang_ref_Reference::_reference_offset?
2430   const TypeX* otype = offset-&gt;find_intptr_t_type();
2431   if (otype != NULL &amp;&amp; otype-&gt;is_con() &amp;&amp;
2432       otype-&gt;get_con() != java_lang_ref_Reference::referent_offset) {
2433     // Constant offset but not the reference_offset so just return
2434     return;
2435   }
2436 
2437   // We only need to generate the runtime guards for instances.
2438   const TypeOopPtr* btype = base_oop-&gt;bottom_type()-&gt;isa_oopptr();
2439   if (btype != NULL) {
2440     if (btype-&gt;isa_aryptr()) {
2441       // Array type so nothing to do
2442       return;
2443     }
2444 
2445     const TypeInstPtr* itype = btype-&gt;isa_instptr();
2446     if (itype != NULL) {
2447       // Can the klass of base_oop be statically determined to be
2448       // _not_ a sub-class of Reference and _not_ Object?
2449       ciKlass* klass = itype-&gt;klass();
2450       if ( klass-&gt;is_loaded() &amp;&amp;
2451           !klass-&gt;is_subtype_of(env()-&gt;Reference_klass()) &amp;&amp;
2452           !env()-&gt;Object_klass()-&gt;is_subtype_of(klass)) {
2453         return;
2454       }
2455     }
2456   }
2457 
2458   // The compile time filters did not reject base_oop/offset so
2459   // we need to generate the following runtime filters
2460   //
2461   // if (offset == java_lang_ref_Reference::_reference_offset) {
2462   //   if (instance_of(base, java.lang.ref.Reference)) {
2463   //     pre_barrier(_, pre_val, ...);
2464   //   }
2465   // }
2466 
2467   float likely   = PROB_LIKELY(  0.999);
2468   float unlikely = PROB_UNLIKELY(0.999);
2469 
2470   IdealKit ideal(this);
2471 #define __ ideal.
2472 
2473   Node* referent_off = __ ConX(java_lang_ref_Reference::referent_offset);
2474 
2475   __ if_then(offset, BoolTest::eq, referent_off, unlikely); {
2476       // Update graphKit memory and control from IdealKit.
2477       sync_kit(ideal);
2478 
2479       Node* ref_klass_con = makecon(TypeKlassPtr::make(env()-&gt;Reference_klass()));
2480       Node* is_instof = gen_instanceof(base_oop, ref_klass_con);
2481 
2482       // Update IdealKit memory and control from graphKit.
2483       __ sync_kit(this);
2484 
2485       Node* one = __ ConI(1);
2486       // is_instof == 0 if base_oop == NULL
2487       __ if_then(is_instof, BoolTest::eq, one, unlikely); {
2488 
2489         // Update graphKit from IdeakKit.
2490         sync_kit(ideal);
2491 
2492         // Use the pre-barrier to record the value in the referent field
2493         pre_barrier(false /* do_load */,
2494                     __ ctrl(),
2495                     NULL /* obj */, NULL /* adr */, max_juint /* alias_idx */, NULL /* val */, NULL /* val_type */,
2496                     pre_val /* pre_val */,
2497                     T_OBJECT);
2498         if (need_mem_bar) {
2499           // Add memory barrier to prevent commoning reads from this field
2500           // across safepoint since GC can change its value.
2501           insert_mem_bar(Op_MemBarCPUOrder);
2502         }
2503         // Update IdealKit from graphKit.
2504         __ sync_kit(this);
2505 
2506       } __ end_if(); // _ref_type != ref_none
2507   } __ end_if(); // offset == referent_offset
2508 
2509   // Final sync IdealKit and GraphKit.
2510   final_sync(ideal);
2511 #undef __
2512 }
2513 
2514 
2515 // Interpret Unsafe.fieldOffset cookies correctly:
2516 extern jlong Unsafe_field_offset_to_byte_offset(jlong field_offset);
2517 
2518 const TypeOopPtr* LibraryCallKit::sharpen_unsafe_type(Compile::AliasType* alias_type, const TypePtr *adr_type, bool is_native_ptr) {
2519   // Attempt to infer a sharper value type from the offset and base type.
2520   ciKlass* sharpened_klass = NULL;
2521 
2522   // See if it is an instance field, with an object type.
2523   if (alias_type-&gt;field() != NULL) {
2524     assert(!is_native_ptr, "native pointer op cannot use a java address");
2525     if (alias_type-&gt;field()-&gt;type()-&gt;is_klass()) {
2526       sharpened_klass = alias_type-&gt;field()-&gt;type()-&gt;as_klass();
2527     }
2528   }
2529 
2530   // See if it is a narrow oop array.
2531   if (adr_type-&gt;isa_aryptr()) {
2532     if (adr_type-&gt;offset() &gt;= objArrayOopDesc::base_offset_in_bytes()) {
2533       const TypeOopPtr *elem_type = adr_type-&gt;is_aryptr()-&gt;elem()-&gt;isa_oopptr();
2534       if (elem_type != NULL) {
2535         sharpened_klass = elem_type-&gt;klass();
2536       }
2537     }
2538   }
2539 
2540   // The sharpened class might be unloaded if there is no class loader
2541   // contraint in place.
2542   if (sharpened_klass != NULL &amp;&amp; sharpened_klass-&gt;is_loaded()) {
2543     const TypeOopPtr* tjp = TypeOopPtr::make_from_klass(sharpened_klass);
2544 
2545 #ifndef PRODUCT
2546     if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
2547       tty-&gt;print("  from base type: ");  adr_type-&gt;dump();
2548       tty-&gt;print("  sharpened value: ");  tjp-&gt;dump();
2549     }
2550 #endif
2551     // Sharpen the value type.
2552     return tjp;
2553   }
2554   return NULL;
2555 }
2556 
2557 bool LibraryCallKit::inline_unsafe_access(bool is_native_ptr, bool is_store, BasicType type, bool is_volatile, bool unaligned) {
2558   if (callee()-&gt;is_static())  return false;  // caller must have the capability!
2559   assert(type != T_OBJECT || !unaligned, "unaligned access not supported with object type");
2560 
2561 #ifndef PRODUCT
2562   {
2563     ResourceMark rm;
2564     // Check the signatures.
2565     ciSignature* sig = callee()-&gt;signature();
2566 #ifdef ASSERT
2567     if (!is_store) {
2568       // Object getObject(Object base, int/long offset), etc.
2569       BasicType rtype = sig-&gt;return_type()-&gt;basic_type();
2570       if (rtype == T_ADDRESS_HOLDER &amp;&amp; callee()-&gt;name() == ciSymbol::getAddress_name())
2571           rtype = T_ADDRESS;  // it is really a C void*
2572       assert(rtype == type, "getter must return the expected value");
2573       if (!is_native_ptr) {
2574         assert(sig-&gt;count() == 2, "oop getter has 2 arguments");
2575         assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "getter base is object");
2576         assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, "getter offset is correct");
2577       } else {
2578         assert(sig-&gt;count() == 1, "native getter has 1 argument");
2579         assert(sig-&gt;type_at(0)-&gt;basic_type() == T_LONG, "getter base is long");
2580       }
2581     } else {
2582       // void putObject(Object base, int/long offset, Object x), etc.
2583       assert(sig-&gt;return_type()-&gt;basic_type() == T_VOID, "putter must not return a value");
2584       if (!is_native_ptr) {
2585         assert(sig-&gt;count() == 3, "oop putter has 3 arguments");
2586         assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "putter base is object");
2587         assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, "putter offset is correct");
2588       } else {
2589         assert(sig-&gt;count() == 2, "native putter has 2 arguments");
2590         assert(sig-&gt;type_at(0)-&gt;basic_type() == T_LONG, "putter base is long");
2591       }
2592       BasicType vtype = sig-&gt;type_at(sig-&gt;count()-1)-&gt;basic_type();
2593       if (vtype == T_ADDRESS_HOLDER &amp;&amp; callee()-&gt;name() == ciSymbol::putAddress_name())
2594         vtype = T_ADDRESS;  // it is really a C void*
2595       assert(vtype == type, "putter must accept the expected value");
2596     }
2597 #endif // ASSERT
2598  }
2599 #endif //PRODUCT
2600 
2601   C-&gt;set_has_unsafe_access(true);  // Mark eventual nmethod as "unsafe".
2602 
2603   Node* receiver = argument(0);  // type: oop
2604 
2605   // Build address expression.  See the code in inline_unsafe_prefetch.
2606   Node* adr;
2607   Node* heap_base_oop = top();
2608   Node* offset = top();
2609   Node* val;
2610 
2611   if (!is_native_ptr) {
2612     // The base is either a Java object or a value produced by Unsafe.staticFieldBase
2613     Node* base = argument(1);  // type: oop
2614     // The offset is a value produced by Unsafe.staticFieldOffset or Unsafe.objectFieldOffset
2615     offset = argument(2);  // type: long
2616     // We currently rely on the cookies produced by Unsafe.xxxFieldOffset
2617     // to be plain byte offsets, which are also the same as those accepted
2618     // by oopDesc::field_base.
2619     assert(Unsafe_field_offset_to_byte_offset(11) == 11,
2620            "fieldOffset must be byte-scaled");
2621     // 32-bit machines ignore the high half!
2622     offset = ConvL2X(offset);
2623     adr = make_unsafe_address(base, offset);
2624     heap_base_oop = base;
2625     val = is_store ? argument(4) : NULL;
2626   } else {
2627     Node* ptr = argument(1);  // type: long
2628     ptr = ConvL2X(ptr);  // adjust Java long to machine word
2629     adr = make_unsafe_address(NULL, ptr);
2630     val = is_store ? argument(3) : NULL;
2631   }
2632 
2633   // Can base be NULL? Otherwise, always on-heap access.
2634   bool can_access_non_heap = TypePtr::NULL_PTR-&gt;higher_equal(_gvn.type(heap_base_oop));
2635 
2636   const TypePtr *adr_type = _gvn.type(adr)-&gt;isa_ptr();
2637 
2638   // Try to categorize the address.
2639   Compile::AliasType* alias_type = C-&gt;alias_type(adr_type);
2640   assert(alias_type-&gt;index() != Compile::AliasIdxBot, "no bare pointers here");
2641 
2642   if (alias_type-&gt;adr_type() == TypeInstPtr::KLASS ||
2643       alias_type-&gt;adr_type() == TypeAryPtr::RANGE) {
2644     return false; // not supported
2645   }
2646 
2647   bool mismatched = false;
2648   BasicType bt = alias_type-&gt;basic_type();
2649   if (bt != T_ILLEGAL) {
2650     assert(alias_type-&gt;adr_type()-&gt;is_oopptr(), "should be on-heap access");
2651     if (bt == T_BYTE &amp;&amp; adr_type-&gt;isa_aryptr()) {
2652       // Alias type doesn't differentiate between byte[] and boolean[]).
2653       // Use address type to get the element type.
2654       bt = adr_type-&gt;is_aryptr()-&gt;elem()-&gt;array_element_basic_type();
2655     }
2656     if (bt == T_ARRAY || bt == T_NARROWOOP) {
2657       // accessing an array field with getObject is not a mismatch
2658       bt = T_OBJECT;
2659     }
2660     if ((bt == T_OBJECT) != (type == T_OBJECT)) {
2661       // Don't intrinsify mismatched object accesses
2662       return false;
2663     }
2664     mismatched = (bt != type);
2665   }
2666 
2667   assert(!mismatched || alias_type-&gt;adr_type()-&gt;is_oopptr(), "off-heap access can't be mismatched");
2668 
2669   // First guess at the value type.
2670   const Type *value_type = Type::get_const_basic_type(type);
2671 
2672   // We will need memory barriers unless we can determine a unique
2673   // alias category for this reference.  (Note:  If for some reason
2674   // the barriers get omitted and the unsafe reference begins to "pollute"
2675   // the alias analysis of the rest of the graph, either Compile::can_alias
2676   // or Compile::must_alias will throw a diagnostic assert.)
2677   bool need_mem_bar = (alias_type-&gt;adr_type() == TypeOopPtr::BOTTOM);
2678 
2679   // If we are reading the value of the referent field of a Reference
2680   // object (either by using Unsafe directly or through reflection)
2681   // then, if G1 is enabled, we need to record the referent in an
2682   // SATB log buffer using the pre-barrier mechanism.
2683   // Also we need to add memory barrier to prevent commoning reads
2684   // from this field across safepoint since GC can change its value.
2685   bool need_read_barrier = !is_native_ptr &amp;&amp; !is_store &amp;&amp;
2686                            offset != top() &amp;&amp; heap_base_oop != top();
2687 
2688   if (!is_store &amp;&amp; type == T_OBJECT) {
2689     const TypeOopPtr* tjp = sharpen_unsafe_type(alias_type, adr_type, is_native_ptr);
2690     if (tjp != NULL) {
2691       value_type = tjp;
2692     }
2693   }
2694 
2695   receiver = null_check(receiver);
2696   if (stopped()) {
2697     return true;
2698   }
2699   // Heap pointers get a null-check from the interpreter,
2700   // as a courtesy.  However, this is not guaranteed by Unsafe,
2701   // and it is not possible to fully distinguish unintended nulls
2702   // from intended ones in this API.
2703 
2704   if (is_volatile) {
2705     // We need to emit leading and trailing CPU membars (see below) in
2706     // addition to memory membars when is_volatile. This is a little
2707     // too strong, but avoids the need to insert per-alias-type
2708     // volatile membars (for stores; compare Parse::do_put_xxx), which
2709     // we cannot do effectively here because we probably only have a
2710     // rough approximation of type.
2711     need_mem_bar = true;
2712     // For Stores, place a memory ordering barrier now.
2713     if (is_store) {
2714       insert_mem_bar(Op_MemBarRelease);
2715     } else {
2716       if (support_IRIW_for_not_multiple_copy_atomic_cpu) {
2717         insert_mem_bar(Op_MemBarVolatile);
2718       }
2719     }
2720   }
2721 
2722   // Memory barrier to prevent normal and 'unsafe' accesses from
2723   // bypassing each other.  Happens after null checks, so the
2724   // exception paths do not take memory state from the memory barrier,
2725   // so there's no problems making a strong assert about mixing users
2726   // of safe &amp; unsafe memory.  Otherwise fails in a CTW of rt.jar
2727   // around 5701, class sun/reflect/UnsafeBooleanFieldAccessorImpl.
2728   if (need_mem_bar) insert_mem_bar(Op_MemBarCPUOrder);
2729 
2730   if (!is_store) {
2731     MemNode::MemOrd mo = is_volatile ? MemNode::acquire : MemNode::unordered;
2732     // To be valid, unsafe loads may depend on other conditions than
2733     // the one that guards them: pin the Load node
2734     Node* p = make_load(control(), adr, value_type, type, adr_type, mo, LoadNode::Pinned, is_volatile, unaligned, mismatched);
2735     // load value
2736     switch (type) {
2737     case T_BOOLEAN:
2738     case T_CHAR:
2739     case T_BYTE:
2740     case T_SHORT:
2741     case T_INT:
2742     case T_LONG:
2743     case T_FLOAT:
2744     case T_DOUBLE:
2745       break;
2746     case T_OBJECT:
2747       if (need_read_barrier) {
2748         insert_pre_barrier(heap_base_oop, offset, p, !(is_volatile || need_mem_bar));
2749       }
2750       break;
2751     case T_ADDRESS:
2752       // Cast to an int type.
2753       p = _gvn.transform(new (C) CastP2XNode(NULL, p));
2754       p = ConvX2UL(p);
2755       break;
2756     default:
2757       fatal(err_msg_res("unexpected type %d: %s", type, type2name(type)));
2758       break;
2759     }
2760     // The load node has the control of the preceding MemBarCPUOrder.  All
2761     // following nodes will have the control of the MemBarCPUOrder inserted at
2762     // the end of this method.  So, pushing the load onto the stack at a later
2763     // point is fine.
2764     set_result(p);
2765   } else {
2766     // place effect of store into memory
2767     switch (type) {
2768     case T_DOUBLE:
2769       val = dstore_rounding(val);
2770       break;
2771     case T_ADDRESS:
2772       // Repackage the long as a pointer.
2773       val = ConvL2X(val);
2774       val = _gvn.transform(new (C) CastX2PNode(val));
2775       break;
2776     }
2777 
2778     MemNode::MemOrd mo = is_volatile ? MemNode::release : MemNode::unordered;
2779     if (type != T_OBJECT ) {
2780       (void) store_to_memory(control(), adr, val, type, adr_type, mo, is_volatile, unaligned, mismatched);
2781     } else {
2782       // Possibly an oop being stored to Java heap or native memory
2783       if (!can_access_non_heap) {
2784         // oop to Java heap.
2785         (void) store_oop_to_unknown(control(), heap_base_oop, adr, adr_type, val, type, mo, mismatched);
2786       } else {
2787         // We can't tell at compile time if we are storing in the Java heap or outside
2788         // of it. So we need to emit code to conditionally do the proper type of
2789         // store.
2790 
2791         IdealKit ideal(this);
2792 #define __ ideal.
2793         // QQQ who knows what probability is here??
2794         __ if_then(heap_base_oop, BoolTest::ne, null(), PROB_UNLIKELY(0.999)); {
2795           // Sync IdealKit and graphKit.
2796           sync_kit(ideal);
2797           Node* st = store_oop_to_unknown(control(), heap_base_oop, adr, adr_type, val, type, mo, mismatched);
2798           // Update IdealKit memory.
2799           __ sync_kit(this);
2800         } __ else_(); {
2801           __ store(__ ctrl(), adr, val, type, alias_type-&gt;index(), mo, is_volatile, mismatched);
2802         } __ end_if();
2803         // Final sync IdealKit and GraphKit.
2804         final_sync(ideal);
2805 #undef __
2806       }
2807     }
2808   }
2809 
2810   if (is_volatile) {
2811     if (!is_store) {
2812       insert_mem_bar(Op_MemBarAcquire);
2813     } else {
2814       if (!support_IRIW_for_not_multiple_copy_atomic_cpu) {
2815         insert_mem_bar(Op_MemBarVolatile);
2816       }
2817     }
2818   }
2819 
2820   if (need_mem_bar) insert_mem_bar(Op_MemBarCPUOrder);
2821 
2822   return true;
2823 }
2824 
2825 //----------------------------inline_unsafe_prefetch----------------------------
2826 
2827 bool LibraryCallKit::inline_unsafe_prefetch(bool is_native_ptr, bool is_store, bool is_static) {
2828 #ifndef PRODUCT
2829   {
2830     ResourceMark rm;
2831     // Check the signatures.
2832     ciSignature* sig = callee()-&gt;signature();
2833 #ifdef ASSERT
2834     // Object getObject(Object base, int/long offset), etc.
2835     BasicType rtype = sig-&gt;return_type()-&gt;basic_type();
2836     if (!is_native_ptr) {
2837       assert(sig-&gt;count() == 2, "oop prefetch has 2 arguments");
2838       assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "prefetch base is object");
2839       assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, "prefetcha offset is correct");
2840     } else {
2841       assert(sig-&gt;count() == 1, "native prefetch has 1 argument");
2842       assert(sig-&gt;type_at(0)-&gt;basic_type() == T_LONG, "prefetch base is long");
2843     }
2844 #endif // ASSERT
2845   }
2846 #endif // !PRODUCT
2847 
2848   C-&gt;set_has_unsafe_access(true);  // Mark eventual nmethod as "unsafe".
2849 
2850   const int idx = is_static ? 0 : 1;
2851   if (!is_static) {
2852     null_check_receiver();
2853     if (stopped()) {
2854       return true;
2855     }
2856   }
2857 
2858   // Build address expression.  See the code in inline_unsafe_access.
2859   Node *adr;
2860   if (!is_native_ptr) {
2861     // The base is either a Java object or a value produced by Unsafe.staticFieldBase
2862     Node* base   = argument(idx + 0);  // type: oop
2863     // The offset is a value produced by Unsafe.staticFieldOffset or Unsafe.objectFieldOffset
2864     Node* offset = argument(idx + 1);  // type: long
2865     // We currently rely on the cookies produced by Unsafe.xxxFieldOffset
2866     // to be plain byte offsets, which are also the same as those accepted
2867     // by oopDesc::field_base.
2868     assert(Unsafe_field_offset_to_byte_offset(11) == 11,
2869            "fieldOffset must be byte-scaled");
2870     // 32-bit machines ignore the high half!
2871     offset = ConvL2X(offset);
2872     adr = make_unsafe_address(base, offset);
2873   } else {
2874     Node* ptr = argument(idx + 0);  // type: long
2875     ptr = ConvL2X(ptr);  // adjust Java long to machine word
2876     adr = make_unsafe_address(NULL, ptr);
2877   }
2878 
2879   // Generate the read or write prefetch
2880   Node *prefetch;
2881   if (is_store) {
2882     prefetch = new (C) PrefetchWriteNode(i_o(), adr);
2883   } else {
2884     prefetch = new (C) PrefetchReadNode(i_o(), adr);
2885   }
2886   prefetch-&gt;init_req(0, control());
2887   set_i_o(_gvn.transform(prefetch));
2888 
2889   return true;
2890 }
2891 
2892 //----------------------------inline_unsafe_load_store----------------------------
2893 // This method serves a couple of different customers (depending on LoadStoreKind):
2894 //
2895 // LS_cmpxchg:
2896 //   public final native boolean compareAndSwapObject(Object o, long offset, Object expected, Object x);
2897 //   public final native boolean compareAndSwapInt(   Object o, long offset, int    expected, int    x);
2898 //   public final native boolean compareAndSwapLong(  Object o, long offset, long   expected, long   x);
2899 //
2900 // LS_xadd:
2901 //   public int  getAndAddInt( Object o, long offset, int  delta)
2902 //   public long getAndAddLong(Object o, long offset, long delta)
2903 //
2904 // LS_xchg:
2905 //   int    getAndSet(Object o, long offset, int    newValue)
2906 //   long   getAndSet(Object o, long offset, long   newValue)
2907 //   Object getAndSet(Object o, long offset, Object newValue)
2908 //
2909 bool LibraryCallKit::inline_unsafe_load_store(BasicType type, LoadStoreKind kind) {
2910   // This basic scheme here is the same as inline_unsafe_access, but
2911   // differs in enough details that combining them would make the code
2912   // overly confusing.  (This is a true fact! I originally combined
2913   // them, but even I was confused by it!) As much code/comments as
2914   // possible are retained from inline_unsafe_access though to make
2915   // the correspondences clearer. - dl
2916 
2917   if (callee()-&gt;is_static())  return false;  // caller must have the capability!
2918 
2919 #ifndef PRODUCT
2920   BasicType rtype;
2921   {
2922     ResourceMark rm;
2923     // Check the signatures.
2924     ciSignature* sig = callee()-&gt;signature();
2925     rtype = sig-&gt;return_type()-&gt;basic_type();
2926     if (kind == LS_xadd || kind == LS_xchg) {
2927       // Check the signatures.
2928 #ifdef ASSERT
2929       assert(rtype == type, "get and set must return the expected type");
2930       assert(sig-&gt;count() == 3, "get and set has 3 arguments");
2931       assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "get and set base is object");
2932       assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, "get and set offset is long");
2933       assert(sig-&gt;type_at(2)-&gt;basic_type() == type, "get and set must take expected type as new value/delta");
2934 #endif // ASSERT
2935     } else if (kind == LS_cmpxchg) {
2936       // Check the signatures.
2937 #ifdef ASSERT
2938       assert(rtype == T_BOOLEAN, "CAS must return boolean");
2939       assert(sig-&gt;count() == 4, "CAS has 4 arguments");
2940       assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "CAS base is object");
2941       assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, "CAS offset is long");
2942 #endif // ASSERT
2943     } else {
2944       ShouldNotReachHere();
2945     }
2946   }
2947 #endif //PRODUCT
2948 
2949   C-&gt;set_has_unsafe_access(true);  // Mark eventual nmethod as "unsafe".
2950 
2951   // Get arguments:
2952   Node* receiver = NULL;
2953   Node* base     = NULL;
2954   Node* offset   = NULL;
2955   Node* oldval   = NULL;
2956   Node* newval   = NULL;
2957   if (kind == LS_cmpxchg) {
2958     const bool two_slot_type = type2size[type] == 2;
2959     receiver = argument(0);  // type: oop
2960     base     = argument(1);  // type: oop
2961     offset   = argument(2);  // type: long
2962     oldval   = argument(4);  // type: oop, int, or long
2963     newval   = argument(two_slot_type ? 6 : 5);  // type: oop, int, or long
2964   } else if (kind == LS_xadd || kind == LS_xchg){
2965     receiver = argument(0);  // type: oop
2966     base     = argument(1);  // type: oop
2967     offset   = argument(2);  // type: long
2968     oldval   = NULL;
2969     newval   = argument(4);  // type: oop, int, or long
2970   }
2971 
2972   // Build field offset expression.
2973   // We currently rely on the cookies produced by Unsafe.xxxFieldOffset
2974   // to be plain byte offsets, which are also the same as those accepted
2975   // by oopDesc::field_base.
2976   assert(Unsafe_field_offset_to_byte_offset(11) == 11, "fieldOffset must be byte-scaled");
2977   // 32-bit machines ignore the high half of long offsets
2978   offset = ConvL2X(offset);
2979   Node* adr = make_unsafe_address(base, offset);
2980   const TypePtr *adr_type = _gvn.type(adr)-&gt;isa_ptr();
2981 
2982   Compile::AliasType* alias_type = C-&gt;alias_type(adr_type);
2983   BasicType bt = alias_type-&gt;basic_type();
2984   if (bt != T_ILLEGAL &amp;&amp;
2985       ((bt == T_OBJECT || bt == T_ARRAY) != (type == T_OBJECT))) {
2986     // Don't intrinsify mismatched object accesses.
2987     return false;
2988   }
2989 
2990   // For CAS, unlike inline_unsafe_access, there seems no point in
2991   // trying to refine types. Just use the coarse types here.
2992   assert(alias_type-&gt;index() != Compile::AliasIdxBot, "no bare pointers here");
2993   const Type *value_type = Type::get_const_basic_type(type);
2994 
2995   if (kind == LS_xchg &amp;&amp; type == T_OBJECT) {
2996     const TypeOopPtr* tjp = sharpen_unsafe_type(alias_type, adr_type);
2997     if (tjp != NULL) {
2998       value_type = tjp;
2999     }
3000   }
3001 
3002   // Null check receiver.
3003   receiver = null_check(receiver);
3004   if (stopped()) {
3005     return true;
3006   }
3007 
3008   int alias_idx = C-&gt;get_alias_index(adr_type);
3009 
3010   // Memory-model-wise, a LoadStore acts like a little synchronized
3011   // block, so needs barriers on each side.  These don't translate
3012   // into actual barriers on most machines, but we still need rest of
3013   // compiler to respect ordering.
3014 
3015   insert_mem_bar(Op_MemBarRelease);
3016   insert_mem_bar(Op_MemBarCPUOrder);
3017 
3018   // 4984716: MemBars must be inserted before this
3019   //          memory node in order to avoid a false
3020   //          dependency which will confuse the scheduler.
3021   Node *mem = memory(alias_idx);
3022 
3023   // For now, we handle only those cases that actually exist: ints,
3024   // longs, and Object. Adding others should be straightforward.
3025   Node* load_store = NULL;
3026   switch(type) {
3027   case T_INT:
3028     if (kind == LS_xadd) {
3029       load_store = _gvn.transform(new (C) GetAndAddINode(control(), mem, adr, newval, adr_type));
3030     } else if (kind == LS_xchg) {
3031       load_store = _gvn.transform(new (C) GetAndSetINode(control(), mem, adr, newval, adr_type));
3032     } else if (kind == LS_cmpxchg) {
3033       load_store = _gvn.transform(new (C) CompareAndSwapINode(control(), mem, adr, newval, oldval));
3034     } else {
3035       ShouldNotReachHere();
3036     }
3037     break;
3038   case T_LONG:
3039     if (kind == LS_xadd) {
3040       load_store = _gvn.transform(new (C) GetAndAddLNode(control(), mem, adr, newval, adr_type));
3041     } else if (kind == LS_xchg) {
3042       load_store = _gvn.transform(new (C) GetAndSetLNode(control(), mem, adr, newval, adr_type));
3043     } else if (kind == LS_cmpxchg) {
3044       load_store = _gvn.transform(new (C) CompareAndSwapLNode(control(), mem, adr, newval, oldval));
3045     } else {
3046       ShouldNotReachHere();
3047     }
3048     break;
3049   case T_OBJECT:
3050     // Transformation of a value which could be NULL pointer (CastPP #NULL)
3051     // could be delayed during Parse (for example, in adjust_map_after_if()).
3052     // Execute transformation here to avoid barrier generation in such case.
3053     if (_gvn.type(newval) == TypePtr::NULL_PTR)
3054       newval = _gvn.makecon(TypePtr::NULL_PTR);
3055 
3056     // Reference stores need a store barrier.
3057     if (kind == LS_xchg) {
3058       // If pre-barrier must execute before the oop store, old value will require do_load here.
3059       if (!can_move_pre_barrier()) {
3060         pre_barrier(true /* do_load*/,
3061                     control(), base, adr, alias_idx, newval, value_type-&gt;make_oopptr(),
3062                     NULL /* pre_val*/,
3063                     T_OBJECT);
3064       } // Else move pre_barrier to use load_store value, see below.
3065     } else if (kind == LS_cmpxchg) {
3066       // Same as for newval above:
3067       if (_gvn.type(oldval) == TypePtr::NULL_PTR) {
3068         oldval = _gvn.makecon(TypePtr::NULL_PTR);
3069       }
3070       // The only known value which might get overwritten is oldval.
3071       pre_barrier(false /* do_load */,
3072                   control(), NULL, NULL, max_juint, NULL, NULL,
3073                   oldval /* pre_val */,
3074                   T_OBJECT);
3075     } else {
3076       ShouldNotReachHere();
3077     }
3078 
3079 #ifdef _LP64
3080     if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
3081       Node *newval_enc = _gvn.transform(new (C) EncodePNode(newval, newval-&gt;bottom_type()-&gt;make_narrowoop()));
3082       if (kind == LS_xchg) {
3083         load_store = _gvn.transform(new (C) GetAndSetNNode(control(), mem, adr,
3084                                                            newval_enc, adr_type, value_type-&gt;make_narrowoop()));
3085       } else {
3086         assert(kind == LS_cmpxchg, "wrong LoadStore operation");
3087         Node *oldval_enc = _gvn.transform(new (C) EncodePNode(oldval, oldval-&gt;bottom_type()-&gt;make_narrowoop()));
3088         load_store = _gvn.transform(new (C) CompareAndSwapNNode(control(), mem, adr,
3089                                                                 newval_enc, oldval_enc));
3090       }
3091     } else
3092 #endif
3093     {
3094       if (kind == LS_xchg) {
3095         load_store = _gvn.transform(new (C) GetAndSetPNode(control(), mem, adr, newval, adr_type, value_type-&gt;is_oopptr()));
3096       } else {
3097         assert(kind == LS_cmpxchg, "wrong LoadStore operation");
3098         load_store = _gvn.transform(new (C) CompareAndSwapPNode(control(), mem, adr, newval, oldval));
3099       }
3100     }
3101     post_barrier(control(), load_store, base, adr, alias_idx, newval, T_OBJECT, true);
3102     break;
3103   default:
3104     fatal(err_msg_res("unexpected type %d: %s", type, type2name(type)));
3105     break;
3106   }
3107 
3108   // SCMemProjNodes represent the memory state of a LoadStore. Their
3109   // main role is to prevent LoadStore nodes from being optimized away
3110   // when their results aren't used.
3111   Node* proj = _gvn.transform(new (C) SCMemProjNode(load_store));
3112   set_memory(proj, alias_idx);
3113 
3114   if (type == T_OBJECT &amp;&amp; kind == LS_xchg) {
3115 #ifdef _LP64
3116     if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
3117       load_store = _gvn.transform(new (C) DecodeNNode(load_store, load_store-&gt;get_ptr_type()));
3118     }
3119 #endif
3120     if (can_move_pre_barrier()) {
3121       // Don't need to load pre_val. The old value is returned by load_store.
3122       // The pre_barrier can execute after the xchg as long as no safepoint
3123       // gets inserted between them.
3124       pre_barrier(false /* do_load */,
3125                   control(), NULL, NULL, max_juint, NULL, NULL,
3126                   load_store /* pre_val */,
3127                   T_OBJECT);
3128     }
3129   }
3130 
3131   // Add the trailing membar surrounding the access
3132   insert_mem_bar(Op_MemBarCPUOrder);
3133   insert_mem_bar(Op_MemBarAcquire);
3134 
3135   assert(type2size[load_store-&gt;bottom_type()-&gt;basic_type()] == type2size[rtype], "result type should match");
3136   set_result(load_store);
3137   return true;
3138 }
3139 
3140 //----------------------------inline_unsafe_ordered_store----------------------
3141 // public native void sun.misc.Unsafe.putOrderedObject(Object o, long offset, Object x);
3142 // public native void sun.misc.Unsafe.putOrderedInt(Object o, long offset, int x);
3143 // public native void sun.misc.Unsafe.putOrderedLong(Object o, long offset, long x);
3144 bool LibraryCallKit::inline_unsafe_ordered_store(BasicType type) {
3145   // This is another variant of inline_unsafe_access, differing in
3146   // that it always issues store-store ("release") barrier and ensures
3147   // store-atomicity (which only matters for "long").
3148 
3149   if (callee()-&gt;is_static())  return false;  // caller must have the capability!
3150 
3151 #ifndef PRODUCT
3152   {
3153     ResourceMark rm;
3154     // Check the signatures.
3155     ciSignature* sig = callee()-&gt;signature();
3156 #ifdef ASSERT
3157     BasicType rtype = sig-&gt;return_type()-&gt;basic_type();
3158     assert(rtype == T_VOID, "must return void");
3159     assert(sig-&gt;count() == 3, "has 3 arguments");
3160     assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "base is object");
3161     assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, "offset is long");
3162 #endif // ASSERT
3163   }
3164 #endif //PRODUCT
3165 
3166   C-&gt;set_has_unsafe_access(true);  // Mark eventual nmethod as "unsafe".
3167 
3168   // Get arguments:
3169   Node* receiver = argument(0);  // type: oop
3170   Node* base     = argument(1);  // type: oop
3171   Node* offset   = argument(2);  // type: long
3172   Node* val      = argument(4);  // type: oop, int, or long
3173 
3174   // Null check receiver.
3175   receiver = null_check(receiver);
3176   if (stopped()) {
3177     return true;
3178   }
3179 
3180   // Build field offset expression.
3181   assert(Unsafe_field_offset_to_byte_offset(11) == 11, "fieldOffset must be byte-scaled");
3182   // 32-bit machines ignore the high half of long offsets
3183   offset = ConvL2X(offset);
3184   Node* adr = make_unsafe_address(base, offset);
3185   const TypePtr *adr_type = _gvn.type(adr)-&gt;isa_ptr();
3186   const Type *value_type = Type::get_const_basic_type(type);
3187   Compile::AliasType* alias_type = C-&gt;alias_type(adr_type);
3188 
3189   insert_mem_bar(Op_MemBarRelease);
3190   insert_mem_bar(Op_MemBarCPUOrder);
3191   // Ensure that the store is atomic for longs:
3192   const bool require_atomic_access = true;
3193   Node* store;
3194   if (type == T_OBJECT) // reference stores need a store barrier.
3195     store = store_oop_to_unknown(control(), base, adr, adr_type, val, type, MemNode::release);
3196   else {
3197     store = store_to_memory(control(), adr, val, type, adr_type, MemNode::release, require_atomic_access);
3198   }
3199   insert_mem_bar(Op_MemBarCPUOrder);
3200   return true;
3201 }
3202 
3203 bool LibraryCallKit::inline_unsafe_fence(vmIntrinsics::ID id) {
3204   // Regardless of form, don't allow previous ld/st to move down,
3205   // then issue acquire, release, or volatile mem_bar.
3206   insert_mem_bar(Op_MemBarCPUOrder);
3207   switch(id) {
3208     case vmIntrinsics::_loadFence:
3209       insert_mem_bar(Op_LoadFence);
3210       return true;
3211     case vmIntrinsics::_storeFence:
3212       insert_mem_bar(Op_StoreFence);
3213       return true;
3214     case vmIntrinsics::_fullFence:
3215       insert_mem_bar(Op_MemBarVolatile);
3216       return true;
3217     default:
3218       fatal_unexpected_iid(id);
3219       return false;
3220   }
3221 }
3222 
3223 bool LibraryCallKit::klass_needs_init_guard(Node* kls) {
3224   if (!kls-&gt;is_Con()) {
3225     return true;
3226   }
3227   const TypeKlassPtr* klsptr = kls-&gt;bottom_type()-&gt;isa_klassptr();
3228   if (klsptr == NULL) {
3229     return true;
3230   }
3231   ciInstanceKlass* ik = klsptr-&gt;klass()-&gt;as_instance_klass();
3232   // don't need a guard for a klass that is already initialized
3233   return !ik-&gt;is_initialized();
3234 }
3235 
3236 //----------------------------inline_unsafe_allocate---------------------------
3237 // public native Object sun.misc.Unsafe.allocateInstance(Class&lt;?&gt; cls);
3238 bool LibraryCallKit::inline_unsafe_allocate() {
3239   if (callee()-&gt;is_static())  return false;  // caller must have the capability!
3240 
3241   null_check_receiver();  // null-check, then ignore
3242   Node* cls = null_check(argument(1));
3243   if (stopped())  return true;
3244 
3245   Node* kls = load_klass_from_mirror(cls, false, NULL, 0);
3246   kls = null_check(kls);
3247   if (stopped())  return true;  // argument was like int.class
3248 
3249   Node* test = NULL;
3250   if (LibraryCallKit::klass_needs_init_guard(kls)) {
3251     // Note:  The argument might still be an illegal value like
3252     // Serializable.class or Object[].class.   The runtime will handle it.
3253     // But we must make an explicit check for initialization.
3254     Node* insp = basic_plus_adr(kls, in_bytes(InstanceKlass::init_state_offset()));
3255     // Use T_BOOLEAN for InstanceKlass::_init_state so the compiler
3256     // can generate code to load it as unsigned byte.
3257     Node* inst = make_load(NULL, insp, TypeInt::UBYTE, T_BOOLEAN, MemNode::unordered);
3258     Node* bits = intcon(InstanceKlass::fully_initialized);
3259     test = _gvn.transform(new (C) SubINode(inst, bits));
3260     // The 'test' is non-zero if we need to take a slow path.
3261   }
3262 
3263   Node* obj = new_instance(kls, test);
3264   set_result(obj);
3265   return true;
3266 }
3267 
3268 #ifdef TRACE_HAVE_INTRINSICS
3269 /*
3270  * oop -&gt; myklass
3271  * myklass-&gt;trace_id |= USED
3272  * return myklass-&gt;trace_id &amp; ~0x3
3273  */
3274 bool LibraryCallKit::inline_native_classID() {
3275   null_check_receiver();  // null-check, then ignore
3276   Node* cls = null_check(argument(1), T_OBJECT);
3277   Node* kls = load_klass_from_mirror(cls, false, NULL, 0);
3278   kls = null_check(kls, T_OBJECT);
3279   ByteSize offset = TRACE_ID_OFFSET;
3280   Node* insp = basic_plus_adr(kls, in_bytes(offset));
3281   Node* tvalue = make_load(NULL, insp, TypeLong::LONG, T_LONG, MemNode::unordered);
3282   Node* bits = longcon(~0x03l); // ignore bit 0 &amp; 1
3283   Node* andl = _gvn.transform(new (C) AndLNode(tvalue, bits));
3284   Node* clsused = longcon(0x01l); // set the class bit
3285   Node* orl = _gvn.transform(new (C) OrLNode(tvalue, clsused));
3286 
3287   const TypePtr *adr_type = _gvn.type(insp)-&gt;isa_ptr();
3288   store_to_memory(control(), insp, orl, T_LONG, adr_type, MemNode::unordered);
3289   set_result(andl);
3290   return true;
3291 }
3292 
3293 bool LibraryCallKit::inline_native_threadID() {
3294   Node* tls_ptr = NULL;
3295   Node* cur_thr = generate_current_thread(tls_ptr);
3296   Node* p = basic_plus_adr(top()/*!oop*/, tls_ptr, in_bytes(JavaThread::osthread_offset()));
3297   Node* osthread = make_load(NULL, p, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);
3298   p = basic_plus_adr(top()/*!oop*/, osthread, in_bytes(OSThread::thread_id_offset()));
3299 
3300   Node* threadid = NULL;
3301   size_t thread_id_size = OSThread::thread_id_size();
3302   if (thread_id_size == (size_t) BytesPerLong) {
3303     threadid = ConvL2I(make_load(control(), p, TypeLong::LONG, T_LONG, MemNode::unordered));
3304   } else if (thread_id_size == (size_t) BytesPerInt) {
3305     threadid = make_load(control(), p, TypeInt::INT, T_INT, MemNode::unordered);
3306   } else {
3307     ShouldNotReachHere();
3308   }
3309   set_result(threadid);
3310   return true;
3311 }
3312 #endif
3313 
3314 //------------------------inline_native_time_funcs--------------
3315 // inline code for System.currentTimeMillis() and System.nanoTime()
3316 // these have the same type and signature
3317 bool LibraryCallKit::inline_native_time_funcs(address funcAddr, const char* funcName) {
3318   const TypeFunc* tf = OptoRuntime::void_long_Type();
3319   const TypePtr* no_memory_effects = NULL;
3320   Node* time = make_runtime_call(RC_LEAF, tf, funcAddr, funcName, no_memory_effects);
3321   Node* value = _gvn.transform(new (C) ProjNode(time, TypeFunc::Parms+0));
3322 #ifdef ASSERT
3323   Node* value_top = _gvn.transform(new (C) ProjNode(time, TypeFunc::Parms+1));
3324   assert(value_top == top(), "second value must be top");
3325 #endif
3326   set_result(value);
3327   return true;
3328 }
3329 
3330 //------------------------inline_native_currentThread------------------
3331 bool LibraryCallKit::inline_native_currentThread() {
3332   Node* junk = NULL;
3333   set_result(generate_current_thread(junk));
3334   return true;
3335 }
3336 
3337 //------------------------inline_native_isInterrupted------------------
3338 // private native boolean java.lang.Thread.isInterrupted(boolean ClearInterrupted);
3339 bool LibraryCallKit::inline_native_isInterrupted() {
3340   // Add a fast path to t.isInterrupted(clear_int):
3341   //   (t == Thread.current() &amp;&amp;
3342   //    (!TLS._osthread._interrupted || WINDOWS_ONLY(false) NOT_WINDOWS(!clear_int)))
3343   //   ? TLS._osthread._interrupted : /*slow path:*/ t.isInterrupted(clear_int)
3344   // So, in the common case that the interrupt bit is false,
3345   // we avoid making a call into the VM.  Even if the interrupt bit
3346   // is true, if the clear_int argument is false, we avoid the VM call.
3347   // However, if the receiver is not currentThread, we must call the VM,
3348   // because there must be some locking done around the operation.
3349 
3350   // We only go to the fast case code if we pass two guards.
3351   // Paths which do not pass are accumulated in the slow_region.
3352 
3353   enum {
3354     no_int_result_path   = 1, // t == Thread.current() &amp;&amp; !TLS._osthread._interrupted
3355     no_clear_result_path = 2, // t == Thread.current() &amp;&amp;  TLS._osthread._interrupted &amp;&amp; !clear_int
3356     slow_result_path     = 3, // slow path: t.isInterrupted(clear_int)
3357     PATH_LIMIT
3358   };
3359 
3360   // Ensure that it's not possible to move the load of TLS._osthread._interrupted flag
3361   // out of the function.
3362   insert_mem_bar(Op_MemBarCPUOrder);
3363 
3364   RegionNode* result_rgn = new (C) RegionNode(PATH_LIMIT);
3365   PhiNode*    result_val = new (C) PhiNode(result_rgn, TypeInt::BOOL);
3366 
3367   RegionNode* slow_region = new (C) RegionNode(1);
3368   record_for_igvn(slow_region);
3369 
3370   // (a) Receiving thread must be the current thread.
3371   Node* rec_thr = argument(0);
3372   Node* tls_ptr = NULL;
3373   Node* cur_thr = generate_current_thread(tls_ptr);
3374   Node* cmp_thr = _gvn.transform(new (C) CmpPNode(cur_thr, rec_thr));
3375   Node* bol_thr = _gvn.transform(new (C) BoolNode(cmp_thr, BoolTest::ne));
3376 
3377   generate_slow_guard(bol_thr, slow_region);
3378 
3379   // (b) Interrupt bit on TLS must be false.
3380   Node* p = basic_plus_adr(top()/*!oop*/, tls_ptr, in_bytes(JavaThread::osthread_offset()));
3381   Node* osthread = make_load(NULL, p, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);
3382   p = basic_plus_adr(top()/*!oop*/, osthread, in_bytes(OSThread::interrupted_offset()));
3383 
3384   // Set the control input on the field _interrupted read to prevent it floating up.
3385   Node* int_bit = make_load(control(), p, TypeInt::BOOL, T_INT, MemNode::unordered);
3386   Node* cmp_bit = _gvn.transform(new (C) CmpINode(int_bit, intcon(0)));
3387   Node* bol_bit = _gvn.transform(new (C) BoolNode(cmp_bit, BoolTest::ne));
3388 
3389   IfNode* iff_bit = create_and_map_if(control(), bol_bit, PROB_UNLIKELY_MAG(3), COUNT_UNKNOWN);
3390 
3391   // First fast path:  if (!TLS._interrupted) return false;
3392   Node* false_bit = _gvn.transform(new (C) IfFalseNode(iff_bit));
3393   result_rgn-&gt;init_req(no_int_result_path, false_bit);
3394   result_val-&gt;init_req(no_int_result_path, intcon(0));
3395 
3396   // drop through to next case
3397   set_control( _gvn.transform(new (C) IfTrueNode(iff_bit)));
3398 
3399 #ifndef TARGET_OS_FAMILY_windows
3400   // (c) Or, if interrupt bit is set and clear_int is false, use 2nd fast path.
3401   Node* clr_arg = argument(1);
3402   Node* cmp_arg = _gvn.transform(new (C) CmpINode(clr_arg, intcon(0)));
3403   Node* bol_arg = _gvn.transform(new (C) BoolNode(cmp_arg, BoolTest::ne));
3404   IfNode* iff_arg = create_and_map_if(control(), bol_arg, PROB_FAIR, COUNT_UNKNOWN);
3405 
3406   // Second fast path:  ... else if (!clear_int) return true;
3407   Node* false_arg = _gvn.transform(new (C) IfFalseNode(iff_arg));
3408   result_rgn-&gt;init_req(no_clear_result_path, false_arg);
3409   result_val-&gt;init_req(no_clear_result_path, intcon(1));
3410 
3411   // drop through to next case
3412   set_control( _gvn.transform(new (C) IfTrueNode(iff_arg)));
3413 #else
3414   // To return true on Windows you must read the _interrupted field
3415   // and check the the event state i.e. take the slow path.
3416 #endif // TARGET_OS_FAMILY_windows
3417 
3418   // (d) Otherwise, go to the slow path.
3419   slow_region-&gt;add_req(control());
3420   set_control( _gvn.transform(slow_region));
3421 
3422   if (stopped()) {
3423     // There is no slow path.
3424     result_rgn-&gt;init_req(slow_result_path, top());
3425     result_val-&gt;init_req(slow_result_path, top());
3426   } else {
3427     // non-virtual because it is a private non-static
3428     CallJavaNode* slow_call = generate_method_call(vmIntrinsics::_isInterrupted);
3429 
3430     Node* slow_val = set_results_for_java_call(slow_call);
3431     // this-&gt;control() comes from set_results_for_java_call
3432 
3433     Node* fast_io  = slow_call-&gt;in(TypeFunc::I_O);
3434     Node* fast_mem = slow_call-&gt;in(TypeFunc::Memory);
3435 
3436     // These two phis are pre-filled with copies of of the fast IO and Memory
3437     PhiNode* result_mem  = PhiNode::make(result_rgn, fast_mem, Type::MEMORY, TypePtr::BOTTOM);
3438     PhiNode* result_io   = PhiNode::make(result_rgn, fast_io,  Type::ABIO);
3439 
3440     result_rgn-&gt;init_req(slow_result_path, control());
3441     result_io -&gt;init_req(slow_result_path, i_o());
3442     result_mem-&gt;init_req(slow_result_path, reset_memory());
3443     result_val-&gt;init_req(slow_result_path, slow_val);
3444 
3445     set_all_memory(_gvn.transform(result_mem));
3446     set_i_o(       _gvn.transform(result_io));
3447   }
3448 
3449   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
3450   set_result(result_rgn, result_val);
3451   return true;
3452 }
3453 
3454 //---------------------------load_mirror_from_klass----------------------------
3455 // Given a klass oop, load its java mirror (a java.lang.Class oop).
3456 Node* LibraryCallKit::load_mirror_from_klass(Node* klass) {
3457   Node* p = basic_plus_adr(klass, in_bytes(Klass::java_mirror_offset()));
3458   return make_load(NULL, p, TypeInstPtr::MIRROR, T_OBJECT, MemNode::unordered);
3459 }
3460 
3461 //-----------------------load_klass_from_mirror_common-------------------------
3462 // Given a java mirror (a java.lang.Class oop), load its corresponding klass oop.
3463 // Test the klass oop for null (signifying a primitive Class like Integer.TYPE),
3464 // and branch to the given path on the region.
3465 // If never_see_null, take an uncommon trap on null, so we can optimistically
3466 // compile for the non-null case.
3467 // If the region is NULL, force never_see_null = true.
3468 Node* LibraryCallKit::load_klass_from_mirror_common(Node* mirror,
3469                                                     bool never_see_null,
3470                                                     RegionNode* region,
3471                                                     int null_path,
3472                                                     int offset) {
3473   if (region == NULL)  never_see_null = true;
3474   Node* p = basic_plus_adr(mirror, offset);
3475   const TypeKlassPtr*  kls_type = TypeKlassPtr::OBJECT_OR_NULL;
3476   Node* kls = _gvn.transform(LoadKlassNode::make(_gvn, NULL, immutable_memory(), p, TypeRawPtr::BOTTOM, kls_type));
3477   Node* null_ctl = top();
3478   kls = null_check_oop(kls, &amp;null_ctl, never_see_null);
3479   if (region != NULL) {
3480     // Set region-&gt;in(null_path) if the mirror is a primitive (e.g, int.class).
3481     region-&gt;init_req(null_path, null_ctl);
3482   } else {
3483     assert(null_ctl == top(), "no loose ends");
3484   }
3485   return kls;
3486 }
3487 
3488 //--------------------(inline_native_Class_query helpers)---------------------
3489 // Use this for JVM_ACC_INTERFACE, JVM_ACC_IS_CLONEABLE, JVM_ACC_HAS_FINALIZER.
3490 // Fall through if (mods &amp; mask) == bits, take the guard otherwise.
3491 Node* LibraryCallKit::generate_access_flags_guard(Node* kls, int modifier_mask, int modifier_bits, RegionNode* region) {
3492   // Branch around if the given klass has the given modifier bit set.
3493   // Like generate_guard, adds a new path onto the region.
3494   Node* modp = basic_plus_adr(kls, in_bytes(Klass::access_flags_offset()));
3495   Node* mods = make_load(NULL, modp, TypeInt::INT, T_INT, MemNode::unordered);
3496   Node* mask = intcon(modifier_mask);
3497   Node* bits = intcon(modifier_bits);
3498   Node* mbit = _gvn.transform(new (C) AndINode(mods, mask));
3499   Node* cmp  = _gvn.transform(new (C) CmpINode(mbit, bits));
3500   Node* bol  = _gvn.transform(new (C) BoolNode(cmp, BoolTest::ne));
3501   return generate_fair_guard(bol, region);
3502 }
3503 Node* LibraryCallKit::generate_interface_guard(Node* kls, RegionNode* region) {
3504   return generate_access_flags_guard(kls, JVM_ACC_INTERFACE, 0, region);
3505 }
3506 
3507 //-------------------------inline_native_Class_query-------------------
3508 bool LibraryCallKit::inline_native_Class_query(vmIntrinsics::ID id) {
3509   const Type* return_type = TypeInt::BOOL;
3510   Node* prim_return_value = top();  // what happens if it's a primitive class?
3511   bool never_see_null = !too_many_traps(Deoptimization::Reason_null_check);
3512   bool expect_prim = false;     // most of these guys expect to work on refs
3513 
3514   enum { _normal_path = 1, _prim_path = 2, PATH_LIMIT };
3515 
3516   Node* mirror = argument(0);
3517   Node* obj    = top();
3518 
3519   switch (id) {
3520   case vmIntrinsics::_isInstance:
3521     // nothing is an instance of a primitive type
3522     prim_return_value = intcon(0);
3523     obj = argument(1);
3524     break;
3525   case vmIntrinsics::_getModifiers:
3526     prim_return_value = intcon(JVM_ACC_ABSTRACT | JVM_ACC_FINAL | JVM_ACC_PUBLIC);
3527     assert(is_power_of_2((int)JVM_ACC_WRITTEN_FLAGS+1), "change next line");
3528     return_type = TypeInt::make(0, JVM_ACC_WRITTEN_FLAGS, Type::WidenMin);
3529     break;
3530   case vmIntrinsics::_isInterface:
3531     prim_return_value = intcon(0);
3532     break;
3533   case vmIntrinsics::_isArray:
3534     prim_return_value = intcon(0);
3535     expect_prim = true;  // cf. ObjectStreamClass.getClassSignature
3536     break;
3537   case vmIntrinsics::_isPrimitive:
3538     prim_return_value = intcon(1);
3539     expect_prim = true;  // obviously
3540     break;
3541   case vmIntrinsics::_getSuperclass:
3542     prim_return_value = null();
3543     return_type = TypeInstPtr::MIRROR-&gt;cast_to_ptr_type(TypePtr::BotPTR);
3544     break;
3545   case vmIntrinsics::_getComponentType:
3546     prim_return_value = null();
3547     return_type = TypeInstPtr::MIRROR-&gt;cast_to_ptr_type(TypePtr::BotPTR);
3548     break;
3549   case vmIntrinsics::_getClassAccessFlags:
3550     prim_return_value = intcon(JVM_ACC_ABSTRACT | JVM_ACC_FINAL | JVM_ACC_PUBLIC);
3551     return_type = TypeInt::INT;  // not bool!  6297094
3552     break;
3553   default:
3554     fatal_unexpected_iid(id);
3555     break;
3556   }
3557 
3558   const TypeInstPtr* mirror_con = _gvn.type(mirror)-&gt;isa_instptr();
3559   if (mirror_con == NULL)  return false;  // cannot happen?
3560 
3561 #ifndef PRODUCT
3562   if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
3563     ciType* k = mirror_con-&gt;java_mirror_type();
3564     if (k) {
3565       tty-&gt;print("Inlining %s on constant Class ", vmIntrinsics::name_at(intrinsic_id()));
3566       k-&gt;print_name();
3567       tty-&gt;cr();
3568     }
3569   }
3570 #endif
3571 
3572   // Null-check the mirror, and the mirror's klass ptr (in case it is a primitive).
3573   RegionNode* region = new (C) RegionNode(PATH_LIMIT);
3574   record_for_igvn(region);
3575   PhiNode* phi = new (C) PhiNode(region, return_type);
3576 
3577   // The mirror will never be null of Reflection.getClassAccessFlags, however
3578   // it may be null for Class.isInstance or Class.getModifiers. Throw a NPE
3579   // if it is. See bug 4774291.
3580 
3581   // For Reflection.getClassAccessFlags(), the null check occurs in
3582   // the wrong place; see inline_unsafe_access(), above, for a similar
3583   // situation.
3584   mirror = null_check(mirror);
3585   // If mirror or obj is dead, only null-path is taken.
3586   if (stopped())  return true;
3587 
3588   if (expect_prim)  never_see_null = false;  // expect nulls (meaning prims)
3589 
3590   // Now load the mirror's klass metaobject, and null-check it.
3591   // Side-effects region with the control path if the klass is null.
3592   Node* kls = load_klass_from_mirror(mirror, never_see_null, region, _prim_path);
3593   // If kls is null, we have a primitive mirror.
3594   phi-&gt;init_req(_prim_path, prim_return_value);
3595   if (stopped()) { set_result(region, phi); return true; }
3596   bool safe_for_replace = (region-&gt;in(_prim_path) == top());
3597 
3598   Node* p;  // handy temp
3599   Node* null_ctl;
3600 
3601   // Now that we have the non-null klass, we can perform the real query.
3602   // For constant classes, the query will constant-fold in LoadNode::Value.
3603   Node* query_value = top();
3604   switch (id) {
3605   case vmIntrinsics::_isInstance:
3606     // nothing is an instance of a primitive type
3607     query_value = gen_instanceof(obj, kls, safe_for_replace);
3608     break;
3609 
3610   case vmIntrinsics::_getModifiers:
3611     p = basic_plus_adr(kls, in_bytes(Klass::modifier_flags_offset()));
3612     query_value = make_load(NULL, p, TypeInt::INT, T_INT, MemNode::unordered);
3613     break;
3614 
3615   case vmIntrinsics::_isInterface:
3616     // (To verify this code sequence, check the asserts in JVM_IsInterface.)
3617     if (generate_interface_guard(kls, region) != NULL)
3618       // A guard was added.  If the guard is taken, it was an interface.
3619       phi-&gt;add_req(intcon(1));
3620     // If we fall through, it's a plain class.
3621     query_value = intcon(0);
3622     break;
3623 
3624   case vmIntrinsics::_isArray:
3625     // (To verify this code sequence, check the asserts in JVM_IsArrayClass.)
3626     if (generate_array_guard(kls, region) != NULL)
3627       // A guard was added.  If the guard is taken, it was an array.
3628       phi-&gt;add_req(intcon(1));
3629     // If we fall through, it's a plain class.
3630     query_value = intcon(0);
3631     break;
3632 
3633   case vmIntrinsics::_isPrimitive:
3634     query_value = intcon(0); // "normal" path produces false
3635     break;
3636 
3637   case vmIntrinsics::_getSuperclass:
3638     // The rules here are somewhat unfortunate, but we can still do better
3639     // with random logic than with a JNI call.
3640     // Interfaces store null or Object as _super, but must report null.
3641     // Arrays store an intermediate super as _super, but must report Object.
3642     // Other types can report the actual _super.
3643     // (To verify this code sequence, check the asserts in JVM_IsInterface.)
3644     if (generate_interface_guard(kls, region) != NULL)
3645       // A guard was added.  If the guard is taken, it was an interface.
3646       phi-&gt;add_req(null());
3647     if (generate_array_guard(kls, region) != NULL)
3648       // A guard was added.  If the guard is taken, it was an array.
3649       phi-&gt;add_req(makecon(TypeInstPtr::make(env()-&gt;Object_klass()-&gt;java_mirror())));
3650     // If we fall through, it's a plain class.  Get its _super.
3651     p = basic_plus_adr(kls, in_bytes(Klass::super_offset()));
3652     kls = _gvn.transform(LoadKlassNode::make(_gvn, NULL, immutable_memory(), p, TypeRawPtr::BOTTOM, TypeKlassPtr::OBJECT_OR_NULL));
3653     null_ctl = top();
3654     kls = null_check_oop(kls, &amp;null_ctl);
3655     if (null_ctl != top()) {
3656       // If the guard is taken, Object.superClass is null (both klass and mirror).
3657       region-&gt;add_req(null_ctl);
3658       phi   -&gt;add_req(null());
3659     }
3660     if (!stopped()) {
3661       query_value = load_mirror_from_klass(kls);
3662     }
3663     break;
3664 
3665   case vmIntrinsics::_getComponentType:
3666     if (generate_array_guard(kls, region) != NULL) {
3667       // Be sure to pin the oop load to the guard edge just created:
3668       Node* is_array_ctrl = region-&gt;in(region-&gt;req()-1);
3669       Node* cma = basic_plus_adr(kls, in_bytes(ArrayKlass::component_mirror_offset()));
3670       Node* cmo = make_load(is_array_ctrl, cma, TypeInstPtr::MIRROR, T_OBJECT, MemNode::unordered);
3671       phi-&gt;add_req(cmo);
3672     }
3673     query_value = null();  // non-array case is null
3674     break;
3675 
3676   case vmIntrinsics::_getClassAccessFlags:
3677     p = basic_plus_adr(kls, in_bytes(Klass::access_flags_offset()));
3678     query_value = make_load(NULL, p, TypeInt::INT, T_INT, MemNode::unordered);
3679     break;
3680 
3681   default:
3682     fatal_unexpected_iid(id);
3683     break;
3684   }
3685 
3686   // Fall-through is the normal case of a query to a real class.
3687   phi-&gt;init_req(1, query_value);
3688   region-&gt;init_req(1, control());
3689 
3690   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
3691   set_result(region, phi);
3692   return true;
3693 }
3694 
3695 //--------------------------inline_native_subtype_check------------------------
3696 // This intrinsic takes the JNI calls out of the heart of
3697 // UnsafeFieldAccessorImpl.set, which improves Field.set, readObject, etc.
3698 bool LibraryCallKit::inline_native_subtype_check() {
3699   // Pull both arguments off the stack.
3700   Node* args[2];                // two java.lang.Class mirrors: superc, subc
3701   args[0] = argument(0);
3702   args[1] = argument(1);
3703   Node* klasses[2];             // corresponding Klasses: superk, subk
3704   klasses[0] = klasses[1] = top();
3705 
3706   enum {
3707     // A full decision tree on {superc is prim, subc is prim}:
3708     _prim_0_path = 1,           // {P,N} =&gt; false
3709                                 // {P,P} &amp; superc!=subc =&gt; false
3710     _prim_same_path,            // {P,P} &amp; superc==subc =&gt; true
3711     _prim_1_path,               // {N,P} =&gt; false
3712     _ref_subtype_path,          // {N,N} &amp; subtype check wins =&gt; true
3713     _both_ref_path,             // {N,N} &amp; subtype check loses =&gt; false
3714     PATH_LIMIT
3715   };
3716 
3717   RegionNode* region = new (C) RegionNode(PATH_LIMIT);
3718   Node*       phi    = new (C) PhiNode(region, TypeInt::BOOL);
3719   record_for_igvn(region);
3720 
3721   const TypePtr* adr_type = TypeRawPtr::BOTTOM;   // memory type of loads
3722   const TypeKlassPtr* kls_type = TypeKlassPtr::OBJECT_OR_NULL;
3723   int class_klass_offset = java_lang_Class::klass_offset_in_bytes();
3724 
3725   // First null-check both mirrors and load each mirror's klass metaobject.
3726   int which_arg;
3727   for (which_arg = 0; which_arg &lt;= 1; which_arg++) {
3728     Node* arg = args[which_arg];
3729     arg = null_check(arg);
3730     if (stopped())  break;
3731     args[which_arg] = arg;
3732 
3733     Node* p = basic_plus_adr(arg, class_klass_offset);
3734     Node* kls = LoadKlassNode::make(_gvn, NULL, immutable_memory(), p, adr_type, kls_type);
3735     klasses[which_arg] = _gvn.transform(kls);
3736   }
3737 
3738   // Having loaded both klasses, test each for null.
3739   bool never_see_null = !too_many_traps(Deoptimization::Reason_null_check);
3740   for (which_arg = 0; which_arg &lt;= 1; which_arg++) {
3741     Node* kls = klasses[which_arg];
3742     Node* null_ctl = top();
3743     kls = null_check_oop(kls, &amp;null_ctl, never_see_null);
3744     int prim_path = (which_arg == 0 ? _prim_0_path : _prim_1_path);
3745     region-&gt;init_req(prim_path, null_ctl);
3746     if (stopped())  break;
3747     klasses[which_arg] = kls;
3748   }
3749 
3750   if (!stopped()) {
3751     // now we have two reference types, in klasses[0..1]
3752     Node* subk   = klasses[1];  // the argument to isAssignableFrom
3753     Node* superk = klasses[0];  // the receiver
3754     region-&gt;set_req(_both_ref_path, gen_subtype_check(subk, superk));
3755     // now we have a successful reference subtype check
3756     region-&gt;set_req(_ref_subtype_path, control());
3757   }
3758 
3759   // If both operands are primitive (both klasses null), then
3760   // we must return true when they are identical primitives.
3761   // It is convenient to test this after the first null klass check.
3762   set_control(region-&gt;in(_prim_0_path)); // go back to first null check
3763   if (!stopped()) {
3764     // Since superc is primitive, make a guard for the superc==subc case.
3765     Node* cmp_eq = _gvn.transform(new (C) CmpPNode(args[0], args[1]));
3766     Node* bol_eq = _gvn.transform(new (C) BoolNode(cmp_eq, BoolTest::eq));
3767     generate_guard(bol_eq, region, PROB_FAIR);
3768     if (region-&gt;req() == PATH_LIMIT+1) {
3769       // A guard was added.  If the added guard is taken, superc==subc.
3770       region-&gt;swap_edges(PATH_LIMIT, _prim_same_path);
3771       region-&gt;del_req(PATH_LIMIT);
3772     }
3773     region-&gt;set_req(_prim_0_path, control()); // Not equal after all.
3774   }
3775 
3776   // these are the only paths that produce 'true':
3777   phi-&gt;set_req(_prim_same_path,   intcon(1));
3778   phi-&gt;set_req(_ref_subtype_path, intcon(1));
3779 
3780   // pull together the cases:
3781   assert(region-&gt;req() == PATH_LIMIT, "sane region");
3782   for (uint i = 1; i &lt; region-&gt;req(); i++) {
3783     Node* ctl = region-&gt;in(i);
3784     if (ctl == NULL || ctl == top()) {
3785       region-&gt;set_req(i, top());
3786       phi   -&gt;set_req(i, top());
3787     } else if (phi-&gt;in(i) == NULL) {
3788       phi-&gt;set_req(i, intcon(0)); // all other paths produce 'false'
3789     }
3790   }
3791 
3792   set_control(_gvn.transform(region));
3793   set_result(_gvn.transform(phi));
3794   return true;
3795 }
3796 
3797 //---------------------generate_array_guard_common------------------------
3798 Node* LibraryCallKit::generate_array_guard_common(Node* kls, RegionNode* region,
3799                                                   bool obj_array, bool not_array) {
3800   // If obj_array/non_array==false/false:
3801   // Branch around if the given klass is in fact an array (either obj or prim).
3802   // If obj_array/non_array==false/true:
3803   // Branch around if the given klass is not an array klass of any kind.
3804   // If obj_array/non_array==true/true:
3805   // Branch around if the kls is not an oop array (kls is int[], String, etc.)
3806   // If obj_array/non_array==true/false:
3807   // Branch around if the kls is an oop array (Object[] or subtype)
3808   //
3809   // Like generate_guard, adds a new path onto the region.
3810   jint  layout_con = 0;
3811   Node* layout_val = get_layout_helper(kls, layout_con);
3812   if (layout_val == NULL) {
3813     bool query = (obj_array
3814                   ? Klass::layout_helper_is_objArray(layout_con)
3815                   : Klass::layout_helper_is_array(layout_con));
3816     if (query == not_array) {
3817       return NULL;                       // never a branch
3818     } else {                             // always a branch
3819       Node* always_branch = control();
3820       if (region != NULL)
3821         region-&gt;add_req(always_branch);
3822       set_control(top());
3823       return always_branch;
3824     }
3825   }
3826   // Now test the correct condition.
3827   jint  nval = (obj_array
3828                 ? ((jint)Klass::_lh_array_tag_type_value
3829                    &lt;&lt;    Klass::_lh_array_tag_shift)
3830                 : Klass::_lh_neutral_value);
3831   Node* cmp = _gvn.transform(new(C) CmpINode(layout_val, intcon(nval)));
3832   BoolTest::mask btest = BoolTest::lt;  // correct for testing is_[obj]array
3833   // invert the test if we are looking for a non-array
3834   if (not_array)  btest = BoolTest(btest).negate();
3835   Node* bol = _gvn.transform(new(C) BoolNode(cmp, btest));
3836   return generate_fair_guard(bol, region);
3837 }
3838 
3839 
3840 //-----------------------inline_native_newArray--------------------------
3841 // private static native Object java.lang.reflect.newArray(Class&lt;?&gt; componentType, int length);
3842 bool LibraryCallKit::inline_native_newArray() {
3843   Node* mirror    = argument(0);
3844   Node* count_val = argument(1);
3845 
3846   mirror = null_check(mirror);
3847   // If mirror or obj is dead, only null-path is taken.
3848   if (stopped())  return true;
3849 
3850   enum { _normal_path = 1, _slow_path = 2, PATH_LIMIT };
3851   RegionNode* result_reg = new(C) RegionNode(PATH_LIMIT);
3852   PhiNode*    result_val = new(C) PhiNode(result_reg,
3853                                           TypeInstPtr::NOTNULL);
3854   PhiNode*    result_io  = new(C) PhiNode(result_reg, Type::ABIO);
3855   PhiNode*    result_mem = new(C) PhiNode(result_reg, Type::MEMORY,
3856                                           TypePtr::BOTTOM);
3857 
3858   bool never_see_null = !too_many_traps(Deoptimization::Reason_null_check);
3859   Node* klass_node = load_array_klass_from_mirror(mirror, never_see_null,
3860                                                   result_reg, _slow_path);
3861   Node* normal_ctl   = control();
3862   Node* no_array_ctl = result_reg-&gt;in(_slow_path);
3863 
3864   // Generate code for the slow case.  We make a call to newArray().
3865   set_control(no_array_ctl);
3866   if (!stopped()) {
3867     // Either the input type is void.class, or else the
3868     // array klass has not yet been cached.  Either the
3869     // ensuing call will throw an exception, or else it
3870     // will cache the array klass for next time.
3871     PreserveJVMState pjvms(this);
3872     CallJavaNode* slow_call = generate_method_call_static(vmIntrinsics::_newArray);
3873     Node* slow_result = set_results_for_java_call(slow_call);
3874     // this-&gt;control() comes from set_results_for_java_call
3875     result_reg-&gt;set_req(_slow_path, control());
3876     result_val-&gt;set_req(_slow_path, slow_result);
3877     result_io -&gt;set_req(_slow_path, i_o());
3878     result_mem-&gt;set_req(_slow_path, reset_memory());
3879   }
3880 
3881   set_control(normal_ctl);
3882   if (!stopped()) {
3883     // Normal case:  The array type has been cached in the java.lang.Class.
3884     // The following call works fine even if the array type is polymorphic.
3885     // It could be a dynamic mix of int[], boolean[], Object[], etc.
3886     Node* obj = new_array(klass_node, count_val, 0);  // no arguments to push
3887     result_reg-&gt;init_req(_normal_path, control());
3888     result_val-&gt;init_req(_normal_path, obj);
3889     result_io -&gt;init_req(_normal_path, i_o());
3890     result_mem-&gt;init_req(_normal_path, reset_memory());
3891   }
3892 
3893   // Return the combined state.
3894   set_i_o(        _gvn.transform(result_io)  );
3895   set_all_memory( _gvn.transform(result_mem));
3896 
3897   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
3898   set_result(result_reg, result_val);
3899   return true;
3900 }
3901 
3902 //----------------------inline_native_getLength--------------------------
3903 // public static native int java.lang.reflect.Array.getLength(Object array);
3904 bool LibraryCallKit::inline_native_getLength() {
3905   if (too_many_traps(Deoptimization::Reason_intrinsic))  return false;
3906 
3907   Node* array = null_check(argument(0));
3908   // If array is dead, only null-path is taken.
3909   if (stopped())  return true;
3910 
3911   // Deoptimize if it is a non-array.
3912   Node* non_array = generate_non_array_guard(load_object_klass(array), NULL);
3913 
3914   if (non_array != NULL) {
3915     PreserveJVMState pjvms(this);
3916     set_control(non_array);
3917     uncommon_trap(Deoptimization::Reason_intrinsic,
3918                   Deoptimization::Action_maybe_recompile);
3919   }
3920 
3921   // If control is dead, only non-array-path is taken.
3922   if (stopped())  return true;
3923 
3924   // The works fine even if the array type is polymorphic.
3925   // It could be a dynamic mix of int[], boolean[], Object[], etc.
3926   Node* result = load_array_length(array);
3927 
3928   C-&gt;set_has_split_ifs(true);  // Has chance for split-if optimization
3929   set_result(result);
3930   return true;
3931 }
3932 
3933 //------------------------inline_array_copyOf----------------------------
3934 // public static &lt;T,U&gt; T[] java.util.Arrays.copyOf(     U[] original, int newLength,         Class&lt;? extends T[]&gt; newType);
3935 // public static &lt;T,U&gt; T[] java.util.Arrays.copyOfRange(U[] original, int from,      int to, Class&lt;? extends T[]&gt; newType);
3936 bool LibraryCallKit::inline_array_copyOf(bool is_copyOfRange) {
3937   if (too_many_traps(Deoptimization::Reason_intrinsic))  return false;
3938 
3939   // Get the arguments.
3940   Node* original          = argument(0);
3941   Node* start             = is_copyOfRange? argument(1): intcon(0);
3942   Node* end               = is_copyOfRange? argument(2): argument(1);
3943   Node* array_type_mirror = is_copyOfRange? argument(3): argument(2);
3944 
3945   Node* newcopy = NULL;
3946 
3947   // Set the original stack and the reexecute bit for the interpreter to reexecute
3948   // the bytecode that invokes Arrays.copyOf if deoptimization happens.
3949   { PreserveReexecuteState preexecs(this);
3950     jvms()-&gt;set_should_reexecute(true);
3951 
3952     array_type_mirror = null_check(array_type_mirror);
3953     original          = null_check(original);
3954 
3955     // Check if a null path was taken unconditionally.
3956     if (stopped())  return true;
3957 
3958     Node* orig_length = load_array_length(original);
3959 
3960     Node* klass_node = load_klass_from_mirror(array_type_mirror, false, NULL, 0);
3961     klass_node = null_check(klass_node);
3962 
3963     RegionNode* bailout = new (C) RegionNode(1);
3964     record_for_igvn(bailout);
3965 
3966     // Despite the generic type of Arrays.copyOf, the mirror might be int, int[], etc.
3967     // Bail out if that is so.
3968     Node* not_objArray = generate_non_objArray_guard(klass_node, bailout);
3969     if (not_objArray != NULL) {
3970       // Improve the klass node's type from the new optimistic assumption:
3971       ciKlass* ak = ciArrayKlass::make(env()-&gt;Object_klass());
3972       const Type* akls = TypeKlassPtr::make(TypePtr::NotNull, ak, 0/*offset*/);
3973       Node* cast = new (C) CastPPNode(klass_node, akls);
3974       cast-&gt;init_req(0, control());
3975       klass_node = _gvn.transform(cast);
3976     }
3977 
3978     // Bail out if either start or end is negative.
3979     generate_negative_guard(start, bailout, &amp;start);
3980     generate_negative_guard(end,   bailout, &amp;end);
3981 
3982     Node* length = end;
3983     if (_gvn.type(start) != TypeInt::ZERO) {
3984       length = _gvn.transform(new (C) SubINode(end, start));
3985     }
3986 
3987     // Bail out if length is negative.
3988     // Without this the new_array would throw
3989     // NegativeArraySizeException but IllegalArgumentException is what
3990     // should be thrown
3991     generate_negative_guard(length, bailout, &amp;length);
3992 
3993     if (bailout-&gt;req() &gt; 1) {
3994       PreserveJVMState pjvms(this);
3995       set_control(_gvn.transform(bailout));
3996       uncommon_trap(Deoptimization::Reason_intrinsic,
3997                     Deoptimization::Action_maybe_recompile);
3998     }
3999 
4000     if (!stopped()) {
4001       // How many elements will we copy from the original?
4002       // The answer is MinI(orig_length - start, length).
4003       Node* orig_tail = _gvn.transform(new (C) SubINode(orig_length, start));
4004       Node* moved = generate_min_max(vmIntrinsics::_min, orig_tail, length);
4005 
4006       newcopy = new_array(klass_node, length, 0);  // no argments to push
4007 
4008       // Generate a direct call to the right arraycopy function(s).
4009       // We know the copy is disjoint but we might not know if the
4010       // oop stores need checking.
4011       // Extreme case:  Arrays.copyOf((Integer[])x, 10, String[].class).
4012       // This will fail a store-check if x contains any non-nulls.
4013       bool disjoint_bases = true;
4014       // if start &gt; orig_length then the length of the copy may be
4015       // negative.
4016       bool length_never_negative = !is_copyOfRange;
4017       generate_arraycopy(TypeAryPtr::OOPS, T_OBJECT,
4018                          original, start, newcopy, intcon(0), moved,
4019                          disjoint_bases, length_never_negative);
4020     }
4021   } // original reexecute is set back here
4022 
4023   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
4024   if (!stopped()) {
4025     set_result(newcopy);
4026   }
4027   return true;
4028 }
4029 
4030 
4031 //----------------------generate_virtual_guard---------------------------
4032 // Helper for hashCode and clone.  Peeks inside the vtable to avoid a call.
4033 Node* LibraryCallKit::generate_virtual_guard(Node* obj_klass,
4034                                              RegionNode* slow_region) {
4035   ciMethod* method = callee();
4036   int vtable_index = method-&gt;vtable_index();
4037   assert(vtable_index &gt;= 0 || vtable_index == Method::nonvirtual_vtable_index,
4038          err_msg_res("bad index %d", vtable_index));
4039   // Get the Method* out of the appropriate vtable entry.
4040   int entry_offset  = (InstanceKlass::vtable_start_offset() +
4041                      vtable_index*vtableEntry::size()) * wordSize +
4042                      vtableEntry::method_offset_in_bytes();
4043   Node* entry_addr  = basic_plus_adr(obj_klass, entry_offset);
4044   Node* target_call = make_load(NULL, entry_addr, TypePtr::NOTNULL, T_ADDRESS, MemNode::unordered);
4045 
4046   // Compare the target method with the expected method (e.g., Object.hashCode).
4047   const TypePtr* native_call_addr = TypeMetadataPtr::make(method);
4048 
4049   Node* native_call = makecon(native_call_addr);
4050   Node* chk_native  = _gvn.transform(new(C) CmpPNode(target_call, native_call));
4051   Node* test_native = _gvn.transform(new(C) BoolNode(chk_native, BoolTest::ne));
4052 
4053   return generate_slow_guard(test_native, slow_region);
4054 }
4055 
4056 //-----------------------generate_method_call----------------------------
4057 // Use generate_method_call to make a slow-call to the real
4058 // method if the fast path fails.  An alternative would be to
4059 // use a stub like OptoRuntime::slow_arraycopy_Java.
4060 // This only works for expanding the current library call,
4061 // not another intrinsic.  (E.g., don't use this for making an
4062 // arraycopy call inside of the copyOf intrinsic.)
4063 CallJavaNode*
4064 LibraryCallKit::generate_method_call(vmIntrinsics::ID method_id, bool is_virtual, bool is_static) {
4065   // When compiling the intrinsic method itself, do not use this technique.
4066   guarantee(callee() != C-&gt;method(), "cannot make slow-call to self");
4067 
4068   ciMethod* method = callee();
4069   // ensure the JVMS we have will be correct for this call
4070   guarantee(method_id == method-&gt;intrinsic_id(), "must match");
4071 
4072   const TypeFunc* tf = TypeFunc::make(method);
4073   CallJavaNode* slow_call;
4074   if (is_static) {
4075     assert(!is_virtual, "");
4076     slow_call = new(C) CallStaticJavaNode(C, tf,
4077                            SharedRuntime::get_resolve_static_call_stub(),
4078                            method, bci());
4079   } else if (is_virtual) {
4080     null_check_receiver();
4081     int vtable_index = Method::invalid_vtable_index;
4082     if (UseInlineCaches) {
4083       // Suppress the vtable call
4084     } else {
4085       // hashCode and clone are not a miranda methods,
4086       // so the vtable index is fixed.
4087       // No need to use the linkResolver to get it.
4088        vtable_index = method-&gt;vtable_index();
4089        assert(vtable_index &gt;= 0 || vtable_index == Method::nonvirtual_vtable_index,
4090               err_msg_res("bad index %d", vtable_index));
4091     }
4092     slow_call = new(C) CallDynamicJavaNode(tf,
4093                           SharedRuntime::get_resolve_virtual_call_stub(),
4094                           method, vtable_index, bci());
4095   } else {  // neither virtual nor static:  opt_virtual
4096     null_check_receiver();
4097     slow_call = new(C) CallStaticJavaNode(C, tf,
4098                                 SharedRuntime::get_resolve_opt_virtual_call_stub(),
4099                                 method, bci());
4100     slow_call-&gt;set_optimized_virtual(true);
4101   }
4102   set_arguments_for_java_call(slow_call);
4103   set_edges_for_java_call(slow_call);
4104   return slow_call;
4105 }
4106 
4107 
4108 /**
4109  * Build special case code for calls to hashCode on an object. This call may
4110  * be virtual (invokevirtual) or bound (invokespecial). For each case we generate
4111  * slightly different code.
4112  */
4113 bool LibraryCallKit::inline_native_hashcode(bool is_virtual, bool is_static) {
4114   assert(is_static == callee()-&gt;is_static(), "correct intrinsic selection");
4115   assert(!(is_virtual &amp;&amp; is_static), "either virtual, special, or static");
4116 
4117   enum { _slow_path = 1, _fast_path, _null_path, PATH_LIMIT };
4118 
4119   RegionNode* result_reg = new(C) RegionNode(PATH_LIMIT);
4120   PhiNode*    result_val = new(C) PhiNode(result_reg, TypeInt::INT);
4121   PhiNode*    result_io  = new(C) PhiNode(result_reg, Type::ABIO);
4122   PhiNode*    result_mem = new(C) PhiNode(result_reg, Type::MEMORY, TypePtr::BOTTOM);
4123   Node* obj = NULL;
4124   if (!is_static) {
4125     // Check for hashing null object
4126     obj = null_check_receiver();
4127     if (stopped())  return true;        // unconditionally null
4128     result_reg-&gt;init_req(_null_path, top());
4129     result_val-&gt;init_req(_null_path, top());
4130   } else {
4131     // Do a null check, and return zero if null.
4132     // System.identityHashCode(null) == 0
4133     obj = argument(0);
4134     Node* null_ctl = top();
4135     obj = null_check_oop(obj, &amp;null_ctl);
4136     result_reg-&gt;init_req(_null_path, null_ctl);
4137     result_val-&gt;init_req(_null_path, _gvn.intcon(0));
4138   }
4139 
4140   // Unconditionally null?  Then return right away.
4141   if (stopped()) {
4142     set_control( result_reg-&gt;in(_null_path));
4143     if (!stopped())
4144       set_result(result_val-&gt;in(_null_path));
4145     return true;
4146   }
4147 
4148   // We only go to the fast case code if we pass a number of guards.  The
4149   // paths which do not pass are accumulated in the slow_region.
4150   RegionNode* slow_region = new (C) RegionNode(1);
4151   record_for_igvn(slow_region);
4152 
4153   // If this is a virtual call, we generate a funny guard.  We pull out
4154   // the vtable entry corresponding to hashCode() from the target object.
4155   // If the target method which we are calling happens to be the native
4156   // Object hashCode() method, we pass the guard.  We do not need this
4157   // guard for non-virtual calls -- the caller is known to be the native
4158   // Object hashCode().
4159   if (is_virtual) {
4160     // After null check, get the object's klass.
4161     Node* obj_klass = load_object_klass(obj);
4162     generate_virtual_guard(obj_klass, slow_region);
4163   }
4164 
4165   // Get the header out of the object, use LoadMarkNode when available
4166   Node* header_addr = basic_plus_adr(obj, oopDesc::mark_offset_in_bytes());
4167   // The control of the load must be NULL. Otherwise, the load can move before
4168   // the null check after castPP removal.
4169   Node* no_ctrl = NULL;
4170   Node* header = make_load(no_ctrl, header_addr, TypeX_X, TypeX_X-&gt;basic_type(), MemNode::unordered);
4171 
4172   // Test the header to see if it is unlocked.
4173   Node* lock_mask      = _gvn.MakeConX(markOopDesc::biased_lock_mask_in_place);
4174   Node* lmasked_header = _gvn.transform(new (C) AndXNode(header, lock_mask));
4175   Node* unlocked_val   = _gvn.MakeConX(markOopDesc::unlocked_value);
4176   Node* chk_unlocked   = _gvn.transform(new (C) CmpXNode( lmasked_header, unlocked_val));
4177   Node* test_unlocked  = _gvn.transform(new (C) BoolNode( chk_unlocked, BoolTest::ne));
4178 
4179   generate_slow_guard(test_unlocked, slow_region);
4180 
4181   // Get the hash value and check to see that it has been properly assigned.
4182   // We depend on hash_mask being at most 32 bits and avoid the use of
4183   // hash_mask_in_place because it could be larger than 32 bits in a 64-bit
4184   // vm: see markOop.hpp.
4185   Node* hash_mask      = _gvn.intcon(markOopDesc::hash_mask);
4186   Node* hash_shift     = _gvn.intcon(markOopDesc::hash_shift);
4187   Node* hshifted_header= _gvn.transform(new (C) URShiftXNode(header, hash_shift));
4188   // This hack lets the hash bits live anywhere in the mark object now, as long
4189   // as the shift drops the relevant bits into the low 32 bits.  Note that
4190   // Java spec says that HashCode is an int so there's no point in capturing
4191   // an 'X'-sized hashcode (32 in 32-bit build or 64 in 64-bit build).
4192   hshifted_header      = ConvX2I(hshifted_header);
4193   Node* hash_val       = _gvn.transform(new (C) AndINode(hshifted_header, hash_mask));
4194 
4195   Node* no_hash_val    = _gvn.intcon(markOopDesc::no_hash);
4196   Node* chk_assigned   = _gvn.transform(new (C) CmpINode( hash_val, no_hash_val));
4197   Node* test_assigned  = _gvn.transform(new (C) BoolNode( chk_assigned, BoolTest::eq));
4198 
4199   generate_slow_guard(test_assigned, slow_region);
4200 
4201   Node* init_mem = reset_memory();
4202   // fill in the rest of the null path:
4203   result_io -&gt;init_req(_null_path, i_o());
4204   result_mem-&gt;init_req(_null_path, init_mem);
4205 
4206   result_val-&gt;init_req(_fast_path, hash_val);
4207   result_reg-&gt;init_req(_fast_path, control());
4208   result_io -&gt;init_req(_fast_path, i_o());
4209   result_mem-&gt;init_req(_fast_path, init_mem);
4210 
4211   // Generate code for the slow case.  We make a call to hashCode().
4212   set_control(_gvn.transform(slow_region));
4213   if (!stopped()) {
4214     // No need for PreserveJVMState, because we're using up the present state.
4215     set_all_memory(init_mem);
4216     vmIntrinsics::ID hashCode_id = is_static ? vmIntrinsics::_identityHashCode : vmIntrinsics::_hashCode;
4217     CallJavaNode* slow_call = generate_method_call(hashCode_id, is_virtual, is_static);
4218     Node* slow_result = set_results_for_java_call(slow_call);
4219     // this-&gt;control() comes from set_results_for_java_call
4220     result_reg-&gt;init_req(_slow_path, control());
4221     result_val-&gt;init_req(_slow_path, slow_result);
4222     result_io  -&gt;set_req(_slow_path, i_o());
4223     result_mem -&gt;set_req(_slow_path, reset_memory());
4224   }
4225 
4226   // Return the combined state.
4227   set_i_o(        _gvn.transform(result_io)  );
4228   set_all_memory( _gvn.transform(result_mem));
4229 
4230   set_result(result_reg, result_val);
4231   return true;
4232 }
4233 
4234 //---------------------------inline_native_getClass----------------------------
4235 // public final native Class&lt;?&gt; java.lang.Object.getClass();
4236 //
4237 // Build special case code for calls to getClass on an object.
4238 bool LibraryCallKit::inline_native_getClass() {
4239   Node* obj = null_check_receiver();
4240   if (stopped())  return true;
4241   set_result(load_mirror_from_klass(load_object_klass(obj)));
4242   return true;
4243 }
4244 
4245 //-----------------inline_native_Reflection_getCallerClass---------------------
4246 // public static native Class&lt;?&gt; sun.reflect.Reflection.getCallerClass();
4247 //
4248 // In the presence of deep enough inlining, getCallerClass() becomes a no-op.
4249 //
4250 // NOTE: This code must perform the same logic as JVM_GetCallerClass
4251 // in that it must skip particular security frames and checks for
4252 // caller sensitive methods.
4253 bool LibraryCallKit::inline_native_Reflection_getCallerClass() {
4254 #ifndef PRODUCT
4255   if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
4256     tty-&gt;print_cr("Attempting to inline sun.reflect.Reflection.getCallerClass");
4257   }
4258 #endif
4259 
4260   if (!jvms()-&gt;has_method()) {
4261 #ifndef PRODUCT
4262     if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
4263       tty-&gt;print_cr("  Bailing out because intrinsic was inlined at top level");
4264     }
4265 #endif
4266     return false;
4267   }
4268 
4269   // Walk back up the JVM state to find the caller at the required
4270   // depth.
4271   JVMState* caller_jvms = jvms();
4272 
4273   // Cf. JVM_GetCallerClass
4274   // NOTE: Start the loop at depth 1 because the current JVM state does
4275   // not include the Reflection.getCallerClass() frame.
4276   for (int n = 1; caller_jvms != NULL; caller_jvms = caller_jvms-&gt;caller(), n++) {
4277     ciMethod* m = caller_jvms-&gt;method();
4278     switch (n) {
4279     case 0:
4280       fatal("current JVM state does not include the Reflection.getCallerClass frame");
4281       break;
4282     case 1:
4283       // Frame 0 and 1 must be caller sensitive (see JVM_GetCallerClass).
4284       if (!m-&gt;caller_sensitive()) {
4285 #ifndef PRODUCT
4286         if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
4287           tty-&gt;print_cr("  Bailing out: CallerSensitive annotation expected at frame %d", n);
4288         }
4289 #endif
4290         return false;  // bail-out; let JVM_GetCallerClass do the work
4291       }
4292       break;
4293     default:
4294       if (!m-&gt;is_ignored_by_security_stack_walk()) {
4295         // We have reached the desired frame; return the holder class.
4296         // Acquire method holder as java.lang.Class and push as constant.
4297         ciInstanceKlass* caller_klass = caller_jvms-&gt;method()-&gt;holder();
4298         ciInstance* caller_mirror = caller_klass-&gt;java_mirror();
4299         set_result(makecon(TypeInstPtr::make(caller_mirror)));
4300 
4301 #ifndef PRODUCT
4302         if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
4303           tty-&gt;print_cr("  Succeeded: caller = %d) %s.%s, JVMS depth = %d", n, caller_klass-&gt;name()-&gt;as_utf8(), caller_jvms-&gt;method()-&gt;name()-&gt;as_utf8(), jvms()-&gt;depth());
4304           tty-&gt;print_cr("  JVM state at this point:");
4305           for (int i = jvms()-&gt;depth(), n = 1; i &gt;= 1; i--, n++) {
4306             ciMethod* m = jvms()-&gt;of_depth(i)-&gt;method();
4307             tty-&gt;print_cr("   %d) %s.%s", n, m-&gt;holder()-&gt;name()-&gt;as_utf8(), m-&gt;name()-&gt;as_utf8());
4308           }
4309         }
4310 #endif
4311         return true;
4312       }
4313       break;
4314     }
4315   }
4316 
4317 #ifndef PRODUCT
4318   if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
4319     tty-&gt;print_cr("  Bailing out because caller depth exceeded inlining depth = %d", jvms()-&gt;depth());
4320     tty-&gt;print_cr("  JVM state at this point:");
4321     for (int i = jvms()-&gt;depth(), n = 1; i &gt;= 1; i--, n++) {
4322       ciMethod* m = jvms()-&gt;of_depth(i)-&gt;method();
4323       tty-&gt;print_cr("   %d) %s.%s", n, m-&gt;holder()-&gt;name()-&gt;as_utf8(), m-&gt;name()-&gt;as_utf8());
4324     }
4325   }
4326 #endif
4327 
4328   return false;  // bail-out; let JVM_GetCallerClass do the work
4329 }
4330 
4331 bool LibraryCallKit::inline_fp_conversions(vmIntrinsics::ID id) {
4332   Node* arg = argument(0);
4333   Node* result = NULL;
4334 
4335   switch (id) {
4336   case vmIntrinsics::_floatToRawIntBits:    result = new (C) MoveF2INode(arg);  break;
4337   case vmIntrinsics::_intBitsToFloat:       result = new (C) MoveI2FNode(arg);  break;
4338   case vmIntrinsics::_doubleToRawLongBits:  result = new (C) MoveD2LNode(arg);  break;
4339   case vmIntrinsics::_longBitsToDouble:     result = new (C) MoveL2DNode(arg);  break;
4340 
4341   case vmIntrinsics::_doubleToLongBits: {
4342     // two paths (plus control) merge in a wood
4343     RegionNode *r = new (C) RegionNode(3);
4344     Node *phi = new (C) PhiNode(r, TypeLong::LONG);
4345 
4346     Node *cmpisnan = _gvn.transform(new (C) CmpDNode(arg, arg));
4347     // Build the boolean node
4348     Node *bolisnan = _gvn.transform(new (C) BoolNode(cmpisnan, BoolTest::ne));
4349 
4350     // Branch either way.
4351     // NaN case is less traveled, which makes all the difference.
4352     IfNode *ifisnan = create_and_xform_if(control(), bolisnan, PROB_STATIC_FREQUENT, COUNT_UNKNOWN);
4353     Node *opt_isnan = _gvn.transform(ifisnan);
4354     assert( opt_isnan-&gt;is_If(), "Expect an IfNode");
4355     IfNode *opt_ifisnan = (IfNode*)opt_isnan;
4356     Node *iftrue = _gvn.transform(new (C) IfTrueNode(opt_ifisnan));
4357 
4358     set_control(iftrue);
4359 
4360     static const jlong nan_bits = CONST64(0x7ff8000000000000);
4361     Node *slow_result = longcon(nan_bits); // return NaN
4362     phi-&gt;init_req(1, _gvn.transform( slow_result ));
4363     r-&gt;init_req(1, iftrue);
4364 
4365     // Else fall through
4366     Node *iffalse = _gvn.transform(new (C) IfFalseNode(opt_ifisnan));
4367     set_control(iffalse);
4368 
4369     phi-&gt;init_req(2, _gvn.transform(new (C) MoveD2LNode(arg)));
4370     r-&gt;init_req(2, iffalse);
4371 
4372     // Post merge
4373     set_control(_gvn.transform(r));
4374     record_for_igvn(r);
4375 
4376     C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
4377     result = phi;
4378     assert(result-&gt;bottom_type()-&gt;isa_long(), "must be");
4379     break;
4380   }
4381 
4382   case vmIntrinsics::_floatToIntBits: {
4383     // two paths (plus control) merge in a wood
4384     RegionNode *r = new (C) RegionNode(3);
4385     Node *phi = new (C) PhiNode(r, TypeInt::INT);
4386 
4387     Node *cmpisnan = _gvn.transform(new (C) CmpFNode(arg, arg));
4388     // Build the boolean node
4389     Node *bolisnan = _gvn.transform(new (C) BoolNode(cmpisnan, BoolTest::ne));
4390 
4391     // Branch either way.
4392     // NaN case is less traveled, which makes all the difference.
4393     IfNode *ifisnan = create_and_xform_if(control(), bolisnan, PROB_STATIC_FREQUENT, COUNT_UNKNOWN);
4394     Node *opt_isnan = _gvn.transform(ifisnan);
4395     assert( opt_isnan-&gt;is_If(), "Expect an IfNode");
4396     IfNode *opt_ifisnan = (IfNode*)opt_isnan;
4397     Node *iftrue = _gvn.transform(new (C) IfTrueNode(opt_ifisnan));
4398 
4399     set_control(iftrue);
4400 
4401     static const jint nan_bits = 0x7fc00000;
4402     Node *slow_result = makecon(TypeInt::make(nan_bits)); // return NaN
4403     phi-&gt;init_req(1, _gvn.transform( slow_result ));
4404     r-&gt;init_req(1, iftrue);
4405 
4406     // Else fall through
4407     Node *iffalse = _gvn.transform(new (C) IfFalseNode(opt_ifisnan));
4408     set_control(iffalse);
4409 
4410     phi-&gt;init_req(2, _gvn.transform(new (C) MoveF2INode(arg)));
4411     r-&gt;init_req(2, iffalse);
4412 
4413     // Post merge
4414     set_control(_gvn.transform(r));
4415     record_for_igvn(r);
4416 
4417     C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
4418     result = phi;
4419     assert(result-&gt;bottom_type()-&gt;isa_int(), "must be");
4420     break;
4421   }
4422 
4423   default:
4424     fatal_unexpected_iid(id);
4425     break;
4426   }
4427   set_result(_gvn.transform(result));
4428   return true;
4429 }
4430 
4431 #ifdef _LP64
4432 #define XTOP ,top() /*additional argument*/
4433 #else  //_LP64
4434 #define XTOP        /*no additional argument*/
4435 #endif //_LP64
4436 
4437 //----------------------inline_unsafe_copyMemory-------------------------
4438 // public native void sun.misc.Unsafe.copyMemory(Object srcBase, long srcOffset, Object destBase, long destOffset, long bytes);
4439 bool LibraryCallKit::inline_unsafe_copyMemory() {
4440   if (callee()-&gt;is_static())  return false;  // caller must have the capability!
4441   null_check_receiver();  // null-check receiver
4442   if (stopped())  return true;
4443 
4444   C-&gt;set_has_unsafe_access(true);  // Mark eventual nmethod as "unsafe".
4445 
4446   Node* src_ptr =         argument(1);   // type: oop
4447   Node* src_off = ConvL2X(argument(2));  // type: long
4448   Node* dst_ptr =         argument(4);   // type: oop
4449   Node* dst_off = ConvL2X(argument(5));  // type: long
4450   Node* size    = ConvL2X(argument(7));  // type: long
4451 
4452   assert(Unsafe_field_offset_to_byte_offset(11) == 11,
4453          "fieldOffset must be byte-scaled");
4454 
4455   Node* src = make_unsafe_address(src_ptr, src_off);
4456   Node* dst = make_unsafe_address(dst_ptr, dst_off);
4457 
4458   // Conservatively insert a memory barrier on all memory slices.
4459   // Do not let writes of the copy source or destination float below the copy.
4460   insert_mem_bar(Op_MemBarCPUOrder);
4461 
4462   // Call it.  Note that the length argument is not scaled.
4463   make_runtime_call(RC_LEAF|RC_NO_FP,
4464                     OptoRuntime::fast_arraycopy_Type(),
4465                     StubRoutines::unsafe_arraycopy(),
4466                     "unsafe_arraycopy",
4467                     TypeRawPtr::BOTTOM,
4468                     src, dst, size XTOP);
4469 
4470   // Do not let reads of the copy destination float above the copy.
4471   insert_mem_bar(Op_MemBarCPUOrder);
4472 
4473   return true;
4474 }
4475 
4476 //------------------------clone_coping-----------------------------------
4477 // Helper function for inline_native_clone.
4478 void LibraryCallKit::copy_to_clone(Node* obj, Node* alloc_obj, Node* obj_size, bool is_array, bool card_mark) {
4479   assert(obj_size != NULL, "");
4480   Node* raw_obj = alloc_obj-&gt;in(1);
4481   assert(alloc_obj-&gt;is_CheckCastPP() &amp;&amp; raw_obj-&gt;is_Proj() &amp;&amp; raw_obj-&gt;in(0)-&gt;is_Allocate(), "");
4482 
4483   AllocateNode* alloc = NULL;
4484   if (ReduceBulkZeroing) {
4485     // We will be completely responsible for initializing this object -
4486     // mark Initialize node as complete.
4487     alloc = AllocateNode::Ideal_allocation(alloc_obj, &amp;_gvn);
4488     // The object was just allocated - there should be no any stores!
4489     guarantee(alloc != NULL &amp;&amp; alloc-&gt;maybe_set_complete(&amp;_gvn), "");
4490     // Mark as complete_with_arraycopy so that on AllocateNode
4491     // expansion, we know this AllocateNode is initialized by an array
4492     // copy and a StoreStore barrier exists after the array copy.
4493     alloc-&gt;initialization()-&gt;set_complete_with_arraycopy();
4494   }
4495 
4496   // Copy the fastest available way.
4497   // TODO: generate fields copies for small objects instead.
4498   Node* src  = obj;
4499   Node* dest = alloc_obj;
4500   Node* size = _gvn.transform(obj_size);
4501 
4502   // Exclude the header but include array length to copy by 8 bytes words.
4503   // Can't use base_offset_in_bytes(bt) since basic type is unknown.
4504   int base_off = is_array ? arrayOopDesc::length_offset_in_bytes() :
4505                             instanceOopDesc::base_offset_in_bytes();
4506   // base_off:
4507   // 8  - 32-bit VM
4508   // 12 - 64-bit VM, compressed klass
4509   // 16 - 64-bit VM, normal klass
4510   if (base_off % BytesPerLong != 0) {
4511     assert(UseCompressedClassPointers, "");
4512     if (is_array) {
4513       // Exclude length to copy by 8 bytes words.
4514       base_off += sizeof(int);
4515     } else {
4516       // Include klass to copy by 8 bytes words.
4517       base_off = instanceOopDesc::klass_offset_in_bytes();
4518     }
4519     assert(base_off % BytesPerLong == 0, "expect 8 bytes alignment");
4520   }
4521   src  = basic_plus_adr(src,  base_off);
4522   dest = basic_plus_adr(dest, base_off);
4523 
4524   // Compute the length also, if needed:
4525   Node* countx = size;
4526   countx = _gvn.transform(new (C) SubXNode(countx, MakeConX(base_off)));
4527   countx = _gvn.transform(new (C) URShiftXNode(countx, intcon(LogBytesPerLong) ));
4528 
4529   const TypePtr* raw_adr_type = TypeRawPtr::BOTTOM;
4530   bool disjoint_bases = true;
4531   generate_unchecked_arraycopy(raw_adr_type, T_LONG, disjoint_bases,
4532                                src, NULL, dest, NULL, countx,
4533                                /*dest_uninitialized*/true);
4534 
4535   // If necessary, emit some card marks afterwards.  (Non-arrays only.)
4536   if (card_mark) {
4537     assert(!is_array, "");
4538     // Put in store barrier for any and all oops we are sticking
4539     // into this object.  (We could avoid this if we could prove
4540     // that the object type contains no oop fields at all.)
4541     Node* no_particular_value = NULL;
4542     Node* no_particular_field = NULL;
4543     int raw_adr_idx = Compile::AliasIdxRaw;
4544     post_barrier(control(),
4545                  memory(raw_adr_type),
4546                  alloc_obj,
4547                  no_particular_field,
4548                  raw_adr_idx,
4549                  no_particular_value,
4550                  T_OBJECT,
4551                  false);
4552   }
4553 
4554   // Do not let reads from the cloned object float above the arraycopy.
4555   if (alloc != NULL) {
4556     // Do not let stores that initialize this object be reordered with
4557     // a subsequent store that would make this object accessible by
4558     // other threads.
4559     // Record what AllocateNode this StoreStore protects so that
4560     // escape analysis can go from the MemBarStoreStoreNode to the
4561     // AllocateNode and eliminate the MemBarStoreStoreNode if possible
4562     // based on the escape status of the AllocateNode.
4563     insert_mem_bar(Op_MemBarStoreStore, alloc-&gt;proj_out(AllocateNode::RawAddress));
4564   } else {
4565     insert_mem_bar(Op_MemBarCPUOrder);
4566   }
4567 }
4568 
4569 //------------------------inline_native_clone----------------------------
4570 // protected native Object java.lang.Object.clone();
4571 //
4572 // Here are the simple edge cases:
4573 //  null receiver =&gt; normal trap
4574 //  virtual and clone was overridden =&gt; slow path to out-of-line clone
4575 //  not cloneable or finalizer =&gt; slow path to out-of-line Object.clone
4576 //
4577 // The general case has two steps, allocation and copying.
4578 // Allocation has two cases, and uses GraphKit::new_instance or new_array.
4579 //
4580 // Copying also has two cases, oop arrays and everything else.
4581 // Oop arrays use arrayof_oop_arraycopy (same as System.arraycopy).
4582 // Everything else uses the tight inline loop supplied by CopyArrayNode.
4583 //
4584 // These steps fold up nicely if and when the cloned object's klass
4585 // can be sharply typed as an object array, a type array, or an instance.
4586 //
4587 bool LibraryCallKit::inline_native_clone(bool is_virtual) {
4588   PhiNode* result_val;
4589 
4590   // Set the reexecute bit for the interpreter to reexecute
4591   // the bytecode that invokes Object.clone if deoptimization happens.
4592   { PreserveReexecuteState preexecs(this);
4593     jvms()-&gt;set_should_reexecute(true);
4594 
4595     Node* obj = null_check_receiver();
4596     if (stopped())  return true;
4597 
4598     Node* obj_klass = load_object_klass(obj);
4599     const TypeKlassPtr* tklass = _gvn.type(obj_klass)-&gt;isa_klassptr();
4600     const TypeOopPtr*   toop   = ((tklass != NULL)
4601                                 ? tklass-&gt;as_instance_type()
4602                                 : TypeInstPtr::NOTNULL);
4603 
4604     // Conservatively insert a memory barrier on all memory slices.
4605     // Do not let writes into the original float below the clone.
4606     insert_mem_bar(Op_MemBarCPUOrder);
4607 
4608     // paths into result_reg:
4609     enum {
4610       _slow_path = 1,     // out-of-line call to clone method (virtual or not)
4611       _objArray_path,     // plain array allocation, plus arrayof_oop_arraycopy
4612       _array_path,        // plain array allocation, plus arrayof_long_arraycopy
4613       _instance_path,     // plain instance allocation, plus arrayof_long_arraycopy
4614       PATH_LIMIT
4615     };
4616     RegionNode* result_reg = new(C) RegionNode(PATH_LIMIT);
4617     result_val             = new(C) PhiNode(result_reg,
4618                                             TypeInstPtr::NOTNULL);
4619     PhiNode*    result_i_o = new(C) PhiNode(result_reg, Type::ABIO);
4620     PhiNode*    result_mem = new(C) PhiNode(result_reg, Type::MEMORY,
4621                                             TypePtr::BOTTOM);
4622     record_for_igvn(result_reg);
4623 
4624     const TypePtr* raw_adr_type = TypeRawPtr::BOTTOM;
4625     int raw_adr_idx = Compile::AliasIdxRaw;
4626 
4627     Node* array_ctl = generate_array_guard(obj_klass, (RegionNode*)NULL);
4628     if (array_ctl != NULL) {
4629       // It's an array.
4630       PreserveJVMState pjvms(this);
4631       set_control(array_ctl);
4632       Node* obj_length = load_array_length(obj);
4633       Node* obj_size  = NULL;
4634       Node* alloc_obj = new_array(obj_klass, obj_length, 0, &amp;obj_size);  // no arguments to push
4635 
4636       if (!use_ReduceInitialCardMarks()) {
4637         // If it is an oop array, it requires very special treatment,
4638         // because card marking is required on each card of the array.
4639         Node* is_obja = generate_objArray_guard(obj_klass, (RegionNode*)NULL);
4640         if (is_obja != NULL) {
4641           PreserveJVMState pjvms2(this);
4642           set_control(is_obja);
4643           // Generate a direct call to the right arraycopy function(s).
4644           bool disjoint_bases = true;
4645           bool length_never_negative = true;
4646           generate_arraycopy(TypeAryPtr::OOPS, T_OBJECT,
4647                              obj, intcon(0), alloc_obj, intcon(0),
4648                              obj_length,
4649                              disjoint_bases, length_never_negative);
4650           result_reg-&gt;init_req(_objArray_path, control());
4651           result_val-&gt;init_req(_objArray_path, alloc_obj);
4652           result_i_o -&gt;set_req(_objArray_path, i_o());
4653           result_mem -&gt;set_req(_objArray_path, reset_memory());
4654         }
4655       }
4656       // Otherwise, there are no card marks to worry about.
4657       // (We can dispense with card marks if we know the allocation
4658       //  comes out of eden (TLAB)...  In fact, ReduceInitialCardMarks
4659       //  causes the non-eden paths to take compensating steps to
4660       //  simulate a fresh allocation, so that no further
4661       //  card marks are required in compiled code to initialize
4662       //  the object.)
4663 
4664       if (!stopped()) {
4665         copy_to_clone(obj, alloc_obj, obj_size, true, false);
4666 
4667         // Present the results of the copy.
4668         result_reg-&gt;init_req(_array_path, control());
4669         result_val-&gt;init_req(_array_path, alloc_obj);
4670         result_i_o -&gt;set_req(_array_path, i_o());
4671         result_mem -&gt;set_req(_array_path, reset_memory());
4672       }
4673     }
4674 
4675     // We only go to the instance fast case code if we pass a number of guards.
4676     // The paths which do not pass are accumulated in the slow_region.
4677     RegionNode* slow_region = new (C) RegionNode(1);
4678     record_for_igvn(slow_region);
4679     if (!stopped()) {
4680       // It's an instance (we did array above).  Make the slow-path tests.
4681       // If this is a virtual call, we generate a funny guard.  We grab
4682       // the vtable entry corresponding to clone() from the target object.
4683       // If the target method which we are calling happens to be the
4684       // Object clone() method, we pass the guard.  We do not need this
4685       // guard for non-virtual calls; the caller is known to be the native
4686       // Object clone().
4687       if (is_virtual) {
4688         generate_virtual_guard(obj_klass, slow_region);
4689       }
4690 
4691       // The object must be cloneable and must not have a finalizer.
4692       // Both of these conditions may be checked in a single test.
4693       // We could optimize the cloneable test further, but we don't care.
4694       generate_access_flags_guard(obj_klass,
4695                                   // Test both conditions:
4696                                   JVM_ACC_IS_CLONEABLE | JVM_ACC_HAS_FINALIZER,
4697                                   // Must be cloneable but not finalizer:
4698                                   JVM_ACC_IS_CLONEABLE,
4699                                   slow_region);
4700     }
4701 
4702     if (!stopped()) {
4703       // It's an instance, and it passed the slow-path tests.
4704       PreserveJVMState pjvms(this);
4705       Node* obj_size  = NULL;
4706       // Need to deoptimize on exception from allocation since Object.clone intrinsic
4707       // is reexecuted if deoptimization occurs and there could be problems when merging
4708       // exception state between multiple Object.clone versions (reexecute=true vs reexecute=false).
4709       Node* alloc_obj = new_instance(obj_klass, NULL, &amp;obj_size, /*deoptimize_on_exception=*/true);
4710 
4711       copy_to_clone(obj, alloc_obj, obj_size, false, !use_ReduceInitialCardMarks());
4712 
4713       // Present the results of the slow call.
4714       result_reg-&gt;init_req(_instance_path, control());
4715       result_val-&gt;init_req(_instance_path, alloc_obj);
4716       result_i_o -&gt;set_req(_instance_path, i_o());
4717       result_mem -&gt;set_req(_instance_path, reset_memory());
4718     }
4719 
4720     // Generate code for the slow case.  We make a call to clone().
4721     set_control(_gvn.transform(slow_region));
4722     if (!stopped()) {
4723       PreserveJVMState pjvms(this);
4724       CallJavaNode* slow_call = generate_method_call(vmIntrinsics::_clone, is_virtual);
4725       Node* slow_result = set_results_for_java_call(slow_call);
4726       // this-&gt;control() comes from set_results_for_java_call
4727       result_reg-&gt;init_req(_slow_path, control());
4728       result_val-&gt;init_req(_slow_path, slow_result);
4729       result_i_o -&gt;set_req(_slow_path, i_o());
4730       result_mem -&gt;set_req(_slow_path, reset_memory());
4731     }
4732 
4733     // Return the combined state.
4734     set_control(    _gvn.transform(result_reg));
4735     set_i_o(        _gvn.transform(result_i_o));
4736     set_all_memory( _gvn.transform(result_mem));
4737   } // original reexecute is set back here
4738 
4739   set_result(_gvn.transform(result_val));
4740   return true;
4741 }
4742 
4743 //------------------------------basictype2arraycopy----------------------------
4744 address LibraryCallKit::basictype2arraycopy(BasicType t,
4745                                             Node* src_offset,
4746                                             Node* dest_offset,
4747                                             bool disjoint_bases,
4748                                             const char* &amp;name,
4749                                             bool dest_uninitialized) {
4750   const TypeInt* src_offset_inttype  = gvn().find_int_type(src_offset);;
4751   const TypeInt* dest_offset_inttype = gvn().find_int_type(dest_offset);;
4752 
4753   bool aligned = false;
4754   bool disjoint = disjoint_bases;
4755 
4756   // if the offsets are the same, we can treat the memory regions as
4757   // disjoint, because either the memory regions are in different arrays,
4758   // or they are identical (which we can treat as disjoint.)  We can also
4759   // treat a copy with a destination index  less that the source index
4760   // as disjoint since a low-&gt;high copy will work correctly in this case.
4761   if (src_offset_inttype != NULL &amp;&amp; src_offset_inttype-&gt;is_con() &amp;&amp;
4762       dest_offset_inttype != NULL &amp;&amp; dest_offset_inttype-&gt;is_con()) {
4763     // both indices are constants
4764     int s_offs = src_offset_inttype-&gt;get_con();
4765     int d_offs = dest_offset_inttype-&gt;get_con();
4766     int element_size = type2aelembytes(t);
4767     aligned = ((arrayOopDesc::base_offset_in_bytes(t) + s_offs * element_size) % HeapWordSize == 0) &amp;&amp;
4768               ((arrayOopDesc::base_offset_in_bytes(t) + d_offs * element_size) % HeapWordSize == 0);
4769     if (s_offs &gt;= d_offs)  disjoint = true;
4770   } else if (src_offset == dest_offset &amp;&amp; src_offset != NULL) {
4771     // This can occur if the offsets are identical non-constants.
4772     disjoint = true;
4773   }
4774 
4775   return StubRoutines::select_arraycopy_function(t, aligned, disjoint, name, dest_uninitialized);
4776 }
4777 
4778 
4779 //------------------------------inline_arraycopy-----------------------
4780 // public static native void java.lang.System.arraycopy(Object src,  int  srcPos,
4781 //                                                      Object dest, int destPos,
4782 //                                                      int length);
4783 bool LibraryCallKit::inline_arraycopy() {
4784   // Get the arguments.
4785   Node* src         = argument(0);  // type: oop
4786   Node* src_offset  = argument(1);  // type: int
4787   Node* dest        = argument(2);  // type: oop
4788   Node* dest_offset = argument(3);  // type: int
4789   Node* length      = argument(4);  // type: int
4790 
4791   // Compile time checks.  If any of these checks cannot be verified at compile time,
4792   // we do not make a fast path for this call.  Instead, we let the call remain as it
4793   // is.  The checks we choose to mandate at compile time are:
4794   //
4795   // (1) src and dest are arrays.
4796   const Type* src_type  = src-&gt;Value(&amp;_gvn);
4797   const Type* dest_type = dest-&gt;Value(&amp;_gvn);
4798   const TypeAryPtr* top_src  = src_type-&gt;isa_aryptr();
4799   const TypeAryPtr* top_dest = dest_type-&gt;isa_aryptr();
4800 
4801   // Do we have the type of src?
4802   bool has_src = (top_src != NULL &amp;&amp; top_src-&gt;klass() != NULL);
4803   // Do we have the type of dest?
4804   bool has_dest = (top_dest != NULL &amp;&amp; top_dest-&gt;klass() != NULL);
4805   // Is the type for src from speculation?
4806   bool src_spec = false;
4807   // Is the type for dest from speculation?
4808   bool dest_spec = false;
4809 
4810   if (!has_src || !has_dest) {
4811     // We don't have sufficient type information, let's see if
4812     // speculative types can help. We need to have types for both src
4813     // and dest so that it pays off.
4814 
4815     // Do we already have or could we have type information for src
4816     bool could_have_src = has_src;
4817     // Do we already have or could we have type information for dest
4818     bool could_have_dest = has_dest;
4819 
4820     ciKlass* src_k = NULL;
4821     if (!has_src) {
4822       src_k = src_type-&gt;speculative_type();
4823       if (src_k != NULL &amp;&amp; src_k-&gt;is_array_klass()) {
4824         could_have_src = true;
4825       }
4826     }
4827 
4828     ciKlass* dest_k = NULL;
4829     if (!has_dest) {
4830       dest_k = dest_type-&gt;speculative_type();
4831       if (dest_k != NULL &amp;&amp; dest_k-&gt;is_array_klass()) {
4832         could_have_dest = true;
4833       }
4834     }
4835 
4836     if (could_have_src &amp;&amp; could_have_dest) {
4837       // This is going to pay off so emit the required guards
4838       if (!has_src) {
4839         src = maybe_cast_profiled_obj(src, src_k);
4840         src_type  = _gvn.type(src);
4841         top_src  = src_type-&gt;isa_aryptr();
4842         has_src = (top_src != NULL &amp;&amp; top_src-&gt;klass() != NULL);
4843         src_spec = true;
4844       }
4845       if (!has_dest) {
4846         dest = maybe_cast_profiled_obj(dest, dest_k);
4847         dest_type  = _gvn.type(dest);
4848         top_dest  = dest_type-&gt;isa_aryptr();
4849         has_dest = (top_dest != NULL &amp;&amp; top_dest-&gt;klass() != NULL);
4850         dest_spec = true;
4851       }
4852     }
4853   }
4854 
4855   if (!has_src || !has_dest) {
4856     // Conservatively insert a memory barrier on all memory slices.
4857     // Do not let writes into the source float below the arraycopy.
4858     insert_mem_bar(Op_MemBarCPUOrder);
4859 
4860     // Call StubRoutines::generic_arraycopy stub.
4861     generate_arraycopy(TypeRawPtr::BOTTOM, T_CONFLICT,
4862                        src, src_offset, dest, dest_offset, length);
4863 
4864     // Do not let reads from the destination float above the arraycopy.
4865     // Since we cannot type the arrays, we don't know which slices
4866     // might be affected.  We could restrict this barrier only to those
4867     // memory slices which pertain to array elements--but don't bother.
4868     if (!InsertMemBarAfterArraycopy)
4869       // (If InsertMemBarAfterArraycopy, there is already one in place.)
4870       insert_mem_bar(Op_MemBarCPUOrder);
4871     return true;
4872   }
4873 
4874   // (2) src and dest arrays must have elements of the same BasicType
4875   // Figure out the size and type of the elements we will be copying.
4876   BasicType src_elem  =  top_src-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
4877   BasicType dest_elem = top_dest-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
4878   if (src_elem  == T_ARRAY)  src_elem  = T_OBJECT;
4879   if (dest_elem == T_ARRAY)  dest_elem = T_OBJECT;
4880 
4881   if (src_elem != dest_elem || dest_elem == T_VOID) {
4882     // The component types are not the same or are not recognized.  Punt.
4883     // (But, avoid the native method wrapper to JVM_ArrayCopy.)
4884     generate_slow_arraycopy(TypePtr::BOTTOM,
4885                             src, src_offset, dest, dest_offset, length,
4886                             /*dest_uninitialized*/false);
4887     return true;
4888   }
4889 
4890   if (src_elem == T_OBJECT) {
4891     // If both arrays are object arrays then having the exact types
4892     // for both will remove the need for a subtype check at runtime
4893     // before the call and may make it possible to pick a faster copy
4894     // routine (without a subtype check on every element)
4895     // Do we have the exact type of src?
4896     bool could_have_src = src_spec;
4897     // Do we have the exact type of dest?
4898     bool could_have_dest = dest_spec;
4899     ciKlass* src_k = top_src-&gt;klass();
4900     ciKlass* dest_k = top_dest-&gt;klass();
4901     if (!src_spec) {
4902       src_k = src_type-&gt;speculative_type();
4903       if (src_k != NULL &amp;&amp; src_k-&gt;is_array_klass()) {
4904           could_have_src = true;
4905       }
4906     }
4907     if (!dest_spec) {
4908       dest_k = dest_type-&gt;speculative_type();
4909       if (dest_k != NULL &amp;&amp; dest_k-&gt;is_array_klass()) {
4910         could_have_dest = true;
4911       }
4912     }
4913     if (could_have_src &amp;&amp; could_have_dest) {
4914       // If we can have both exact types, emit the missing guards
4915       if (could_have_src &amp;&amp; !src_spec) {
4916         src = maybe_cast_profiled_obj(src, src_k);
4917       }
4918       if (could_have_dest &amp;&amp; !dest_spec) {
4919         dest = maybe_cast_profiled_obj(dest, dest_k);
4920       }
4921     }
4922   }
4923 
4924   //---------------------------------------------------------------------------
4925   // We will make a fast path for this call to arraycopy.
4926 
4927   // We have the following tests left to perform:
4928   //
4929   // (3) src and dest must not be null.
4930   // (4) src_offset must not be negative.
4931   // (5) dest_offset must not be negative.
4932   // (6) length must not be negative.
4933   // (7) src_offset + length must not exceed length of src.
4934   // (8) dest_offset + length must not exceed length of dest.
4935   // (9) each element of an oop array must be assignable
4936 
4937   RegionNode* slow_region = new (C) RegionNode(1);
4938   record_for_igvn(slow_region);
4939 
4940   // (3) operands must not be null
4941   // We currently perform our null checks with the null_check routine.
4942   // This means that the null exceptions will be reported in the caller
4943   // rather than (correctly) reported inside of the native arraycopy call.
4944   // This should be corrected, given time.  We do our null check with the
4945   // stack pointer restored.
4946   src  = null_check(src,  T_ARRAY);
4947   dest = null_check(dest, T_ARRAY);
4948 
4949   // (4) src_offset must not be negative.
4950   generate_negative_guard(src_offset, slow_region);
4951 
4952   // (5) dest_offset must not be negative.
4953   generate_negative_guard(dest_offset, slow_region);
4954 
4955   // (6) length must not be negative (moved to generate_arraycopy()).
4956   // generate_negative_guard(length, slow_region);
4957 
4958   // (7) src_offset + length must not exceed length of src.
4959   generate_limit_guard(src_offset, length,
4960                        load_array_length(src),
4961                        slow_region);
4962 
4963   // (8) dest_offset + length must not exceed length of dest.
4964   generate_limit_guard(dest_offset, length,
4965                        load_array_length(dest),
4966                        slow_region);
4967 
4968   // (9) each element of an oop array must be assignable
4969   // The generate_arraycopy subroutine checks this.
4970 
4971   // This is where the memory effects are placed:
4972   const TypePtr* adr_type = TypeAryPtr::get_array_body_type(dest_elem);
4973   generate_arraycopy(adr_type, dest_elem,
4974                      src, src_offset, dest, dest_offset, length,
4975                      false, false, slow_region);
4976 
4977   return true;
4978 }
4979 
4980 //-----------------------------generate_arraycopy----------------------
4981 // Generate an optimized call to arraycopy.
4982 // Caller must guard against non-arrays.
4983 // Caller must determine a common array basic-type for both arrays.
4984 // Caller must validate offsets against array bounds.
4985 // The slow_region has already collected guard failure paths
4986 // (such as out of bounds length or non-conformable array types).
4987 // The generated code has this shape, in general:
4988 //
4989 //     if (length == 0)  return   // via zero_path
4990 //     slowval = -1
4991 //     if (types unknown) {
4992 //       slowval = call generic copy loop
4993 //       if (slowval == 0)  return  // via checked_path
4994 //     } else if (indexes in bounds) {
4995 //       if ((is object array) &amp;&amp; !(array type check)) {
4996 //         slowval = call checked copy loop
4997 //         if (slowval == 0)  return  // via checked_path
4998 //       } else {
4999 //         call bulk copy loop
5000 //         return  // via fast_path
5001 //       }
5002 //     }
5003 //     // adjust params for remaining work:
5004 //     if (slowval != -1) {
5005 //       n = -1^slowval; src_offset += n; dest_offset += n; length -= n
5006 //     }
5007 //   slow_region:
5008 //     call slow arraycopy(src, src_offset, dest, dest_offset, length)
5009 //     return  // via slow_call_path
5010 //
5011 // This routine is used from several intrinsics:  System.arraycopy,
5012 // Object.clone (the array subcase), and Arrays.copyOf[Range].
5013 //
5014 void
5015 LibraryCallKit::generate_arraycopy(const TypePtr* adr_type,
5016                                    BasicType basic_elem_type,
5017                                    Node* src,  Node* src_offset,
5018                                    Node* dest, Node* dest_offset,
5019                                    Node* copy_length,
5020                                    bool disjoint_bases,
5021                                    bool length_never_negative,
5022                                    RegionNode* slow_region) {
5023 
5024   if (slow_region == NULL) {
5025     slow_region = new(C) RegionNode(1);
5026     record_for_igvn(slow_region);
5027   }
5028 
5029   Node* original_dest      = dest;
5030   AllocateArrayNode* alloc = NULL;  // used for zeroing, if needed
5031   bool  dest_uninitialized = false;
5032 
5033   // See if this is the initialization of a newly-allocated array.
5034   // If so, we will take responsibility here for initializing it to zero.
5035   // (Note:  Because tightly_coupled_allocation performs checks on the
5036   // out-edges of the dest, we need to avoid making derived pointers
5037   // from it until we have checked its uses.)
5038   if (ReduceBulkZeroing
5039       &amp;&amp; !ZeroTLAB              // pointless if already zeroed
5040       &amp;&amp; basic_elem_type != T_CONFLICT // avoid corner case
5041       &amp;&amp; !src-&gt;eqv_uncast(dest)
5042       &amp;&amp; ((alloc = tightly_coupled_allocation(dest, slow_region))
5043           != NULL)
5044       &amp;&amp; _gvn.find_int_con(alloc-&gt;in(AllocateNode::ALength), 1) &gt; 0
5045       &amp;&amp; alloc-&gt;maybe_set_complete(&amp;_gvn)) {
5046     // "You break it, you buy it."
5047     InitializeNode* init = alloc-&gt;initialization();
5048     assert(init-&gt;is_complete(), "we just did this");
5049     init-&gt;set_complete_with_arraycopy();
5050     assert(dest-&gt;is_CheckCastPP(), "sanity");
5051     assert(dest-&gt;in(0)-&gt;in(0) == init, "dest pinned");
5052     adr_type = TypeRawPtr::BOTTOM;  // all initializations are into raw memory
5053     // From this point on, every exit path is responsible for
5054     // initializing any non-copied parts of the object to zero.
5055     // Also, if this flag is set we make sure that arraycopy interacts properly
5056     // with G1, eliding pre-barriers. See CR 6627983.
5057     dest_uninitialized = true;
5058   } else {
5059     // No zeroing elimination here.
5060     alloc             = NULL;
5061     //original_dest   = dest;
5062     //dest_uninitialized = false;
5063   }
5064 
5065   // Results are placed here:
5066   enum { fast_path        = 1,  // normal void-returning assembly stub
5067          checked_path     = 2,  // special assembly stub with cleanup
5068          slow_call_path   = 3,  // something went wrong; call the VM
5069          zero_path        = 4,  // bypass when length of copy is zero
5070          bcopy_path       = 5,  // copy primitive array by 64-bit blocks
5071          PATH_LIMIT       = 6
5072   };
5073   RegionNode* result_region = new(C) RegionNode(PATH_LIMIT);
5074   PhiNode*    result_i_o    = new(C) PhiNode(result_region, Type::ABIO);
5075   PhiNode*    result_memory = new(C) PhiNode(result_region, Type::MEMORY, adr_type);
5076   record_for_igvn(result_region);
5077   _gvn.set_type_bottom(result_i_o);
5078   _gvn.set_type_bottom(result_memory);
5079   assert(adr_type != TypePtr::BOTTOM, "must be RawMem or a T[] slice");
5080 
5081   // The slow_control path:
5082   Node* slow_control;
5083   Node* slow_i_o = i_o();
5084   Node* slow_mem = memory(adr_type);
5085   debug_only(slow_control = (Node*) badAddress);
5086 
5087   // Checked control path:
5088   Node* checked_control = top();
5089   Node* checked_mem     = NULL;
5090   Node* checked_i_o     = NULL;
5091   Node* checked_value   = NULL;
5092 
5093   if (basic_elem_type == T_CONFLICT) {
5094     assert(!dest_uninitialized, "");
5095     Node* cv = generate_generic_arraycopy(adr_type,
5096                                           src, src_offset, dest, dest_offset,
5097                                           copy_length, dest_uninitialized);
5098     if (cv == NULL)  cv = intcon(-1);  // failure (no stub available)
5099     checked_control = control();
5100     checked_i_o     = i_o();
5101     checked_mem     = memory(adr_type);
5102     checked_value   = cv;
5103     set_control(top());         // no fast path
5104   }
5105 
5106   Node* not_pos = generate_nonpositive_guard(copy_length, length_never_negative);
5107   if (not_pos != NULL) {
5108     PreserveJVMState pjvms(this);
5109     set_control(not_pos);
5110 
5111     // (6) length must not be negative.
5112     if (!length_never_negative) {
5113       generate_negative_guard(copy_length, slow_region);
5114     }
5115 
5116     // copy_length is 0.
5117     if (!stopped() &amp;&amp; dest_uninitialized) {
5118       Node* dest_length = alloc-&gt;in(AllocateNode::ALength);
5119       if (copy_length-&gt;eqv_uncast(dest_length)
5120           || _gvn.find_int_con(dest_length, 1) &lt;= 0) {
5121         // There is no zeroing to do. No need for a secondary raw memory barrier.
5122       } else {
5123         // Clear the whole thing since there are no source elements to copy.
5124         generate_clear_array(adr_type, dest, basic_elem_type,
5125                              intcon(0), NULL,
5126                              alloc-&gt;in(AllocateNode::AllocSize));
5127         // Use a secondary InitializeNode as raw memory barrier.
5128         // Currently it is needed only on this path since other
5129         // paths have stub or runtime calls as raw memory barriers.
5130         InitializeNode* init = insert_mem_bar_volatile(Op_Initialize,
5131                                                        Compile::AliasIdxRaw,
5132                                                        top())-&gt;as_Initialize();
5133         init-&gt;set_complete(&amp;_gvn);  // (there is no corresponding AllocateNode)
5134       }
5135     }
5136 
5137     // Present the results of the fast call.
5138     result_region-&gt;init_req(zero_path, control());
5139     result_i_o   -&gt;init_req(zero_path, i_o());
5140     result_memory-&gt;init_req(zero_path, memory(adr_type));
5141   }
5142 
5143   if (!stopped() &amp;&amp; dest_uninitialized) {
5144     // We have to initialize the *uncopied* part of the array to zero.
5145     // The copy destination is the slice dest[off..off+len].  The other slices
5146     // are dest_head = dest[0..off] and dest_tail = dest[off+len..dest.length].
5147     Node* dest_size   = alloc-&gt;in(AllocateNode::AllocSize);
5148     Node* dest_length = alloc-&gt;in(AllocateNode::ALength);
5149     Node* dest_tail   = _gvn.transform(new(C) AddINode(dest_offset,
5150                                                           copy_length));
5151 
5152     // If there is a head section that needs zeroing, do it now.
5153     if (find_int_con(dest_offset, -1) != 0) {
5154       generate_clear_array(adr_type, dest, basic_elem_type,
5155                            intcon(0), dest_offset,
5156                            NULL);
5157     }
5158 
5159     // Next, perform a dynamic check on the tail length.
5160     // It is often zero, and we can win big if we prove this.
5161     // There are two wins:  Avoid generating the ClearArray
5162     // with its attendant messy index arithmetic, and upgrade
5163     // the copy to a more hardware-friendly word size of 64 bits.
5164     Node* tail_ctl = NULL;
5165     if (!stopped() &amp;&amp; !dest_tail-&gt;eqv_uncast(dest_length)) {
5166       Node* cmp_lt   = _gvn.transform(new(C) CmpINode(dest_tail, dest_length));
5167       Node* bol_lt   = _gvn.transform(new(C) BoolNode(cmp_lt, BoolTest::lt));
5168       tail_ctl = generate_slow_guard(bol_lt, NULL);
5169       assert(tail_ctl != NULL || !stopped(), "must be an outcome");
5170     }
5171 
5172     // At this point, let's assume there is no tail.
5173     if (!stopped() &amp;&amp; alloc != NULL &amp;&amp; basic_elem_type != T_OBJECT) {
5174       // There is no tail.  Try an upgrade to a 64-bit copy.
5175       bool didit = false;
5176       { PreserveJVMState pjvms(this);
5177         didit = generate_block_arraycopy(adr_type, basic_elem_type, alloc,
5178                                          src, src_offset, dest, dest_offset,
5179                                          dest_size, dest_uninitialized);
5180         if (didit) {
5181           // Present the results of the block-copying fast call.
5182           result_region-&gt;init_req(bcopy_path, control());
5183           result_i_o   -&gt;init_req(bcopy_path, i_o());
5184           result_memory-&gt;init_req(bcopy_path, memory(adr_type));
5185         }
5186       }
5187       if (didit)
5188         set_control(top());     // no regular fast path
5189     }
5190 
5191     // Clear the tail, if any.
5192     if (tail_ctl != NULL) {
5193       Node* notail_ctl = stopped() ? NULL : control();
5194       set_control(tail_ctl);
5195       if (notail_ctl == NULL) {
5196         generate_clear_array(adr_type, dest, basic_elem_type,
5197                              dest_tail, NULL,
5198                              dest_size);
5199       } else {
5200         // Make a local merge.
5201         Node* done_ctl = new(C) RegionNode(3);
5202         Node* done_mem = new(C) PhiNode(done_ctl, Type::MEMORY, adr_type);
5203         done_ctl-&gt;init_req(1, notail_ctl);
5204         done_mem-&gt;init_req(1, memory(adr_type));
5205         generate_clear_array(adr_type, dest, basic_elem_type,
5206                              dest_tail, NULL,
5207                              dest_size);
5208         done_ctl-&gt;init_req(2, control());
5209         done_mem-&gt;init_req(2, memory(adr_type));
5210         set_control( _gvn.transform(done_ctl));
5211         set_memory(  _gvn.transform(done_mem), adr_type );
5212       }
5213     }
5214   }
5215 
5216   BasicType copy_type = basic_elem_type;
5217   assert(basic_elem_type != T_ARRAY, "caller must fix this");
5218   if (!stopped() &amp;&amp; copy_type == T_OBJECT) {
5219     // If src and dest have compatible element types, we can copy bits.
5220     // Types S[] and D[] are compatible if D is a supertype of S.
5221     //
5222     // If they are not, we will use checked_oop_disjoint_arraycopy,
5223     // which performs a fast optimistic per-oop check, and backs off
5224     // further to JVM_ArrayCopy on the first per-oop check that fails.
5225     // (Actually, we don't move raw bits only; the GC requires card marks.)
5226 
5227     // Get the Klass* for both src and dest
5228     Node* src_klass  = load_object_klass(src);
5229     Node* dest_klass = load_object_klass(dest);
5230 
5231     // Generate the subtype check.
5232     // This might fold up statically, or then again it might not.
5233     //
5234     // Non-static example:  Copying List&lt;String&gt;.elements to a new String[].
5235     // The backing store for a List&lt;String&gt; is always an Object[],
5236     // but its elements are always type String, if the generic types
5237     // are correct at the source level.
5238     //
5239     // Test S[] against D[], not S against D, because (probably)
5240     // the secondary supertype cache is less busy for S[] than S.
5241     // This usually only matters when D is an interface.
5242     Node* not_subtype_ctrl = gen_subtype_check(src_klass, dest_klass);
5243     // Plug failing path into checked_oop_disjoint_arraycopy
5244     if (not_subtype_ctrl != top()) {
5245       PreserveJVMState pjvms(this);
5246       set_control(not_subtype_ctrl);
5247       // (At this point we can assume disjoint_bases, since types differ.)
5248       int ek_offset = in_bytes(ObjArrayKlass::element_klass_offset());
5249       Node* p1 = basic_plus_adr(dest_klass, ek_offset);
5250       Node* n1 = LoadKlassNode::make(_gvn, NULL, immutable_memory(), p1, TypeRawPtr::BOTTOM);
5251       Node* dest_elem_klass = _gvn.transform(n1);
5252       Node* cv = generate_checkcast_arraycopy(adr_type,
5253                                               dest_elem_klass,
5254                                               src, src_offset, dest, dest_offset,
5255                                               ConvI2X(copy_length), dest_uninitialized);
5256       if (cv == NULL)  cv = intcon(-1);  // failure (no stub available)
5257       checked_control = control();
5258       checked_i_o     = i_o();
5259       checked_mem     = memory(adr_type);
5260       checked_value   = cv;
5261     }
5262     // At this point we know we do not need type checks on oop stores.
5263 
5264     // Let's see if we need card marks:
5265     if (alloc != NULL &amp;&amp; use_ReduceInitialCardMarks()) {
5266       // If we do not need card marks, copy using the jint or jlong stub.
5267       copy_type = LP64_ONLY(UseCompressedOops ? T_INT : T_LONG) NOT_LP64(T_INT);
5268       assert(type2aelembytes(basic_elem_type) == type2aelembytes(copy_type),
5269              "sizes agree");
5270     }
5271   }
5272 
5273   if (!stopped()) {
5274     // Generate the fast path, if possible.
5275     PreserveJVMState pjvms(this);
5276     generate_unchecked_arraycopy(adr_type, copy_type, disjoint_bases,
5277                                  src, src_offset, dest, dest_offset,
5278                                  ConvI2X(copy_length), dest_uninitialized);
5279 
5280     // Present the results of the fast call.
5281     result_region-&gt;init_req(fast_path, control());
5282     result_i_o   -&gt;init_req(fast_path, i_o());
5283     result_memory-&gt;init_req(fast_path, memory(adr_type));
5284   }
5285 
5286   // Here are all the slow paths up to this point, in one bundle:
5287   slow_control = top();
5288   if (slow_region != NULL)
5289     slow_control = _gvn.transform(slow_region);
5290   DEBUG_ONLY(slow_region = (RegionNode*)badAddress);
5291 
5292   set_control(checked_control);
5293   if (!stopped()) {
5294     // Clean up after the checked call.
5295     // The returned value is either 0 or -1^K,
5296     // where K = number of partially transferred array elements.
5297     Node* cmp = _gvn.transform(new(C) CmpINode(checked_value, intcon(0)));
5298     Node* bol = _gvn.transform(new(C) BoolNode(cmp, BoolTest::eq));
5299     IfNode* iff = create_and_map_if(control(), bol, PROB_MAX, COUNT_UNKNOWN);
5300 
5301     // If it is 0, we are done, so transfer to the end.
5302     Node* checks_done = _gvn.transform(new(C) IfTrueNode(iff));
5303     result_region-&gt;init_req(checked_path, checks_done);
5304     result_i_o   -&gt;init_req(checked_path, checked_i_o);
5305     result_memory-&gt;init_req(checked_path, checked_mem);
5306 
5307     // If it is not zero, merge into the slow call.
5308     set_control( _gvn.transform(new(C) IfFalseNode(iff) ));
5309     RegionNode* slow_reg2 = new(C) RegionNode(3);
5310     PhiNode*    slow_i_o2 = new(C) PhiNode(slow_reg2, Type::ABIO);
5311     PhiNode*    slow_mem2 = new(C) PhiNode(slow_reg2, Type::MEMORY, adr_type);
5312     record_for_igvn(slow_reg2);
5313     slow_reg2  -&gt;init_req(1, slow_control);
5314     slow_i_o2  -&gt;init_req(1, slow_i_o);
5315     slow_mem2  -&gt;init_req(1, slow_mem);
5316     slow_reg2  -&gt;init_req(2, control());
5317     slow_i_o2  -&gt;init_req(2, checked_i_o);
5318     slow_mem2  -&gt;init_req(2, checked_mem);
5319 
5320     slow_control = _gvn.transform(slow_reg2);
5321     slow_i_o     = _gvn.transform(slow_i_o2);
5322     slow_mem     = _gvn.transform(slow_mem2);
5323 
5324     if (alloc != NULL) {
5325       // We'll restart from the very beginning, after zeroing the whole thing.
5326       // This can cause double writes, but that's OK since dest is brand new.
5327       // So we ignore the low 31 bits of the value returned from the stub.
5328     } else {
5329       // We must continue the copy exactly where it failed, or else
5330       // another thread might see the wrong number of writes to dest.
5331       Node* checked_offset = _gvn.transform(new(C) XorINode(checked_value, intcon(-1)));
5332       Node* slow_offset    = new(C) PhiNode(slow_reg2, TypeInt::INT);
5333       slow_offset-&gt;init_req(1, intcon(0));
5334       slow_offset-&gt;init_req(2, checked_offset);
5335       slow_offset  = _gvn.transform(slow_offset);
5336 
5337       // Adjust the arguments by the conditionally incoming offset.
5338       Node* src_off_plus  = _gvn.transform(new(C) AddINode(src_offset,  slow_offset));
5339       Node* dest_off_plus = _gvn.transform(new(C) AddINode(dest_offset, slow_offset));
5340       Node* length_minus  = _gvn.transform(new(C) SubINode(copy_length, slow_offset));
5341 
5342       // Tweak the node variables to adjust the code produced below:
5343       src_offset  = src_off_plus;
5344       dest_offset = dest_off_plus;
5345       copy_length = length_minus;
5346     }
5347   }
5348 
5349   set_control(slow_control);
5350   if (!stopped()) {
5351     // Generate the slow path, if needed.
5352     PreserveJVMState pjvms(this);   // replace_in_map may trash the map
5353 
5354     set_memory(slow_mem, adr_type);
5355     set_i_o(slow_i_o);
5356 
5357     if (dest_uninitialized) {
5358       generate_clear_array(adr_type, dest, basic_elem_type,
5359                            intcon(0), NULL,
5360                            alloc-&gt;in(AllocateNode::AllocSize));
5361     }
5362 
5363     generate_slow_arraycopy(adr_type,
5364                             src, src_offset, dest, dest_offset,
5365                             copy_length, /*dest_uninitialized*/false);
5366 
5367     result_region-&gt;init_req(slow_call_path, control());
5368     result_i_o   -&gt;init_req(slow_call_path, i_o());
5369     result_memory-&gt;init_req(slow_call_path, memory(adr_type));
5370   }
5371 
5372   // Remove unused edges.
5373   for (uint i = 1; i &lt; result_region-&gt;req(); i++) {
5374     if (result_region-&gt;in(i) == NULL)
5375       result_region-&gt;init_req(i, top());
5376   }
5377 
5378   // Finished; return the combined state.
5379   set_control( _gvn.transform(result_region));
5380   set_i_o(     _gvn.transform(result_i_o)    );
5381   set_memory(  _gvn.transform(result_memory), adr_type );
5382 
5383   // The memory edges above are precise in order to model effects around
5384   // array copies accurately to allow value numbering of field loads around
5385   // arraycopy.  Such field loads, both before and after, are common in Java
5386   // collections and similar classes involving header/array data structures.
5387   //
5388   // But with low number of register or when some registers are used or killed
5389   // by arraycopy calls it causes registers spilling on stack. See 6544710.
5390   // The next memory barrier is added to avoid it. If the arraycopy can be
5391   // optimized away (which it can, sometimes) then we can manually remove
5392   // the membar also.
5393   //
5394   // Do not let reads from the cloned object float above the arraycopy.
5395   if (alloc != NULL) {
5396     // Do not let stores that initialize this object be reordered with
5397     // a subsequent store that would make this object accessible by
5398     // other threads.
5399     // Record what AllocateNode this StoreStore protects so that
5400     // escape analysis can go from the MemBarStoreStoreNode to the
5401     // AllocateNode and eliminate the MemBarStoreStoreNode if possible
5402     // based on the escape status of the AllocateNode.
5403     insert_mem_bar(Op_MemBarStoreStore, alloc-&gt;proj_out(AllocateNode::RawAddress));
5404   } else if (InsertMemBarAfterArraycopy)
5405     insert_mem_bar(Op_MemBarCPUOrder);
5406 }
5407 
5408 
5409 // Helper function which determines if an arraycopy immediately follows
5410 // an allocation, with no intervening tests or other escapes for the object.
5411 AllocateArrayNode*
5412 LibraryCallKit::tightly_coupled_allocation(Node* ptr,
5413                                            RegionNode* slow_region) {
5414   if (stopped())             return NULL;  // no fast path
5415   if (C-&gt;AliasLevel() == 0)  return NULL;  // no MergeMems around
5416 
5417   AllocateArrayNode* alloc = AllocateArrayNode::Ideal_array_allocation(ptr, &amp;_gvn);
5418   if (alloc == NULL)  return NULL;
5419 
5420   Node* rawmem = memory(Compile::AliasIdxRaw);
5421   // Is the allocation's memory state untouched?
5422   if (!(rawmem-&gt;is_Proj() &amp;&amp; rawmem-&gt;in(0)-&gt;is_Initialize())) {
5423     // Bail out if there have been raw-memory effects since the allocation.
5424     // (Example:  There might have been a call or safepoint.)
5425     return NULL;
5426   }
5427   rawmem = rawmem-&gt;in(0)-&gt;as_Initialize()-&gt;memory(Compile::AliasIdxRaw);
5428   if (!(rawmem-&gt;is_Proj() &amp;&amp; rawmem-&gt;in(0) == alloc)) {
5429     return NULL;
5430   }
5431 
5432   // There must be no unexpected observers of this allocation.
5433   for (DUIterator_Fast imax, i = ptr-&gt;fast_outs(imax); i &lt; imax; i++) {
5434     Node* obs = ptr-&gt;fast_out(i);
5435     if (obs != this-&gt;map()) {
5436       return NULL;
5437     }
5438   }
5439 
5440   // This arraycopy must unconditionally follow the allocation of the ptr.
5441   Node* alloc_ctl = ptr-&gt;in(0);
5442   assert(just_allocated_object(alloc_ctl) == ptr, "most recent allo");
5443 
5444   Node* ctl = control();
5445   while (ctl != alloc_ctl) {
5446     // There may be guards which feed into the slow_region.
5447     // Any other control flow means that we might not get a chance
5448     // to finish initializing the allocated object.
5449     if ((ctl-&gt;is_IfFalse() || ctl-&gt;is_IfTrue()) &amp;&amp; ctl-&gt;in(0)-&gt;is_If()) {
5450       IfNode* iff = ctl-&gt;in(0)-&gt;as_If();
5451       Node* not_ctl = iff-&gt;proj_out(1 - ctl-&gt;as_Proj()-&gt;_con);
5452       assert(not_ctl != NULL &amp;&amp; not_ctl != ctl, "found alternate");
5453       if (slow_region != NULL &amp;&amp; slow_region-&gt;find_edge(not_ctl) &gt;= 1) {
5454         ctl = iff-&gt;in(0);       // This test feeds the known slow_region.
5455         continue;
5456       }
5457       // One more try:  Various low-level checks bottom out in
5458       // uncommon traps.  If the debug-info of the trap omits
5459       // any reference to the allocation, as we've already
5460       // observed, then there can be no objection to the trap.
5461       bool found_trap = false;
5462       for (DUIterator_Fast jmax, j = not_ctl-&gt;fast_outs(jmax); j &lt; jmax; j++) {
5463         Node* obs = not_ctl-&gt;fast_out(j);
5464         if (obs-&gt;in(0) == not_ctl &amp;&amp; obs-&gt;is_Call() &amp;&amp;
5465             (obs-&gt;as_Call()-&gt;entry_point() == SharedRuntime::uncommon_trap_blob()-&gt;entry_point())) {
5466           found_trap = true; break;
5467         }
5468       }
5469       if (found_trap) {
5470         ctl = iff-&gt;in(0);       // This test feeds a harmless uncommon trap.
5471         continue;
5472       }
5473     }
5474     return NULL;
5475   }
5476 
5477   // If we get this far, we have an allocation which immediately
5478   // precedes the arraycopy, and we can take over zeroing the new object.
5479   // The arraycopy will finish the initialization, and provide
5480   // a new control state to which we will anchor the destination pointer.
5481 
5482   return alloc;
5483 }
5484 
5485 // Helper for initialization of arrays, creating a ClearArray.
5486 // It writes zero bits in [start..end), within the body of an array object.
5487 // The memory effects are all chained onto the 'adr_type' alias category.
5488 //
5489 // Since the object is otherwise uninitialized, we are free
5490 // to put a little "slop" around the edges of the cleared area,
5491 // as long as it does not go back into the array's header,
5492 // or beyond the array end within the heap.
5493 //
5494 // The lower edge can be rounded down to the nearest jint and the
5495 // upper edge can be rounded up to the nearest MinObjAlignmentInBytes.
5496 //
5497 // Arguments:
5498 //   adr_type           memory slice where writes are generated
5499 //   dest               oop of the destination array
5500 //   basic_elem_type    element type of the destination
5501 //   slice_idx          array index of first element to store
5502 //   slice_len          number of elements to store (or NULL)
5503 //   dest_size          total size in bytes of the array object
5504 //
5505 // Exactly one of slice_len or dest_size must be non-NULL.
5506 // If dest_size is non-NULL, zeroing extends to the end of the object.
5507 // If slice_len is non-NULL, the slice_idx value must be a constant.
5508 void
5509 LibraryCallKit::generate_clear_array(const TypePtr* adr_type,
5510                                      Node* dest,
5511                                      BasicType basic_elem_type,
5512                                      Node* slice_idx,
5513                                      Node* slice_len,
5514                                      Node* dest_size) {
5515   // one or the other but not both of slice_len and dest_size:
5516   assert((slice_len != NULL? 1: 0) + (dest_size != NULL? 1: 0) == 1, "");
5517   if (slice_len == NULL)  slice_len = top();
5518   if (dest_size == NULL)  dest_size = top();
5519 
5520   // operate on this memory slice:
5521   Node* mem = memory(adr_type); // memory slice to operate on
5522 
5523   // scaling and rounding of indexes:
5524   int scale = exact_log2(type2aelembytes(basic_elem_type));
5525   int abase = arrayOopDesc::base_offset_in_bytes(basic_elem_type);
5526   int clear_low = (-1 &lt;&lt; scale) &amp; (BytesPerInt  - 1);
5527   int bump_bit  = (-1 &lt;&lt; scale) &amp; BytesPerInt;
5528 
5529   // determine constant starts and ends
5530   const intptr_t BIG_NEG = -128;
5531   assert(BIG_NEG + 2*abase &lt; 0, "neg enough");
5532   intptr_t slice_idx_con = (intptr_t) find_int_con(slice_idx, BIG_NEG);
5533   intptr_t slice_len_con = (intptr_t) find_int_con(slice_len, BIG_NEG);
5534   if (slice_len_con == 0) {
5535     return;                     // nothing to do here
5536   }
5537   intptr_t start_con = (abase + (slice_idx_con &lt;&lt; scale)) &amp; ~clear_low;
5538   intptr_t end_con   = find_intptr_t_con(dest_size, -1);
5539   if (slice_idx_con &gt;= 0 &amp;&amp; slice_len_con &gt;= 0) {
5540     assert(end_con &lt; 0, "not two cons");
5541     end_con = round_to(abase + ((slice_idx_con + slice_len_con) &lt;&lt; scale),
5542                        BytesPerLong);
5543   }
5544 
5545   if (start_con &gt;= 0 &amp;&amp; end_con &gt;= 0) {
5546     // Constant start and end.  Simple.
5547     mem = ClearArrayNode::clear_memory(control(), mem, dest,
5548                                        start_con, end_con, &amp;_gvn);
5549   } else if (start_con &gt;= 0 &amp;&amp; dest_size != top()) {
5550     // Constant start, pre-rounded end after the tail of the array.
5551     Node* end = dest_size;
5552     mem = ClearArrayNode::clear_memory(control(), mem, dest,
5553                                        start_con, end, &amp;_gvn);
5554   } else if (start_con &gt;= 0 &amp;&amp; slice_len != top()) {
5555     // Constant start, non-constant end.  End needs rounding up.
5556     // End offset = round_up(abase + ((slice_idx_con + slice_len) &lt;&lt; scale), 8)
5557     intptr_t end_base  = abase + (slice_idx_con &lt;&lt; scale);
5558     int      end_round = (-1 &lt;&lt; scale) &amp; (BytesPerLong  - 1);
5559     Node*    end       = ConvI2X(slice_len);
5560     if (scale != 0)
5561       end = _gvn.transform(new(C) LShiftXNode(end, intcon(scale) ));
5562     end_base += end_round;
5563     end = _gvn.transform(new(C) AddXNode(end, MakeConX(end_base)));
5564     end = _gvn.transform(new(C) AndXNode(end, MakeConX(~end_round)));
5565     mem = ClearArrayNode::clear_memory(control(), mem, dest,
5566                                        start_con, end, &amp;_gvn);
5567   } else if (start_con &lt; 0 &amp;&amp; dest_size != top()) {
5568     // Non-constant start, pre-rounded end after the tail of the array.
5569     // This is almost certainly a "round-to-end" operation.
5570     Node* start = slice_idx;
5571     start = ConvI2X(start);
5572     if (scale != 0)
5573       start = _gvn.transform(new(C) LShiftXNode( start, intcon(scale) ));
5574     start = _gvn.transform(new(C) AddXNode(start, MakeConX(abase)));
5575     if ((bump_bit | clear_low) != 0) {
5576       int to_clear = (bump_bit | clear_low);
5577       // Align up mod 8, then store a jint zero unconditionally
5578       // just before the mod-8 boundary.
5579       if (((abase + bump_bit) &amp; ~to_clear) - bump_bit
5580           &lt; arrayOopDesc::length_offset_in_bytes() + BytesPerInt) {
5581         bump_bit = 0;
5582         assert((abase &amp; to_clear) == 0, "array base must be long-aligned");
5583       } else {
5584         // Bump 'start' up to (or past) the next jint boundary:
5585         start = _gvn.transform(new(C) AddXNode(start, MakeConX(bump_bit)));
5586         assert((abase &amp; clear_low) == 0, "array base must be int-aligned");
5587       }
5588       // Round bumped 'start' down to jlong boundary in body of array.
5589       start = _gvn.transform(new(C) AndXNode(start, MakeConX(~to_clear)));
5590       if (bump_bit != 0) {
5591         // Store a zero to the immediately preceding jint:
5592         Node* x1 = _gvn.transform(new(C) AddXNode(start, MakeConX(-bump_bit)));
5593         Node* p1 = basic_plus_adr(dest, x1);
5594         mem = StoreNode::make(_gvn, control(), mem, p1, adr_type, intcon(0), T_INT, MemNode::unordered);
5595         mem = _gvn.transform(mem);
5596       }
5597     }
5598     Node* end = dest_size; // pre-rounded
5599     mem = ClearArrayNode::clear_memory(control(), mem, dest,
5600                                        start, end, &amp;_gvn);
5601   } else {
5602     // Non-constant start, unrounded non-constant end.
5603     // (Nobody zeroes a random midsection of an array using this routine.)
5604     ShouldNotReachHere();       // fix caller
5605   }
5606 
5607   // Done.
5608   set_memory(mem, adr_type);
5609 }
5610 
5611 
5612 bool
5613 LibraryCallKit::generate_block_arraycopy(const TypePtr* adr_type,
5614                                          BasicType basic_elem_type,
5615                                          AllocateNode* alloc,
5616                                          Node* src,  Node* src_offset,
5617                                          Node* dest, Node* dest_offset,
5618                                          Node* dest_size, bool dest_uninitialized) {
5619   // See if there is an advantage from block transfer.
5620   int scale = exact_log2(type2aelembytes(basic_elem_type));
5621   if (scale &gt;= LogBytesPerLong)
5622     return false;               // it is already a block transfer
5623 
5624   // Look at the alignment of the starting offsets.
5625   int abase = arrayOopDesc::base_offset_in_bytes(basic_elem_type);
5626 
5627   intptr_t src_off_con  = (intptr_t) find_int_con(src_offset, -1);
5628   intptr_t dest_off_con = (intptr_t) find_int_con(dest_offset, -1);
5629   if (src_off_con &lt; 0 || dest_off_con &lt; 0)
5630     // At present, we can only understand constants.
5631     return false;
5632 
5633   intptr_t src_off  = abase + (src_off_con  &lt;&lt; scale);
5634   intptr_t dest_off = abase + (dest_off_con &lt;&lt; scale);
5635 
5636   if (((src_off | dest_off) &amp; (BytesPerLong-1)) != 0) {
5637     // Non-aligned; too bad.
5638     // One more chance:  Pick off an initial 32-bit word.
5639     // This is a common case, since abase can be odd mod 8.
5640     if (((src_off | dest_off) &amp; (BytesPerLong-1)) == BytesPerInt &amp;&amp;
5641         ((src_off ^ dest_off) &amp; (BytesPerLong-1)) == 0) {
5642       Node* sptr = basic_plus_adr(src,  src_off);
5643       Node* dptr = basic_plus_adr(dest, dest_off);
5644       Node* sval = make_load(control(), sptr, TypeInt::INT, T_INT, adr_type, MemNode::unordered);
5645       store_to_memory(control(), dptr, sval, T_INT, adr_type, MemNode::unordered);
5646       src_off += BytesPerInt;
5647       dest_off += BytesPerInt;
5648     } else {
5649       return false;
5650     }
5651   }
5652   assert(src_off % BytesPerLong == 0, "");
5653   assert(dest_off % BytesPerLong == 0, "");
5654 
5655   // Do this copy by giant steps.
5656   Node* sptr  = basic_plus_adr(src,  src_off);
5657   Node* dptr  = basic_plus_adr(dest, dest_off);
5658   Node* countx = dest_size;
5659   countx = _gvn.transform(new (C) SubXNode(countx, MakeConX(dest_off)));
5660   countx = _gvn.transform(new (C) URShiftXNode(countx, intcon(LogBytesPerLong)));
5661 
5662   bool disjoint_bases = true;   // since alloc != NULL
5663   generate_unchecked_arraycopy(adr_type, T_LONG, disjoint_bases,
5664                                sptr, NULL, dptr, NULL, countx, dest_uninitialized);
5665 
5666   return true;
5667 }
5668 
5669 
5670 // Helper function; generates code for the slow case.
5671 // We make a call to a runtime method which emulates the native method,
5672 // but without the native wrapper overhead.
5673 void
5674 LibraryCallKit::generate_slow_arraycopy(const TypePtr* adr_type,
5675                                         Node* src,  Node* src_offset,
5676                                         Node* dest, Node* dest_offset,
5677                                         Node* copy_length, bool dest_uninitialized) {
5678   assert(!dest_uninitialized, "Invariant");
5679   Node* call = make_runtime_call(RC_NO_LEAF | RC_UNCOMMON,
5680                                  OptoRuntime::slow_arraycopy_Type(),
5681                                  OptoRuntime::slow_arraycopy_Java(),
5682                                  "slow_arraycopy", adr_type,
5683                                  src, src_offset, dest, dest_offset,
5684                                  copy_length);
5685 
5686   // Handle exceptions thrown by this fellow:
5687   make_slow_call_ex(call, env()-&gt;Throwable_klass(), false);
5688 }
5689 
5690 // Helper function; generates code for cases requiring runtime checks.
5691 Node*
5692 LibraryCallKit::generate_checkcast_arraycopy(const TypePtr* adr_type,
5693                                              Node* dest_elem_klass,
5694                                              Node* src,  Node* src_offset,
5695                                              Node* dest, Node* dest_offset,
5696                                              Node* copy_length, bool dest_uninitialized) {
5697   if (stopped())  return NULL;
5698 
5699   address copyfunc_addr = StubRoutines::checkcast_arraycopy(dest_uninitialized);
5700   if (copyfunc_addr == NULL) { // Stub was not generated, go slow path.
5701     return NULL;
5702   }
5703 
5704   // Pick out the parameters required to perform a store-check
5705   // for the target array.  This is an optimistic check.  It will
5706   // look in each non-null element's class, at the desired klass's
5707   // super_check_offset, for the desired klass.
5708   int sco_offset = in_bytes(Klass::super_check_offset_offset());
5709   Node* p3 = basic_plus_adr(dest_elem_klass, sco_offset);
5710   Node* n3 = new(C) LoadINode(NULL, memory(p3), p3, _gvn.type(p3)-&gt;is_ptr(), TypeInt::INT, MemNode::unordered);
5711   Node* check_offset = ConvI2X(_gvn.transform(n3));
5712   Node* check_value  = dest_elem_klass;
5713 
5714   Node* src_start  = array_element_address(src,  src_offset,  T_OBJECT);
5715   Node* dest_start = array_element_address(dest, dest_offset, T_OBJECT);
5716 
5717   // (We know the arrays are never conjoint, because their types differ.)
5718   Node* call = make_runtime_call(RC_LEAF|RC_NO_FP,
5719                                  OptoRuntime::checkcast_arraycopy_Type(),
5720                                  copyfunc_addr, "checkcast_arraycopy", adr_type,
5721                                  // five arguments, of which two are
5722                                  // intptr_t (jlong in LP64)
5723                                  src_start, dest_start,
5724                                  copy_length XTOP,
5725                                  check_offset XTOP,
5726                                  check_value);
5727 
5728   return _gvn.transform(new (C) ProjNode(call, TypeFunc::Parms));
5729 }
5730 
5731 
5732 // Helper function; generates code for cases requiring runtime checks.
5733 Node*
5734 LibraryCallKit::generate_generic_arraycopy(const TypePtr* adr_type,
5735                                            Node* src,  Node* src_offset,
5736                                            Node* dest, Node* dest_offset,
5737                                            Node* copy_length, bool dest_uninitialized) {
5738   assert(!dest_uninitialized, "Invariant");
5739   if (stopped())  return NULL;
5740   address copyfunc_addr = StubRoutines::generic_arraycopy();
5741   if (copyfunc_addr == NULL) { // Stub was not generated, go slow path.
5742     return NULL;
5743   }
5744 
5745   Node* call = make_runtime_call(RC_LEAF|RC_NO_FP,
5746                     OptoRuntime::generic_arraycopy_Type(),
5747                     copyfunc_addr, "generic_arraycopy", adr_type,
5748                     src, src_offset, dest, dest_offset, copy_length);
5749 
5750   return _gvn.transform(new (C) ProjNode(call, TypeFunc::Parms));
5751 }
5752 
5753 // Helper function; generates the fast out-of-line call to an arraycopy stub.
5754 void
5755 LibraryCallKit::generate_unchecked_arraycopy(const TypePtr* adr_type,
5756                                              BasicType basic_elem_type,
5757                                              bool disjoint_bases,
5758                                              Node* src,  Node* src_offset,
5759                                              Node* dest, Node* dest_offset,
5760                                              Node* copy_length, bool dest_uninitialized) {
5761   if (stopped())  return;               // nothing to do
5762 
5763   Node* src_start  = src;
5764   Node* dest_start = dest;
5765   if (src_offset != NULL || dest_offset != NULL) {
5766     assert(src_offset != NULL &amp;&amp; dest_offset != NULL, "");
5767     src_start  = array_element_address(src,  src_offset,  basic_elem_type);
5768     dest_start = array_element_address(dest, dest_offset, basic_elem_type);
5769   }
5770 
5771   // Figure out which arraycopy runtime method to call.
5772   const char* copyfunc_name = "arraycopy";
5773   address     copyfunc_addr =
5774       basictype2arraycopy(basic_elem_type, src_offset, dest_offset,
5775                           disjoint_bases, copyfunc_name, dest_uninitialized);
5776 
5777   // Call it.  Note that the count_ix value is not scaled to a byte-size.
5778   make_runtime_call(RC_LEAF|RC_NO_FP,
5779                     OptoRuntime::fast_arraycopy_Type(),
5780                     copyfunc_addr, copyfunc_name, adr_type,
5781                     src_start, dest_start, copy_length XTOP);
5782 }
5783 
5784 //-------------inline_encodeISOArray-----------------------------------
5785 // encode char[] to byte[] in ISO_8859_1
5786 bool LibraryCallKit::inline_encodeISOArray() {
5787   assert(callee()-&gt;signature()-&gt;size() == 5, "encodeISOArray has 5 parameters");
5788   // no receiver since it is static method
5789   Node *src         = argument(0);
5790   Node *src_offset  = argument(1);
5791   Node *dst         = argument(2);
5792   Node *dst_offset  = argument(3);
5793   Node *length      = argument(4);
5794 
5795   const Type* src_type = src-&gt;Value(&amp;_gvn);
5796   const Type* dst_type = dst-&gt;Value(&amp;_gvn);
5797   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
5798   const TypeAryPtr* top_dest = dst_type-&gt;isa_aryptr();
5799   if (top_src  == NULL || top_src-&gt;klass()  == NULL ||
5800       top_dest == NULL || top_dest-&gt;klass() == NULL) {
5801     // failed array check
5802     return false;
5803   }
5804 
5805   // Figure out the size and type of the elements we will be copying.
5806   BasicType src_elem = src_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5807   BasicType dst_elem = dst_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5808   if (src_elem != T_CHAR || dst_elem != T_BYTE) {
5809     return false;
5810   }
5811   Node* src_start = array_element_address(src, src_offset, src_elem);
5812   Node* dst_start = array_element_address(dst, dst_offset, dst_elem);
5813   // 'src_start' points to src array + scaled offset
5814   // 'dst_start' points to dst array + scaled offset
5815 
5816   const TypeAryPtr* mtype = TypeAryPtr::BYTES;
5817   Node* enc = new (C) EncodeISOArrayNode(control(), memory(mtype), src_start, dst_start, length);
5818   enc = _gvn.transform(enc);
5819   Node* res_mem = _gvn.transform(new (C) SCMemProjNode(enc));
5820   set_memory(res_mem, mtype);
5821   set_result(enc);
5822   return true;
5823 }
5824 
5825 //-------------inline_multiplyToLen-----------------------------------
5826 bool LibraryCallKit::inline_multiplyToLen() {
5827   assert(UseMultiplyToLenIntrinsic, "not implementated on this platform");
5828 
5829   address stubAddr = StubRoutines::multiplyToLen();
5830   if (stubAddr == NULL) {
5831     return false; // Intrinsic's stub is not implemented on this platform
5832   }
5833   const char* stubName = "multiplyToLen";
5834 
5835   assert(callee()-&gt;signature()-&gt;size() == 5, "multiplyToLen has 5 parameters");
5836 
5837   // no receiver because it is a static method
5838   Node* x    = argument(0);
5839   Node* xlen = argument(1);
5840   Node* y    = argument(2);
5841   Node* ylen = argument(3);
5842   Node* z    = argument(4);
5843 
5844   const Type* x_type = x-&gt;Value(&amp;_gvn);
5845   const Type* y_type = y-&gt;Value(&amp;_gvn);
5846   const TypeAryPtr* top_x = x_type-&gt;isa_aryptr();
5847   const TypeAryPtr* top_y = y_type-&gt;isa_aryptr();
5848   if (top_x  == NULL || top_x-&gt;klass()  == NULL ||
5849       top_y == NULL || top_y-&gt;klass() == NULL) {
5850     // failed array check
5851     return false;
5852   }
5853 
5854   BasicType x_elem = x_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5855   BasicType y_elem = y_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5856   if (x_elem != T_INT || y_elem != T_INT) {
5857     return false;
5858   }
5859 
5860   // Set the original stack and the reexecute bit for the interpreter to reexecute
5861   // the bytecode that invokes BigInteger.multiplyToLen() if deoptimization happens
5862   // on the return from z array allocation in runtime.
5863   { PreserveReexecuteState preexecs(this);
5864     jvms()-&gt;set_should_reexecute(true);
5865 
5866     Node* x_start = array_element_address(x, intcon(0), x_elem);
5867     Node* y_start = array_element_address(y, intcon(0), y_elem);
5868     // 'x_start' points to x array + scaled xlen
5869     // 'y_start' points to y array + scaled ylen
5870 
5871     // Allocate the result array
5872     Node* zlen = _gvn.transform(new(C) AddINode(xlen, ylen));
5873     ciKlass* klass = ciTypeArrayKlass::make(T_INT);
5874     Node* klass_node = makecon(TypeKlassPtr::make(klass));
5875 
5876     IdealKit ideal(this);
5877 
5878 #define __ ideal.
5879      Node* one = __ ConI(1);
5880      Node* zero = __ ConI(0);
5881      IdealVariable need_alloc(ideal), z_alloc(ideal);  __ declarations_done();
5882      __ set(need_alloc, zero);
5883      __ set(z_alloc, z);
5884      __ if_then(z, BoolTest::eq, null()); {
5885        __ increment (need_alloc, one);
5886      } __ else_(); {
5887        // Update graphKit memory and control from IdealKit.
5888        sync_kit(ideal);
5889        Node* zlen_arg = load_array_length(z);
5890        // Update IdealKit memory and control from graphKit.
5891        __ sync_kit(this);
5892        __ if_then(zlen_arg, BoolTest::lt, zlen); {
5893          __ increment (need_alloc, one);
5894        } __ end_if();
5895      } __ end_if();
5896 
5897      __ if_then(__ value(need_alloc), BoolTest::ne, zero); {
5898        // Update graphKit memory and control from IdealKit.
5899        sync_kit(ideal);
5900        Node * narr = new_array(klass_node, zlen, 1);
5901        // Update IdealKit memory and control from graphKit.
5902        __ sync_kit(this);
5903        __ set(z_alloc, narr);
5904      } __ end_if();
5905 
5906      sync_kit(ideal);
5907      z = __ value(z_alloc);
5908      // Can't use TypeAryPtr::INTS which uses Bottom offset.
5909      _gvn.set_type(z, TypeOopPtr::make_from_klass(klass));
5910      // Final sync IdealKit and GraphKit.
5911      final_sync(ideal);
5912 #undef __
5913 
5914     Node* z_start = array_element_address(z, intcon(0), T_INT);
5915 
5916     Node* call = make_runtime_call(RC_LEAF|RC_NO_FP,
5917                                    OptoRuntime::multiplyToLen_Type(),
5918                                    stubAddr, stubName, TypePtr::BOTTOM,
5919                                    x_start, xlen, y_start, ylen, z_start, zlen);
5920   } // original reexecute is set back here
5921 
5922   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
5923   set_result(z);
5924   return true;
5925 }
5926 
5927 //-------------inline_squareToLen------------------------------------
5928 bool LibraryCallKit::inline_squareToLen() {
5929   assert(UseSquareToLenIntrinsic, "not implementated on this platform");
5930 
5931   address stubAddr = StubRoutines::squareToLen();
5932   if (stubAddr == NULL) {
5933     return false; // Intrinsic's stub is not implemented on this platform
5934   }
5935   const char* stubName = "squareToLen";
5936 
5937   assert(callee()-&gt;signature()-&gt;size() == 4, "implSquareToLen has 4 parameters");
5938 
5939   Node* x    = argument(0);
5940   Node* len  = argument(1);
5941   Node* z    = argument(2);
5942   Node* zlen = argument(3);
5943 
5944   const Type* x_type = x-&gt;Value(&amp;_gvn);
5945   const Type* z_type = z-&gt;Value(&amp;_gvn);
5946   const TypeAryPtr* top_x = x_type-&gt;isa_aryptr();
5947   const TypeAryPtr* top_z = z_type-&gt;isa_aryptr();
5948   if (top_x  == NULL || top_x-&gt;klass()  == NULL ||
5949       top_z  == NULL || top_z-&gt;klass()  == NULL) {
5950     // failed array check
5951     return false;
5952   }
5953 
5954   BasicType x_elem = x_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5955   BasicType z_elem = z_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5956   if (x_elem != T_INT || z_elem != T_INT) {
5957     return false;
5958   }
5959 
5960 
5961   Node* x_start = array_element_address(x, intcon(0), x_elem);
5962   Node* z_start = array_element_address(z, intcon(0), z_elem);
5963 
5964   Node*  call = make_runtime_call(RC_LEAF|RC_NO_FP,
5965                                   OptoRuntime::squareToLen_Type(),
5966                                   stubAddr, stubName, TypePtr::BOTTOM,
5967                                   x_start, len, z_start, zlen);
5968 
5969   set_result(z);
5970   return true;
5971 }
5972 
5973 //-------------inline_mulAdd------------------------------------------
5974 bool LibraryCallKit::inline_mulAdd() {
5975   assert(UseMulAddIntrinsic, "not implementated on this platform");
5976 
5977   address stubAddr = StubRoutines::mulAdd();
5978   if (stubAddr == NULL) {
5979     return false; // Intrinsic's stub is not implemented on this platform
5980   }
5981   const char* stubName = "mulAdd";
5982 
5983   assert(callee()-&gt;signature()-&gt;size() == 5, "mulAdd has 5 parameters");
5984 
5985   Node* out      = argument(0);
5986   Node* in       = argument(1);
5987   Node* offset   = argument(2);
5988   Node* len      = argument(3);
5989   Node* k        = argument(4);
5990 
5991   const Type* out_type = out-&gt;Value(&amp;_gvn);
5992   const Type* in_type = in-&gt;Value(&amp;_gvn);
5993   const TypeAryPtr* top_out = out_type-&gt;isa_aryptr();
5994   const TypeAryPtr* top_in = in_type-&gt;isa_aryptr();
5995   if (top_out  == NULL || top_out-&gt;klass()  == NULL ||
5996       top_in == NULL || top_in-&gt;klass() == NULL) {
5997     // failed array check
5998     return false;
5999   }
6000 
6001   BasicType out_elem = out_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
6002   BasicType in_elem = in_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
6003   if (out_elem != T_INT || in_elem != T_INT) {
6004     return false;
6005   }
6006 
6007   Node* outlen = load_array_length(out);
6008   Node* new_offset = _gvn.transform(new (C) SubINode(outlen, offset));
6009   Node* out_start = array_element_address(out, intcon(0), out_elem);
6010   Node* in_start = array_element_address(in, intcon(0), in_elem);
6011 
6012   Node*  call = make_runtime_call(RC_LEAF|RC_NO_FP,
6013                                   OptoRuntime::mulAdd_Type(),
6014                                   stubAddr, stubName, TypePtr::BOTTOM,
6015                                   out_start,in_start, new_offset, len, k);
6016   Node* result = _gvn.transform(new (C) ProjNode(call, TypeFunc::Parms));
6017   set_result(result);
6018   return true;
6019 }
6020 
6021 //-------------inline_montgomeryMultiply-----------------------------------
6022 bool LibraryCallKit::inline_montgomeryMultiply() {
6023   address stubAddr = StubRoutines::montgomeryMultiply();
6024   if (stubAddr == NULL) {
6025     return false; // Intrinsic's stub is not implemented on this platform
6026   }
6027 
6028   assert(UseMontgomeryMultiplyIntrinsic, "not implemented on this platform");
6029   const char* stubName = "montgomery_square";
6030 
6031   assert(callee()-&gt;signature()-&gt;size() == 7, "montgomeryMultiply has 7 parameters");
6032 
6033   Node* a    = argument(0);
6034   Node* b    = argument(1);
6035   Node* n    = argument(2);
6036   Node* len  = argument(3);
6037   Node* inv  = argument(4);
6038   Node* m    = argument(6);
6039 
6040   const Type* a_type = a-&gt;Value(&amp;_gvn);
6041   const TypeAryPtr* top_a = a_type-&gt;isa_aryptr();
6042   const Type* b_type = b-&gt;Value(&amp;_gvn);
6043   const TypeAryPtr* top_b = b_type-&gt;isa_aryptr();
6044   const Type* n_type = a-&gt;Value(&amp;_gvn);
6045   const TypeAryPtr* top_n = n_type-&gt;isa_aryptr();
6046   const Type* m_type = a-&gt;Value(&amp;_gvn);
6047   const TypeAryPtr* top_m = m_type-&gt;isa_aryptr();
6048   if (top_a  == NULL || top_a-&gt;klass()  == NULL ||
6049       top_b == NULL || top_b-&gt;klass()  == NULL ||
6050       top_n == NULL || top_n-&gt;klass()  == NULL ||
6051       top_m == NULL || top_m-&gt;klass()  == NULL) {
6052     // failed array check
6053     return false;
6054   }
6055 
6056   BasicType a_elem = a_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
6057   BasicType b_elem = b_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
6058   BasicType n_elem = n_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
6059   BasicType m_elem = m_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
6060   if (a_elem != T_INT || b_elem != T_INT || n_elem != T_INT || m_elem != T_INT) {
6061     return false;
6062   }
6063 
6064   // Make the call
6065   {
6066     Node* a_start = array_element_address(a, intcon(0), a_elem);
6067     Node* b_start = array_element_address(b, intcon(0), b_elem);
6068     Node* n_start = array_element_address(n, intcon(0), n_elem);
6069     Node* m_start = array_element_address(m, intcon(0), m_elem);
6070 
<a name="1" id="anc1"></a><span class="changed">6071     Node* call = make_runtime_call(RC_LEAF,</span>









6072                                    OptoRuntime::montgomeryMultiply_Type(),
6073                                    stubAddr, stubName, TypePtr::BOTTOM,
6074                                    a_start, b_start, n_start, len, inv, top(),
6075                                    m_start);
<a name="2" id="anc2"></a>
6076     set_result(m);
6077   }
6078 
6079   return true;
6080 }
6081 
6082 bool LibraryCallKit::inline_montgomerySquare() {
6083   address stubAddr = StubRoutines::montgomerySquare();
6084   if (stubAddr == NULL) {
6085     return false; // Intrinsic's stub is not implemented on this platform
6086   }
6087 
6088   assert(UseMontgomerySquareIntrinsic, "not implemented on this platform");
6089   const char* stubName = "montgomery_square";
6090 
6091   assert(callee()-&gt;signature()-&gt;size() == 6, "montgomerySquare has 6 parameters");
6092 
6093   Node* a    = argument(0);
6094   Node* n    = argument(1);
6095   Node* len  = argument(2);
6096   Node* inv  = argument(3);
6097   Node* m    = argument(5);
6098 
6099   const Type* a_type = a-&gt;Value(&amp;_gvn);
6100   const TypeAryPtr* top_a = a_type-&gt;isa_aryptr();
6101   const Type* n_type = a-&gt;Value(&amp;_gvn);
6102   const TypeAryPtr* top_n = n_type-&gt;isa_aryptr();
6103   const Type* m_type = a-&gt;Value(&amp;_gvn);
6104   const TypeAryPtr* top_m = m_type-&gt;isa_aryptr();
6105   if (top_a  == NULL || top_a-&gt;klass()  == NULL ||
6106       top_n == NULL || top_n-&gt;klass()  == NULL ||
6107       top_m == NULL || top_m-&gt;klass()  == NULL) {
6108     // failed array check
6109     return false;
6110   }
6111 
6112   BasicType a_elem = a_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
6113   BasicType n_elem = n_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
6114   BasicType m_elem = m_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
6115   if (a_elem != T_INT || n_elem != T_INT || m_elem != T_INT) {
6116     return false;
6117   }
6118 
6119   // Make the call
6120   {
6121     Node* a_start = array_element_address(a, intcon(0), a_elem);
6122     Node* n_start = array_element_address(n, intcon(0), n_elem);
6123     Node* m_start = array_element_address(m, intcon(0), m_elem);
6124 
<a name="3" id="anc3"></a><span class="changed">6125     Node* call = make_runtime_call(RC_LEAF,</span>









6126                                    OptoRuntime::montgomerySquare_Type(),
6127                                    stubAddr, stubName, TypePtr::BOTTOM,
6128                                    a_start, n_start, len, inv, top(),
6129                                    m_start);
<a name="4" id="anc4"></a>

6130     set_result(m);
6131   }
6132 
6133   return true;
6134 }
6135 
6136 
6137 /**
6138  * Calculate CRC32 for byte.
6139  * int java.util.zip.CRC32.update(int crc, int b)
6140  */
6141 bool LibraryCallKit::inline_updateCRC32() {
6142   assert(UseCRC32Intrinsics, "need AVX and LCMUL instructions support");
6143   assert(callee()-&gt;signature()-&gt;size() == 2, "update has 2 parameters");
6144   // no receiver since it is static method
6145   Node* crc  = argument(0); // type: int
6146   Node* b    = argument(1); // type: int
6147 
6148   /*
6149    *    int c = ~ crc;
6150    *    b = timesXtoThe32[(b ^ c) &amp; 0xFF];
6151    *    b = b ^ (c &gt;&gt;&gt; 8);
6152    *    crc = ~b;
6153    */
6154 
6155   Node* M1 = intcon(-1);
6156   crc = _gvn.transform(new (C) XorINode(crc, M1));
6157   Node* result = _gvn.transform(new (C) XorINode(crc, b));
6158   result = _gvn.transform(new (C) AndINode(result, intcon(0xFF)));
6159 
6160   Node* base = makecon(TypeRawPtr::make(StubRoutines::crc_table_addr()));
6161   Node* offset = _gvn.transform(new (C) LShiftINode(result, intcon(0x2)));
6162   Node* adr = basic_plus_adr(top(), base, ConvI2X(offset));
6163   result = make_load(control(), adr, TypeInt::INT, T_INT, MemNode::unordered);
6164 
6165   crc = _gvn.transform(new (C) URShiftINode(crc, intcon(8)));
6166   result = _gvn.transform(new (C) XorINode(crc, result));
6167   result = _gvn.transform(new (C) XorINode(result, M1));
6168   set_result(result);
6169   return true;
6170 }
6171 
6172 /**
6173  * Calculate CRC32 for byte[] array.
6174  * int java.util.zip.CRC32.updateBytes(int crc, byte[] buf, int off, int len)
6175  */
6176 bool LibraryCallKit::inline_updateBytesCRC32() {
6177   assert(UseCRC32Intrinsics, "need AVX and LCMUL instructions support");
6178   assert(callee()-&gt;signature()-&gt;size() == 4, "updateBytes has 4 parameters");
6179   // no receiver since it is static method
6180   Node* crc     = argument(0); // type: int
6181   Node* src     = argument(1); // type: oop
6182   Node* offset  = argument(2); // type: int
6183   Node* length  = argument(3); // type: int
6184 
6185   const Type* src_type = src-&gt;Value(&amp;_gvn);
6186   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
6187   if (top_src  == NULL || top_src-&gt;klass()  == NULL) {
6188     // failed array check
6189     return false;
6190   }
6191 
6192   // Figure out the size and type of the elements we will be copying.
6193   BasicType src_elem = src_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
6194   if (src_elem != T_BYTE) {
6195     return false;
6196   }
6197 
6198   // 'src_start' points to src array + scaled offset
6199   Node* src_start = array_element_address(src, offset, src_elem);
6200 
6201   // We assume that range check is done by caller.
6202   // TODO: generate range check (offset+length &lt; src.length) in debug VM.
6203 
6204   // Call the stub.
6205   address stubAddr = StubRoutines::updateBytesCRC32();
6206   const char *stubName = "updateBytesCRC32";
6207 
6208   Node* call = make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::updateBytesCRC32_Type(),
6209                                  stubAddr, stubName, TypePtr::BOTTOM,
6210                                  crc, src_start, length);
6211   Node* result = _gvn.transform(new (C) ProjNode(call, TypeFunc::Parms));
6212   set_result(result);
6213   return true;
6214 }
6215 
6216 /**
6217  * Calculate CRC32 for ByteBuffer.
6218  * int java.util.zip.CRC32.updateByteBuffer(int crc, long buf, int off, int len)
6219  */
6220 bool LibraryCallKit::inline_updateByteBufferCRC32() {
6221   assert(UseCRC32Intrinsics, "need AVX and LCMUL instructions support");
6222   assert(callee()-&gt;signature()-&gt;size() == 5, "updateByteBuffer has 4 parameters and one is long");
6223   // no receiver since it is static method
6224   Node* crc     = argument(0); // type: int
6225   Node* src     = argument(1); // type: long
6226   Node* offset  = argument(3); // type: int
6227   Node* length  = argument(4); // type: int
6228 
6229   src = ConvL2X(src);  // adjust Java long to machine word
6230   Node* base = _gvn.transform(new (C) CastX2PNode(src));
6231   offset = ConvI2X(offset);
6232 
6233   // 'src_start' points to src array + scaled offset
6234   Node* src_start = basic_plus_adr(top(), base, offset);
6235 
6236   // Call the stub.
6237   address stubAddr = StubRoutines::updateBytesCRC32();
6238   const char *stubName = "updateBytesCRC32";
6239 
6240   Node* call = make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::updateBytesCRC32_Type(),
6241                                  stubAddr, stubName, TypePtr::BOTTOM,
6242                                  crc, src_start, length);
6243   Node* result = _gvn.transform(new (C) ProjNode(call, TypeFunc::Parms));
6244   set_result(result);
6245   return true;
6246 }
6247 
6248 //----------------------------inline_reference_get----------------------------
6249 // public T java.lang.ref.Reference.get();
6250 bool LibraryCallKit::inline_reference_get() {
6251   const int referent_offset = java_lang_ref_Reference::referent_offset;
6252   guarantee(referent_offset &gt; 0, "should have already been set");
6253 
6254   // Get the argument:
6255   Node* reference_obj = null_check_receiver();
6256   if (stopped()) return true;
6257 
6258   Node* adr = basic_plus_adr(reference_obj, reference_obj, referent_offset);
6259 
6260   ciInstanceKlass* klass = env()-&gt;Object_klass();
6261   const TypeOopPtr* object_type = TypeOopPtr::make_from_klass(klass);
6262 
6263   Node* no_ctrl = NULL;
6264   Node* result = make_load(no_ctrl, adr, object_type, T_OBJECT, MemNode::unordered);
6265 
6266   // Use the pre-barrier to record the value in the referent field
6267   pre_barrier(false /* do_load */,
6268               control(),
6269               NULL /* obj */, NULL /* adr */, max_juint /* alias_idx */, NULL /* val */, NULL /* val_type */,
6270               result /* pre_val */,
6271               T_OBJECT);
6272 
6273   // Add memory barrier to prevent commoning reads from this field
6274   // across safepoint since GC can change its value.
6275   insert_mem_bar(Op_MemBarCPUOrder);
6276 
6277   set_result(result);
6278   return true;
6279 }
6280 
6281 
6282 Node * LibraryCallKit::load_field_from_object(Node * fromObj, const char * fieldName, const char * fieldTypeString,
6283                                               bool is_exact=true, bool is_static=false) {
6284 
6285   const TypeInstPtr* tinst = _gvn.type(fromObj)-&gt;isa_instptr();
6286   assert(tinst != NULL, "obj is null");
6287   assert(tinst-&gt;klass()-&gt;is_loaded(), "obj is not loaded");
6288   assert(!is_exact || tinst-&gt;klass_is_exact(), "klass not exact");
6289 
6290   ciField* field = tinst-&gt;klass()-&gt;as_instance_klass()-&gt;get_field_by_name(ciSymbol::make(fieldName),
6291                                                                           ciSymbol::make(fieldTypeString),
6292                                                                           is_static);
6293   if (field == NULL) return (Node *) NULL;
6294   assert (field != NULL, "undefined field");
6295 
6296   // Next code  copied from Parse::do_get_xxx():
6297 
6298   // Compute address and memory type.
6299   int offset  = field-&gt;offset_in_bytes();
6300   bool is_vol = field-&gt;is_volatile();
6301   ciType* field_klass = field-&gt;type();
6302   assert(field_klass-&gt;is_loaded(), "should be loaded");
6303   const TypePtr* adr_type = C-&gt;alias_type(field)-&gt;adr_type();
6304   Node *adr = basic_plus_adr(fromObj, fromObj, offset);
6305   BasicType bt = field-&gt;layout_type();
6306 
6307   // Build the resultant type of the load
6308   const Type *type;
6309   if (bt == T_OBJECT) {
6310     type = TypeOopPtr::make_from_klass(field_klass-&gt;as_klass());
6311   } else {
6312     type = Type::get_const_basic_type(bt);
6313   }
6314 
6315   if (support_IRIW_for_not_multiple_copy_atomic_cpu &amp;&amp; is_vol) {
6316     insert_mem_bar(Op_MemBarVolatile);   // StoreLoad barrier
6317   }
6318   // Build the load.
6319   MemNode::MemOrd mo = is_vol ? MemNode::acquire : MemNode::unordered;
6320   Node* loadedField = make_load(NULL, adr, type, bt, adr_type, mo, LoadNode::DependsOnlyOnTest, is_vol);
6321   // If reference is volatile, prevent following memory ops from
6322   // floating up past the volatile read.  Also prevents commoning
6323   // another volatile read.
6324   if (is_vol) {
6325     // Memory barrier includes bogus read of value to force load BEFORE membar
6326     insert_mem_bar(Op_MemBarAcquire, loadedField);
6327   }
6328   return loadedField;
6329 }
6330 
6331 
6332 //------------------------------inline_aescrypt_Block-----------------------
6333 bool LibraryCallKit::inline_aescrypt_Block(vmIntrinsics::ID id) {
6334   address stubAddr = NULL;
6335   const char *stubName;
6336   assert(UseAES, "need AES instruction support");
6337 
6338   switch(id) {
6339   case vmIntrinsics::_aescrypt_encryptBlock:
6340     stubAddr = StubRoutines::aescrypt_encryptBlock();
6341     stubName = "aescrypt_encryptBlock";
6342     break;
6343   case vmIntrinsics::_aescrypt_decryptBlock:
6344     stubAddr = StubRoutines::aescrypt_decryptBlock();
6345     stubName = "aescrypt_decryptBlock";
6346     break;
6347   }
6348   if (stubAddr == NULL) return false;
6349 
6350   Node* aescrypt_object = argument(0);
6351   Node* src             = argument(1);
6352   Node* src_offset      = argument(2);
6353   Node* dest            = argument(3);
6354   Node* dest_offset     = argument(4);
6355 
6356   // (1) src and dest are arrays.
6357   const Type* src_type = src-&gt;Value(&amp;_gvn);
6358   const Type* dest_type = dest-&gt;Value(&amp;_gvn);
6359   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
6360   const TypeAryPtr* top_dest = dest_type-&gt;isa_aryptr();
6361   assert (top_src  != NULL &amp;&amp; top_src-&gt;klass()  != NULL &amp;&amp;  top_dest != NULL &amp;&amp; top_dest-&gt;klass() != NULL, "args are strange");
6362 
6363   // for the quick and dirty code we will skip all the checks.
6364   // we are just trying to get the call to be generated.
6365   Node* src_start  = src;
6366   Node* dest_start = dest;
6367   if (src_offset != NULL || dest_offset != NULL) {
6368     assert(src_offset != NULL &amp;&amp; dest_offset != NULL, "");
6369     src_start  = array_element_address(src,  src_offset,  T_BYTE);
6370     dest_start = array_element_address(dest, dest_offset, T_BYTE);
6371   }
6372 
6373   // now need to get the start of its expanded key array
6374   // this requires a newer class file that has this array as littleEndian ints, otherwise we revert to java
6375   Node* k_start = get_key_start_from_aescrypt_object(aescrypt_object);
6376   if (k_start == NULL) return false;
6377 
6378   if (Matcher::pass_original_key_for_aes()) {
6379     // on SPARC we need to pass the original key since key expansion needs to happen in intrinsics due to
6380     // compatibility issues between Java key expansion and SPARC crypto instructions
6381     Node* original_k_start = get_original_key_start_from_aescrypt_object(aescrypt_object);
6382     if (original_k_start == NULL) return false;
6383 
6384     // Call the stub.
6385     make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::aescrypt_block_Type(),
6386                       stubAddr, stubName, TypePtr::BOTTOM,
6387                       src_start, dest_start, k_start, original_k_start);
6388   } else {
6389     // Call the stub.
6390     make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::aescrypt_block_Type(),
6391                       stubAddr, stubName, TypePtr::BOTTOM,
6392                       src_start, dest_start, k_start);
6393   }
6394 
6395   return true;
6396 }
6397 
6398 //------------------------------inline_cipherBlockChaining_AESCrypt-----------------------
6399 bool LibraryCallKit::inline_cipherBlockChaining_AESCrypt(vmIntrinsics::ID id) {
6400   address stubAddr = NULL;
6401   const char *stubName = NULL;
6402 
6403   assert(UseAES, "need AES instruction support");
6404 
6405   switch(id) {
6406   case vmIntrinsics::_cipherBlockChaining_encryptAESCrypt:
6407     stubAddr = StubRoutines::cipherBlockChaining_encryptAESCrypt();
6408     stubName = "cipherBlockChaining_encryptAESCrypt";
6409     break;
6410   case vmIntrinsics::_cipherBlockChaining_decryptAESCrypt:
6411     stubAddr = StubRoutines::cipherBlockChaining_decryptAESCrypt();
6412     stubName = "cipherBlockChaining_decryptAESCrypt";
6413     break;
6414   }
6415   if (stubAddr == NULL) return false;
6416 
6417   Node* cipherBlockChaining_object = argument(0);
6418   Node* src                        = argument(1);
6419   Node* src_offset                 = argument(2);
6420   Node* len                        = argument(3);
6421   Node* dest                       = argument(4);
6422   Node* dest_offset                = argument(5);
6423 
6424   // (1) src and dest are arrays.
6425   const Type* src_type = src-&gt;Value(&amp;_gvn);
6426   const Type* dest_type = dest-&gt;Value(&amp;_gvn);
6427   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
6428   const TypeAryPtr* top_dest = dest_type-&gt;isa_aryptr();
6429   assert (top_src  != NULL &amp;&amp; top_src-&gt;klass()  != NULL
6430           &amp;&amp;  top_dest != NULL &amp;&amp; top_dest-&gt;klass() != NULL, "args are strange");
6431 
6432   // checks are the responsibility of the caller
6433   Node* src_start  = src;
6434   Node* dest_start = dest;
6435   if (src_offset != NULL || dest_offset != NULL) {
6436     assert(src_offset != NULL &amp;&amp; dest_offset != NULL, "");
6437     src_start  = array_element_address(src,  src_offset,  T_BYTE);
6438     dest_start = array_element_address(dest, dest_offset, T_BYTE);
6439   }
6440 
6441   // if we are in this set of code, we "know" the embeddedCipher is an AESCrypt object
6442   // (because of the predicated logic executed earlier).
6443   // so we cast it here safely.
6444   // this requires a newer class file that has this array as littleEndian ints, otherwise we revert to java
6445 
6446   Node* embeddedCipherObj = load_field_from_object(cipherBlockChaining_object, "embeddedCipher", "Lcom/sun/crypto/provider/SymmetricCipher;", /*is_exact*/ false);
6447   if (embeddedCipherObj == NULL) return false;
6448 
6449   // cast it to what we know it will be at runtime
6450   const TypeInstPtr* tinst = _gvn.type(cipherBlockChaining_object)-&gt;isa_instptr();
6451   assert(tinst != NULL, "CBC obj is null");
6452   assert(tinst-&gt;klass()-&gt;is_loaded(), "CBC obj is not loaded");
6453   ciKlass* klass_AESCrypt = tinst-&gt;klass()-&gt;as_instance_klass()-&gt;find_klass(ciSymbol::make("com/sun/crypto/provider/AESCrypt"));
6454   assert(klass_AESCrypt-&gt;is_loaded(), "predicate checks that this class is loaded");
6455 
6456   ciInstanceKlass* instklass_AESCrypt = klass_AESCrypt-&gt;as_instance_klass();
6457   const TypeKlassPtr* aklass = TypeKlassPtr::make(instklass_AESCrypt);
6458   const TypeOopPtr* xtype = aklass-&gt;as_instance_type();
6459   Node* aescrypt_object = new(C) CheckCastPPNode(control(), embeddedCipherObj, xtype);
6460   aescrypt_object = _gvn.transform(aescrypt_object);
6461 
6462   // we need to get the start of the aescrypt_object's expanded key array
6463   Node* k_start = get_key_start_from_aescrypt_object(aescrypt_object);
6464   if (k_start == NULL) return false;
6465 
6466   // similarly, get the start address of the r vector
6467   Node* objRvec = load_field_from_object(cipherBlockChaining_object, "r", "[B", /*is_exact*/ false);
6468   if (objRvec == NULL) return false;
6469   Node* r_start = array_element_address(objRvec, intcon(0), T_BYTE);
6470 
6471   Node* cbcCrypt;
6472   if (Matcher::pass_original_key_for_aes()) {
6473     // on SPARC we need to pass the original key since key expansion needs to happen in intrinsics due to
6474     // compatibility issues between Java key expansion and SPARC crypto instructions
6475     Node* original_k_start = get_original_key_start_from_aescrypt_object(aescrypt_object);
6476     if (original_k_start == NULL) return false;
6477 
6478     // Call the stub, passing src_start, dest_start, k_start, r_start, src_len and original_k_start
6479     cbcCrypt = make_runtime_call(RC_LEAF|RC_NO_FP,
6480                                  OptoRuntime::cipherBlockChaining_aescrypt_Type(),
6481                                  stubAddr, stubName, TypePtr::BOTTOM,
6482                                  src_start, dest_start, k_start, r_start, len, original_k_start);
6483   } else {
6484     // Call the stub, passing src_start, dest_start, k_start, r_start and src_len
6485     cbcCrypt = make_runtime_call(RC_LEAF|RC_NO_FP,
6486                                  OptoRuntime::cipherBlockChaining_aescrypt_Type(),
6487                                  stubAddr, stubName, TypePtr::BOTTOM,
6488                                  src_start, dest_start, k_start, r_start, len);
6489   }
6490 
6491   // return cipher length (int)
6492   Node* retvalue = _gvn.transform(new (C) ProjNode(cbcCrypt, TypeFunc::Parms));
6493   set_result(retvalue);
6494   return true;
6495 }
6496 
6497 //------------------------------get_key_start_from_aescrypt_object-----------------------
6498 Node * LibraryCallKit::get_key_start_from_aescrypt_object(Node *aescrypt_object) {
6499 #ifdef PPC64
6500   // MixColumns for decryption can be reduced by preprocessing MixColumns with round keys.
6501   // Intel's extention is based on this optimization and AESCrypt generates round keys by preprocessing MixColumns.
6502   // However, ppc64 vncipher processes MixColumns and requires the same round keys with encryption.
6503   // The ppc64 stubs of encryption and decryption use the same round keys (sessionK[0]).
6504   Node* objSessionK = load_field_from_object(aescrypt_object, "sessionK", "[[I", /*is_exact*/ false);
6505   assert (objSessionK != NULL, "wrong version of com.sun.crypto.provider.AESCrypt");
6506   if (objSessionK == NULL) {
6507     return (Node *) NULL;
6508   }
6509   Node* objAESCryptKey = load_array_element(control(), objSessionK, intcon(0), TypeAryPtr::OOPS);
6510 #else
6511   Node* objAESCryptKey = load_field_from_object(aescrypt_object, "K", "[I", /*is_exact*/ false);
6512 #endif // PPC64
6513   assert (objAESCryptKey != NULL, "wrong version of com.sun.crypto.provider.AESCrypt");
6514   if (objAESCryptKey == NULL) return (Node *) NULL;
6515 
6516   // now have the array, need to get the start address of the K array
6517   Node* k_start = array_element_address(objAESCryptKey, intcon(0), T_INT);
6518   return k_start;
6519 }
6520 
6521 //------------------------------get_original_key_start_from_aescrypt_object-----------------------
6522 Node * LibraryCallKit::get_original_key_start_from_aescrypt_object(Node *aescrypt_object) {
6523   Node* objAESCryptKey = load_field_from_object(aescrypt_object, "lastKey", "[B", /*is_exact*/ false);
6524   assert (objAESCryptKey != NULL, "wrong version of com.sun.crypto.provider.AESCrypt");
6525   if (objAESCryptKey == NULL) return (Node *) NULL;
6526 
6527   // now have the array, need to get the start address of the lastKey array
6528   Node* original_k_start = array_element_address(objAESCryptKey, intcon(0), T_BYTE);
6529   return original_k_start;
6530 }
6531 
6532 //----------------------------inline_cipherBlockChaining_AESCrypt_predicate----------------------------
6533 // Return node representing slow path of predicate check.
6534 // the pseudo code we want to emulate with this predicate is:
6535 // for encryption:
6536 //    if (embeddedCipherObj instanceof AESCrypt) do_intrinsic, else do_javapath
6537 // for decryption:
6538 //    if ((embeddedCipherObj instanceof AESCrypt) &amp;&amp; (cipher!=plain)) do_intrinsic, else do_javapath
6539 //    note cipher==plain is more conservative than the original java code but that's OK
6540 //
6541 Node* LibraryCallKit::inline_cipherBlockChaining_AESCrypt_predicate(bool decrypting) {
6542   // The receiver was checked for NULL already.
6543   Node* objCBC = argument(0);
6544 
6545   // Load embeddedCipher field of CipherBlockChaining object.
6546   Node* embeddedCipherObj = load_field_from_object(objCBC, "embeddedCipher", "Lcom/sun/crypto/provider/SymmetricCipher;", /*is_exact*/ false);
6547 
6548   // get AESCrypt klass for instanceOf check
6549   // AESCrypt might not be loaded yet if some other SymmetricCipher got us to this compile point
6550   // will have same classloader as CipherBlockChaining object
6551   const TypeInstPtr* tinst = _gvn.type(objCBC)-&gt;isa_instptr();
6552   assert(tinst != NULL, "CBCobj is null");
6553   assert(tinst-&gt;klass()-&gt;is_loaded(), "CBCobj is not loaded");
6554 
6555   // we want to do an instanceof comparison against the AESCrypt class
6556   ciKlass* klass_AESCrypt = tinst-&gt;klass()-&gt;as_instance_klass()-&gt;find_klass(ciSymbol::make("com/sun/crypto/provider/AESCrypt"));
6557   if (!klass_AESCrypt-&gt;is_loaded()) {
6558     // if AESCrypt is not even loaded, we never take the intrinsic fast path
6559     Node* ctrl = control();
6560     set_control(top()); // no regular fast path
6561     return ctrl;
6562   }
6563   ciInstanceKlass* instklass_AESCrypt = klass_AESCrypt-&gt;as_instance_klass();
6564 
6565   Node* instof = gen_instanceof(embeddedCipherObj, makecon(TypeKlassPtr::make(instklass_AESCrypt)));
6566   Node* cmp_instof  = _gvn.transform(new (C) CmpINode(instof, intcon(1)));
6567   Node* bool_instof  = _gvn.transform(new (C) BoolNode(cmp_instof, BoolTest::ne));
6568 
6569   Node* instof_false = generate_guard(bool_instof, NULL, PROB_MIN);
6570 
6571   // for encryption, we are done
6572   if (!decrypting)
6573     return instof_false;  // even if it is NULL
6574 
6575   // for decryption, we need to add a further check to avoid
6576   // taking the intrinsic path when cipher and plain are the same
6577   // see the original java code for why.
6578   RegionNode* region = new(C) RegionNode(3);
6579   region-&gt;init_req(1, instof_false);
6580   Node* src = argument(1);
6581   Node* dest = argument(4);
6582   Node* cmp_src_dest = _gvn.transform(new (C) CmpPNode(src, dest));
6583   Node* bool_src_dest = _gvn.transform(new (C) BoolNode(cmp_src_dest, BoolTest::eq));
6584   Node* src_dest_conjoint = generate_guard(bool_src_dest, NULL, PROB_MIN);
6585   region-&gt;init_req(2, src_dest_conjoint);
6586 
6587   record_for_igvn(region);
6588   return _gvn.transform(region);
6589 }
6590 
6591 //------------------------------inline_sha_implCompress-----------------------
6592 //
6593 // Calculate SHA (i.e., SHA-1) for single-block byte[] array.
6594 // void com.sun.security.provider.SHA.implCompress(byte[] buf, int ofs)
6595 //
6596 // Calculate SHA2 (i.e., SHA-244 or SHA-256) for single-block byte[] array.
6597 // void com.sun.security.provider.SHA2.implCompress(byte[] buf, int ofs)
6598 //
6599 // Calculate SHA5 (i.e., SHA-384 or SHA-512) for single-block byte[] array.
6600 // void com.sun.security.provider.SHA5.implCompress(byte[] buf, int ofs)
6601 //
6602 bool LibraryCallKit::inline_sha_implCompress(vmIntrinsics::ID id) {
6603   assert(callee()-&gt;signature()-&gt;size() == 2, "sha_implCompress has 2 parameters");
6604 
6605   Node* sha_obj = argument(0);
6606   Node* src     = argument(1); // type oop
6607   Node* ofs     = argument(2); // type int
6608 
6609   const Type* src_type = src-&gt;Value(&amp;_gvn);
6610   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
6611   if (top_src  == NULL || top_src-&gt;klass()  == NULL) {
6612     // failed array check
6613     return false;
6614   }
6615   // Figure out the size and type of the elements we will be copying.
6616   BasicType src_elem = src_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
6617   if (src_elem != T_BYTE) {
6618     return false;
6619   }
6620   // 'src_start' points to src array + offset
6621   Node* src_start = array_element_address(src, ofs, src_elem);
6622   Node* state = NULL;
6623   address stubAddr;
6624   const char *stubName;
6625 
6626   switch(id) {
6627   case vmIntrinsics::_sha_implCompress:
6628     assert(UseSHA1Intrinsics, "need SHA1 instruction support");
6629     state = get_state_from_sha_object(sha_obj);
6630     stubAddr = StubRoutines::sha1_implCompress();
6631     stubName = "sha1_implCompress";
6632     break;
6633   case vmIntrinsics::_sha2_implCompress:
6634     assert(UseSHA256Intrinsics, "need SHA256 instruction support");
6635     state = get_state_from_sha_object(sha_obj);
6636     stubAddr = StubRoutines::sha256_implCompress();
6637     stubName = "sha256_implCompress";
6638     break;
6639   case vmIntrinsics::_sha5_implCompress:
6640     assert(UseSHA512Intrinsics, "need SHA512 instruction support");
6641     state = get_state_from_sha5_object(sha_obj);
6642     stubAddr = StubRoutines::sha512_implCompress();
6643     stubName = "sha512_implCompress";
6644     break;
6645   default:
6646     fatal_unexpected_iid(id);
6647     return false;
6648   }
6649   if (state == NULL) return false;
6650 
6651   // Call the stub.
6652   Node* call = make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::sha_implCompress_Type(),
6653                                  stubAddr, stubName, TypePtr::BOTTOM,
6654                                  src_start, state);
6655 
6656   return true;
6657 }
6658 
6659 //------------------------------inline_digestBase_implCompressMB-----------------------
6660 //
6661 // Calculate SHA/SHA2/SHA5 for multi-block byte[] array.
6662 // int com.sun.security.provider.DigestBase.implCompressMultiBlock(byte[] b, int ofs, int limit)
6663 //
6664 bool LibraryCallKit::inline_digestBase_implCompressMB(int predicate) {
6665   assert(UseSHA1Intrinsics || UseSHA256Intrinsics || UseSHA512Intrinsics,
6666          "need SHA1/SHA256/SHA512 instruction support");
6667   assert((uint)predicate &lt; 3, "sanity");
6668   assert(callee()-&gt;signature()-&gt;size() == 3, "digestBase_implCompressMB has 3 parameters");
6669 
6670   Node* digestBase_obj = argument(0); // The receiver was checked for NULL already.
6671   Node* src            = argument(1); // byte[] array
6672   Node* ofs            = argument(2); // type int
6673   Node* limit          = argument(3); // type int
6674 
6675   const Type* src_type = src-&gt;Value(&amp;_gvn);
6676   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
6677   if (top_src  == NULL || top_src-&gt;klass()  == NULL) {
6678     // failed array check
6679     return false;
6680   }
6681   // Figure out the size and type of the elements we will be copying.
6682   BasicType src_elem = src_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
6683   if (src_elem != T_BYTE) {
6684     return false;
6685   }
6686   // 'src_start' points to src array + offset
6687   Node* src_start = array_element_address(src, ofs, src_elem);
6688 
6689   const char* klass_SHA_name = NULL;
6690   const char* stub_name = NULL;
6691   address     stub_addr = NULL;
6692   bool        long_state = false;
6693 
6694   switch (predicate) {
6695   case 0:
6696     if (UseSHA1Intrinsics) {
6697       klass_SHA_name = "sun/security/provider/SHA";
6698       stub_name = "sha1_implCompressMB";
6699       stub_addr = StubRoutines::sha1_implCompressMB();
6700     }
6701     break;
6702   case 1:
6703     if (UseSHA256Intrinsics) {
6704       klass_SHA_name = "sun/security/provider/SHA2";
6705       stub_name = "sha256_implCompressMB";
6706       stub_addr = StubRoutines::sha256_implCompressMB();
6707     }
6708     break;
6709   case 2:
6710     if (UseSHA512Intrinsics) {
6711       klass_SHA_name = "sun/security/provider/SHA5";
6712       stub_name = "sha512_implCompressMB";
6713       stub_addr = StubRoutines::sha512_implCompressMB();
6714       long_state = true;
6715     }
6716     break;
6717   default:
6718     fatal(err_msg_res("unknown SHA intrinsic predicate: %d", predicate));
6719   }
6720   if (klass_SHA_name != NULL) {
6721     // get DigestBase klass to lookup for SHA klass
6722     const TypeInstPtr* tinst = _gvn.type(digestBase_obj)-&gt;isa_instptr();
6723     assert(tinst != NULL, "digestBase_obj is not instance???");
6724     assert(tinst-&gt;klass()-&gt;is_loaded(), "DigestBase is not loaded");
6725 
6726     ciKlass* klass_SHA = tinst-&gt;klass()-&gt;as_instance_klass()-&gt;find_klass(ciSymbol::make(klass_SHA_name));
6727     assert(klass_SHA-&gt;is_loaded(), "predicate checks that this class is loaded");
6728     ciInstanceKlass* instklass_SHA = klass_SHA-&gt;as_instance_klass();
6729     return inline_sha_implCompressMB(digestBase_obj, instklass_SHA, long_state, stub_addr, stub_name, src_start, ofs, limit);
6730   }
6731   return false;
6732 }
6733 //------------------------------inline_sha_implCompressMB-----------------------
6734 bool LibraryCallKit::inline_sha_implCompressMB(Node* digestBase_obj, ciInstanceKlass* instklass_SHA,
6735                                                bool long_state, address stubAddr, const char *stubName,
6736                                                Node* src_start, Node* ofs, Node* limit) {
6737   const TypeKlassPtr* aklass = TypeKlassPtr::make(instklass_SHA);
6738   const TypeOopPtr* xtype = aklass-&gt;as_instance_type();
6739   Node* sha_obj = new (C) CheckCastPPNode(control(), digestBase_obj, xtype);
6740   sha_obj = _gvn.transform(sha_obj);
6741 
6742   Node* state;
6743   if (long_state) {
6744     state = get_state_from_sha5_object(sha_obj);
6745   } else {
6746     state = get_state_from_sha_object(sha_obj);
6747   }
6748   if (state == NULL) return false;
6749 
6750   // Call the stub.
6751   Node* call = make_runtime_call(RC_LEAF|RC_NO_FP,
6752                                  OptoRuntime::digestBase_implCompressMB_Type(),
6753                                  stubAddr, stubName, TypePtr::BOTTOM,
6754                                  src_start, state, ofs, limit);
6755   // return ofs (int)
6756   Node* result = _gvn.transform(new (C) ProjNode(call, TypeFunc::Parms));
6757   set_result(result);
6758 
6759   return true;
6760 }
6761 
6762 //------------------------------get_state_from_sha_object-----------------------
6763 Node * LibraryCallKit::get_state_from_sha_object(Node *sha_object) {
6764   Node* sha_state = load_field_from_object(sha_object, "state", "[I", /*is_exact*/ false);
6765   assert (sha_state != NULL, "wrong version of sun.security.provider.SHA/SHA2");
6766   if (sha_state == NULL) return (Node *) NULL;
6767 
6768   // now have the array, need to get the start address of the state array
6769   Node* state = array_element_address(sha_state, intcon(0), T_INT);
6770   return state;
6771 }
6772 
6773 //------------------------------get_state_from_sha5_object-----------------------
6774 Node * LibraryCallKit::get_state_from_sha5_object(Node *sha_object) {
6775   Node* sha_state = load_field_from_object(sha_object, "state", "[J", /*is_exact*/ false);
6776   assert (sha_state != NULL, "wrong version of sun.security.provider.SHA5");
6777   if (sha_state == NULL) return (Node *) NULL;
6778 
6779   // now have the array, need to get the start address of the state array
6780   Node* state = array_element_address(sha_state, intcon(0), T_LONG);
6781   return state;
6782 }
6783 
6784 //----------------------------inline_digestBase_implCompressMB_predicate----------------------------
6785 // Return node representing slow path of predicate check.
6786 // the pseudo code we want to emulate with this predicate is:
6787 //    if (digestBaseObj instanceof SHA/SHA2/SHA5) do_intrinsic, else do_javapath
6788 //
6789 Node* LibraryCallKit::inline_digestBase_implCompressMB_predicate(int predicate) {
6790   assert(UseSHA1Intrinsics || UseSHA256Intrinsics || UseSHA512Intrinsics,
6791          "need SHA1/SHA256/SHA512 instruction support");
6792   assert((uint)predicate &lt; 3, "sanity");
6793 
6794   // The receiver was checked for NULL already.
6795   Node* digestBaseObj = argument(0);
6796 
6797   // get DigestBase klass for instanceOf check
6798   const TypeInstPtr* tinst = _gvn.type(digestBaseObj)-&gt;isa_instptr();
6799   assert(tinst != NULL, "digestBaseObj is null");
6800   assert(tinst-&gt;klass()-&gt;is_loaded(), "DigestBase is not loaded");
6801 
6802   const char* klass_SHA_name = NULL;
6803   switch (predicate) {
6804   case 0:
6805     if (UseSHA1Intrinsics) {
6806       // we want to do an instanceof comparison against the SHA class
6807       klass_SHA_name = "sun/security/provider/SHA";
6808     }
6809     break;
6810   case 1:
6811     if (UseSHA256Intrinsics) {
6812       // we want to do an instanceof comparison against the SHA2 class
6813       klass_SHA_name = "sun/security/provider/SHA2";
6814     }
6815     break;
6816   case 2:
6817     if (UseSHA512Intrinsics) {
6818       // we want to do an instanceof comparison against the SHA5 class
6819       klass_SHA_name = "sun/security/provider/SHA5";
6820     }
6821     break;
6822   default:
6823     fatal(err_msg_res("unknown SHA intrinsic predicate: %d", predicate));
6824   }
6825 
6826   ciKlass* klass_SHA = NULL;
6827   if (klass_SHA_name != NULL) {
6828     klass_SHA = tinst-&gt;klass()-&gt;as_instance_klass()-&gt;find_klass(ciSymbol::make(klass_SHA_name));
6829   }
6830   if ((klass_SHA == NULL) || !klass_SHA-&gt;is_loaded()) {
6831     // if none of SHA/SHA2/SHA5 is loaded, we never take the intrinsic fast path
6832     Node* ctrl = control();
6833     set_control(top()); // no intrinsic path
6834     return ctrl;
6835   }
6836   ciInstanceKlass* instklass_SHA = klass_SHA-&gt;as_instance_klass();
6837 
6838   Node* instofSHA = gen_instanceof(digestBaseObj, makecon(TypeKlassPtr::make(instklass_SHA)));
6839   Node* cmp_instof = _gvn.transform(new (C) CmpINode(instofSHA, intcon(1)));
6840   Node* bool_instof = _gvn.transform(new (C) BoolNode(cmp_instof, BoolTest::ne));
6841   Node* instof_false = generate_guard(bool_instof, NULL, PROB_MIN);
6842 
6843   return instof_false;  // even if it is NULL
6844 }
6845 
6846 bool LibraryCallKit::inline_profileBoolean() {
6847   Node* counts = argument(1);
6848   const TypeAryPtr* ary = NULL;
6849   ciArray* aobj = NULL;
6850   if (counts-&gt;is_Con()
6851       &amp;&amp; (ary = counts-&gt;bottom_type()-&gt;isa_aryptr()) != NULL
6852       &amp;&amp; (aobj = ary-&gt;const_oop()-&gt;as_array()) != NULL
6853       &amp;&amp; (aobj-&gt;length() == 2)) {
6854     // Profile is int[2] where [0] and [1] correspond to false and true value occurrences respectively.
6855     jint false_cnt = aobj-&gt;element_value(0).as_int();
6856     jint  true_cnt = aobj-&gt;element_value(1).as_int();
6857 
6858     if (C-&gt;log() != NULL) {
6859       C-&gt;log()-&gt;elem("observe source='profileBoolean' false='%d' true='%d'",
6860                      false_cnt, true_cnt);
6861     }
6862 
6863     if (false_cnt + true_cnt == 0) {
6864       // According to profile, never executed.
6865       uncommon_trap_exact(Deoptimization::Reason_intrinsic,
6866                           Deoptimization::Action_reinterpret);
6867       return true;
6868     }
6869 
6870     // result is a boolean (0 or 1) and its profile (false_cnt &amp; true_cnt)
6871     // is a number of each value occurrences.
6872     Node* result = argument(0);
6873     if (false_cnt == 0 || true_cnt == 0) {
6874       // According to profile, one value has been never seen.
6875       int expected_val = (false_cnt == 0) ? 1 : 0;
6876 
6877       Node* cmp  = _gvn.transform(new (C) CmpINode(result, intcon(expected_val)));
6878       Node* test = _gvn.transform(new (C) BoolNode(cmp, BoolTest::eq));
6879 
6880       IfNode* check = create_and_map_if(control(), test, PROB_ALWAYS, COUNT_UNKNOWN);
6881       Node* fast_path = _gvn.transform(new (C) IfTrueNode(check));
6882       Node* slow_path = _gvn.transform(new (C) IfFalseNode(check));
6883 
6884       { // Slow path: uncommon trap for never seen value and then reexecute
6885         // MethodHandleImpl::profileBoolean() to bump the count, so JIT knows
6886         // the value has been seen at least once.
6887         PreserveJVMState pjvms(this);
6888         PreserveReexecuteState preexecs(this);
6889         jvms()-&gt;set_should_reexecute(true);
6890 
6891         set_control(slow_path);
6892         set_i_o(i_o());
6893 
6894         uncommon_trap_exact(Deoptimization::Reason_intrinsic,
6895                             Deoptimization::Action_reinterpret);
6896       }
6897       // The guard for never seen value enables sharpening of the result and
6898       // returning a constant. It allows to eliminate branches on the same value
6899       // later on.
6900       set_control(fast_path);
6901       result = intcon(expected_val);
6902     }
6903     // Stop profiling.
6904     // MethodHandleImpl::profileBoolean() has profiling logic in its bytecode.
6905     // By replacing method body with profile data (represented as ProfileBooleanNode
6906     // on IR level) we effectively disable profiling.
6907     // It enables full speed execution once optimized code is generated.
6908     Node* profile = _gvn.transform(new (C) ProfileBooleanNode(result, false_cnt, true_cnt));
6909     C-&gt;record_for_igvn(profile);
6910     set_result(profile);
6911     return true;
6912   } else {
6913     // Continue profiling.
6914     // Profile data isn't available at the moment. So, execute method's bytecode version.
6915     // Usually, when GWT LambdaForms are profiled it means that a stand-alone nmethod
6916     // is compiled and counters aren't available since corresponding MethodHandle
6917     // isn't a compile-time constant.
6918     return false;
6919   }
6920 }
<a name="5" id="anc5"></a><b style="font-size: large; color: red">--- EOF ---</b>















































































</pre><form name="eof"><input name="value" value="5" type="hidden" /></form></body></html>
